---
title: "Take-home Assignment 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta"

title-block-banner: true

date: "20 february 2023"
date-modified: last-modified
author: "Rhonda Ho Kah Yee"

format:
  html:
      code-fold: true
      code-tools: true

execute: 
  message: false
  warning: false
  #eval: false

editor: visual
---

# 1. Overview

Hello! This is Rhonda Ho's take-home Assignment 2 for IS415 module.

To view/hide all the code at once, please click on the "\</\> code" tab beside the title of this html document and select the option to view/hide the code.

The full details of this assignment can be found [here](https://is415-ay2022-23t2.netlify.app/th_ex2.html).

## 1.1 Objectives

Exploratory Spatial Data Analysis (ESDA) hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatio-temporal trends of COVID-19 vaccination in DKI Jakarta.

## 1.2 The Data

1.  **Aspatial data**

For the purpose of this assignment, data from [Riwayat File Vaksinasi DKI Jakarta](https://riwayat-file-vaksinasi-dki-jakarta-jakartagis.hub.arcgis.com/) will be used. Daily vaccination data are provided. We are only required to download either the first day of the month or last day of the month of the study period.

2.  **Geospatial data**

For the purpose of this study, DKI Jakarta administration boundary 2019 will be used. The data set can be downloaded at Indonesia Geospatial portal, specifically at [this page](https://www.indonesia-geospasial.com/2020/04/download-shapefile-shp-batas-desa.html).

::: callout-note
-   The national Projected Coordinates Systems of Indonesia is DGN95 / Indonesia TM-3 zone 54.1.
-   Exclude all the outer islands from the DKI Jakarta sf data frame, and
-   Retain the first nine fields in the DKI Jakarta sf data frame. The ninth field JUMLAH_PEN = Total Population.
:::

## 1.3 Tasks

The specific tasks of this take-home exercise are as follows:

1.  **Choropleth Mapping and Analysis**

    -   Compute the monthly vaccination rate from July 2021 to June 2022 at sub-district (also known as kelurahan in Bahasa Indonesia) level,

    -   Prepare the monthly vaccination rate maps by using appropriate tmap functions,

    -   Describe the spatial patterns revealed by the choropleth maps (not more than 200 words).

2.  **Local Gi\* Analysis**

    With reference to the vaccination rate maps prepared in ESDA:

    -   Compute local Gi\* values of the monthly vaccination rate,

    -   Display the Gi\* maps of the monthly vaccination rate. The maps should only display the significant (i.e. p-value \< 0.05)

    -   With reference to the analysis results, draw statistical conclusions (not more than 250 words).

3.  **Emerging Hot Spot Analysis(EHSA)**

    With reference to the local Gi\* values of the vaccination rate maps prepared in the previous section:

    -   Perform Mann-Kendall Test by using the spatio-temporal local Gi\* values,

    -   Select three sub-districts and describe the temporal trends revealed (not more than 250 words), and

    -   Prepared a EHSA map of the Gi\* values of vaccination rate. The maps should only display the significant (i.e. p-value \< 0.05).\

    -   With reference to the EHSA map prepared, describe the spatial patterns revealed. (not more than 250 words).

# 2. Getting Started

The following packages will be used:

-   [sf](https://r-spatial.github.io/sf/): to import, manage, and process geospatial data

-   [tmap](https://cran.r-project.org/web/packages/tmap/): provides functions for plotting cartographic quality static point patterns maps or interactive maps

-   [plotly](https://cran.r-project.org/package=plotly): for creating interactive web-based graphs

-   [sfdep](https://cran.r-project.org/web/packages/sfdep/): for spatial dependence of simple features

-   [readxl](https://readxl.tidyverse.org/reference/read_excel.html): to import Excel worksheets(.xlsx)

-   [Kendall](https://cran.r-project.org/package=Kendall): for Mann-Kendall Test

-   [tidyverse](https://www.tidyverse.org/): a collection of packages for data science tasks

```{r}
pacman::p_load(sf, tmap, plotly, sfdep, readxl, kendall, tidyverse)
```

# 3. Data Wrangling

## 3.1 Geospatial Data

### 3.1.1 Import shapefile into r environment

```{r}
jkt2019 <- st_read(dsn = "data/geospatial", 
                 layer = "BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA") 
```

From the output, we can observe that:

1.  Geometry type is MULTIPOLYGON

2.  CRS information is [WGS 84](https://epsg.io/4326) instead of the CRS for Indonesia, Jakarta.

### 3.1.2 Identifying Relevant Columns

```{r}
glimpse(jkt2019)
```

Having a glimpse at our data, some of relevant column(s) that we need for our task is:

-   `KAB_KOTA` : represents the cities in Jakarta

-   `KECAMATAN` : represents the district in Jakarta

-   `DESA_KELUR` : represents the sub-districts in Jakarta.

### 3.1.3 Data Pre-Processing

#### 3.1.3.1 Check for Invalid Geometries

```{r}
#check for invalid geometries
length(which(st_is_valid(jkt2019) == FALSE))
```

There is no invalid geometries!

#### 3.1.3.2 Check for Missing Values

```{r}
#check for missing values
sum(is.na(jkt2019))
```

Based on the output, we have discovered that there are 14 missing values in our dataset which may affect our analysis. Thus, I removed all missing values in the sub-districts(a part of our main study focus), using the code chunk below.

```{r}
jkt2019 <- na.omit(jkt2019,c("DESA_KELUR"))
```

#### 3.1.3.3 Transformation of Coordinates Information

Earlier on, we discovered that the assigned coordinates system of our data is WGS 84 which is not appropriate for an Indonesian-specific geospatial dataset. Another way of checking it is using the function, [st_crs()](https://www.rdocumentation.org/packages/sf/versions/1.0-9/topics/st_crs).

```{r}
#check whether need to transform
st_crs(jkt2019)
```

As this is an Indonesian-specific geospatial dataset, I will be using [EPSG:23845.](https://epsg.io/23845)

```{r}
# transforms the CRS to DGN95, ESPG code 23845
jkt2019 <- st_transform(jkt2019, 23845)

st_crs(jkt2019)
```

We have successfully managed to transform our dataset to the appropriate coordinates system!

#### 3.1.3.4 Removal of Outer Islands

To start off, let visualise our data!

```{r}
plot(st_geometry(jkt2019))
```

The visualisation above shows that our data consist of outer islands beyond Jakarta. As this is not important for our analysis, we should remove it.

Previously, we have identified 3 important attributes which represents the divisions of Jakarta. They are `KAB_KOTA` (City), `KECAMATAN` (District) and `DESA_KELUR` (Village). Removing the outer islands by the City would help us to remove all the districts and villages within it, so I proceeded to check the unique values of it.

```{r}
unique(jkt2019$"KAB_KOTA")
```

From this, we can observe that all the values has "JAKARTA" in it except "KEPULAUAN SERIBU"(Thousand Island). The code chunk below visualises the different cities in Jakarta.

```{r}
tm_shape(jkt2019) + 
  tm_polygons("KAB_KOTA")
```

Next, we will need to remove the outer islands by using the [filter()](https://www.educative.io/answers/what-is-the-filter-method-in-r) function to filter out the outer islands.

```{r}
# accepts only if the value of KAB_KOTA is NOT KEPULAUAN SERIBU
jkt2019 <- filter(jkt2019, KAB_KOTA != "KEPULAUAN SERIBU")
```

For this assignment, we are required to only retain the first 9 fields.

```{r}
# filters out other fields by accepting only the first 9 fields
jkt2019 <- jkt2019[, 0:9]
```

For easier comprehension of our data, I have decided to translate the column headers to English.

```{r}
jkt2019 <- jkt2019 %>% 
  dplyr::rename(
    Object_ID=OBJECT_ID,
    Province=PROVINSI, 
    City=KAB_KOTA, 
    District=KECAMATAN, 
    Village_Code=KODE_DESA, 
    Village=DESA, 
    Sub_District=DESA_KELUR,
    Code=KODE, 
    Total_Population=JUMLAH_PEN
    )
```

With that, we are done for our geospatial data!

## 3.2 Aspatial Data

### 3.2.1 Observations of Data Website

Examining DKI Jakarta Vaccination File History [website](https://riwayat-file-vaksinasi-dki-jakarta-jakartagis.hub.arcgis.com/), some of my observations were:

-   Entire page is in Indonesian

    -   To translate to English, simply right click anywhere on the webpage and click "Translate to English" button

-   There were two columns for DKI Jakarta Vaccination File History, one for "*Vaccination Data Based on Village and District*", another for "*RT-Based Vaccination Data*"

    -   We will be focusing on the "V*accination Data Based on Village and District*"column as our task require us to analyse on a sub-district level.

-   Many instances of missing data!

    -   E.g 1) The hyperlink to March 1 2022 will direct you to March 2 2022 data and March 1 2022 cannot be found

    -   E.g 2) Certain months such as July 2021, August 2021, February 2022 were missing the record of the date written in the table and its respective data

        -   A few examples of missing records are 19 July 2021, 28 August 2022, 11 September, 28 February 2022, 17 March 2022

        -   Upon further research and some of my assumptions, missing data can be due to:

            -   A glitch in the system, which prevents the data from being uploaded

            -   An update of the system which may prevent records from being uploaded on that day

Under our task we are required to compute monthly vaccination rate from [July 2021 to June 2022]{.underline} and we are only required to download the [first or last day of the month of study period]{.underline}.

For this assignment, I will be using data of the [first day of each month]{.underline} of the study period with the exception of March 2021, I will be using March 2 instead of March 1 as March 1's data is missing.

### 3.2.2 Importing Aspatial Data

In our 'data/aspatial' folder, we have downloaded multiple .xlsx files ranging from 1 July 2021 to 1 June 2022. But before we compile it together, we need to understand each dataset and check for any discrepancies.

The purpose of the code chunk below is to reach each excel file under the folder 'data/aspatial' , create its dataframe(df) and assign it to a variable name which is in the format of "month_year". E.g April_2022 . The following functions are used:

-   [list.files():](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/list.files) creates a list of the files in a directory/folder

-   [substr():](https://www.digitalocean.com/community/tutorials/substring-function-in-r) extract the characters present in the data

-   [str_trim():](https://stringr.tidyverse.org/reference/str_trim.html) remove whitespace

-   [paste():](https://www.digitalocean.com/community/tutorials/paste-in-r) concatenate strings and values into a single element

-   [read_excel()](https://readxl.tidyverse.org/reference/read_excel.html#:~:text=read_excel()%20calls%20excel_format(),file%20itself%2C%20in%20that%20order.): to read the data in xlsx format

-   [nchar():](https://www.geeksforgeeks.org/finding-the-length-of-string-in-r-programming-nchar-method/#:~:text=nchar()%20method%20in%20R,character%20in%20a%20string%20object.&text=Where%3A%20String%20is%20object.,the%20length%20of%20a%20string.) gets the length of string

```{r}
#gets all files under the folder 'data/aspatial' which is in '.xlsx' format
study_period_data <- list.files('data/aspatial',pattern = "*.xlsx")

#instantiate an empty list which will be used later to contain all the df names
df_list <-list()

for (x in study_period_data){
 # eg of x: Data Vaksinasi Berbasis Kelurahan (01 April 2022).xlsx
 
 # extract the year and month from the data file name
 month_yr <- str_trim(substr(x, 38, nchar(x)-6)) #e.g output: April 2022
 
 #split the month and year by the empty space inbetween
 split_month_yr <- strsplit(month_yr," ") #e.g output: [[1]][1]'April' '2022'
 
 #split_month_yr[[1]][1] shows the month, split_month_yr[[1]][2] shows the year 
 
 #join the month and year with "_"
 join_month_yr <- paste(split_month_yr[[1]][1],split_month_yr[[1]][2],sep="_") 
 #e.g output: April_2022
 #join_month_yr will be used as the variable name to store the df
 
 #add each month name to list
 df_list<-append(df_list,join_month_yr)
 
 # get filepath that we need to read each month's excel file
 filepath <- paste("data/aspatial/",x,sep="")
 # eg output: "data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 April 2022).xlsx"
   
 #reach each excel file and assign it to its variable name 
 assign(join_month_yr,read_excel(filepath))
 
 
}
   
```

### 3.2.3 Data Pre-processing

#### 3.2.3.1 Check for Duplicated Values

To check for any duplicate columns for each dataset, I created a function called duplication_check() which takes in the df filename as a variable.

```{r}
duplication_check <- function(file_name) {
  duplicated_columns <- duplicated(as.list(file_name))
  duplicated_chara <- colnames(file_name[duplicated_columns])
  return(duplicated_chara)
}
```

I then made use of df_list which I populated earlier on to contain all of our df names and looped through it. This allows me to check for each time period df, which df has duplicated columns using the function I created.

```{r}
for (df in df_list) {
  duplicated_col <- duplication_check(get(df))
  if(identical(duplicated_col, character(0))){
    duplicated_col="none"
  }
  
  print(paste("Under df", df, "| duplicated col:", toString(duplicated_col)))
}

```

Based on the output, we can observe that for df April_2022, Juni_2022 and Mei_2022. There is a duplicated column called `LANSIA\r\nDOSIS 3`.

Diving deeper into each dataset with a duplicated column with the use of glimpse(), we can observe that column `LANSIA\r\nDOSIS 3` and column `LANSIA\r\nDOSIS 2` are exactly the same.

```{r}
glimpse(April_2022)
```

**But how does this affect our dataset?**

Upon translation, i discovered that `LANSIA\r\nDOSIS` refers to elderly dosage.

This means that in our datasets with duplicated columns, there is 4 related columns - Elderly Dosage 1, Elderly Dosage 2, Elderly Dosage 3, Total Elderly Dosage where Dosage 3 values are a repeat of Dosage 2 values.

I suspected that the Total elderly vaccine delivered should be the sum of Elderly Dosage 1, 2 and 3. However, according to April_2022 df, total elderly vaccine delivered is 1536442 which is neither the sum of elderly dosage 1, 2 and 3 nor the sum of elderly dosage 1 and 2. This suggests that either the total sum is wrongly added or the values for elderly dosage 3 was input wrongly. Regardless, this reduces the reliability of those data columns.

#### 3.2.3.2 Check for Missing Values

To check for missing values, I created a function, check_missing_value() and used it to check for each df.

```{r}
check_missing_value <- function(filename){
  sum(is.na(filename))
}

for (df in df_list) {
  print(check_missing_value(get(df)))
}
```

Based on the output, I realised that all the df were missing 3 fields. Looking deeper, I realised it was the same 3 fields (1st row, first 3 columns) for all the df that were blank. The reason why it is empty is due to the fact that is was merged with the top column in the excel and the 1st row of each excel file was meant to represent the total dosages across all districts. Thus, we will be removing it later.

#### 3.2.3.3 Combination of every .xlsx Files

Now, we need to combine the data files for every time period of our study period.

First, I would need to add the date in a new column called `Time_Period` .

```{r}
for (df_str in df_list) {
  df<-get(df_str)
  
  new_df<- df %>%
    #extract relevant columns
    select(c('WILAYAH KOTA', # CITY AREA
             'KECAMATAN', # DISTRICT
             'KELURAHAN', # SUBDISTRICT
             'SASARAN', # TARGET population to be vacinated
             'BELUM VAKSIN' #Yet to be vacinnated
             )) %>%
    #add the month_year
    add_column(Time_Period = df_str)
  
  assign(df_str,new_df)
}

combined_df <- setNames(data.frame(matrix(ncol = 6, nrow = 0)), 
                        c('WILAYAH KOTA',
                          'KECAMATAN',
                          'KELURAHAN',
                          'SASARAN',
                          'BELUM VAKSIN',
                          'Time_Period'))

#combining all tgt
for (df_str in df_list) {
  combined_df <- rbind(combined_df,get(df_str))
}

```

Next, I translated the Indonesian column headers into English so we can understand it better.

```{r}
#translate  colnames into eng
colnames(combined_df) <- c("CITY_AREA", "DISTRICT","SUBDISTRICT", "TARGET", "YET_TO_BE_VACINATED", "TIME_PERIOD")

```

Afterwards, I wanted to convert our time period into the date format yyyy-mm-dd.

```{r}
#convert into date format yyyy-mm-dd
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Desember_2021'] <- '2021-12-01'
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Februari_2022'] <- '2022-02-01'
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Januari_2022'] <- '2022-01-01'
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Juli_2021'] <- '2021-07-01'
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Juni_2022'] <- '2022-06-01'
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Mei_2022'] <- '2022-05-01'
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Oktober_2021'] <- '2021-10-01'
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Maret_2022'] <- '2022-03-02'
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Agustus_2021'] <- '2021-08-01'
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'September_2021'] <- '2021-09-01'
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'November_2021'] <- '2021-11-01'
combined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'April_2022'] <- '2022-04-01'

#convert to date format
combined_df$TIME_PERIOD <-as.Date(combined_df$TIME_PERIOD, format="%Y-%m-%d")
class(combined_df$TIME_PERIOD)
```

```{r}
#check missing values
combined_df[rowSums(is.na(combined_df))!=0,]

```

As identified earlier, there are missing values under `CITY AREA` and `DISTRICT` where the `SUBDISTRICT` is "TOTAL". As our analysis focuses on the Subdistricts of Jakarta, knowing the "Total" is not important. Hence, I decided to remove it.

```{r}
combined_df <- na.omit(combined_df)
```

```{r}
sum(is.na(combined_df))
```

#### 3.2.3.4 Removal of Outer Islands

Next, we need to check whether this data consist of any outer islands and remove it accordingly. The code chunk below follow the steps earlier shown in the removal of outer islands.

```{r}
#identify unique values of city areas in Jakarta
unique(combined_df$"CITY_AREA")
```

```{r}
#remove the outer island i.e KAB.ADM.KEP.SERIBU
combined_df <- filter(combined_df, CITY_AREA != "KAB.ADM.KEP.SERIBU")
```

# 4. Geospatial Data Integration

Now that we have prepared our geospatial and aspatial data frames, we'll need to join them. A quick look at their headers tell us what their common fields are:

```{r}
colnames(jkt2019)
```

```{r}
colnames(combined_df)
```

Analyzing the columns headers, columns related to the city, district and subdistrictshould match.

```{r}
# joins based on district, Sub_District and City
combined_jakarta <- left_join(jkt2019, combined_df,
                              by=c(
                                "City"="CITY_AREA", 
                                "District"="DISTRICT",
                                "Sub_District"="SUBDISTRICT")
                              )
```

Now, let's visualise our combined_jakarta and with its target and to be vaccinated values count.

```{r}
target_vaccines = tm_shape(combined_jakarta)+
  tm_fill("TARGET") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title="Target Vaccinated Count")

yet_to_be_vaccines = tm_shape(combined_jakarta)+
  tm_fill("YET_TO_BE_VACINATED") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title="Yet To Be Vaccinated Count")

tmap_arrange(target_vaccines, yet_to_be_vaccines)
```

Both our maps show that there are missing values even though both dataframes missing values were removed earlier.

Thus, we need to look deeper into our values. Some of my observations looking at the `SUBDISTRICT` and `Sub_District` values directly were that there were alternate ways of calling the same sub district. For example, "KRAMAT JATI" in combined_df, but "KRAMAT ATI" in jkt2019.

The code chunk below further explores the unique values of sub districts found in count_subdistrict but not in bd_subdistrict and vice versa.

```{r}
count_subdistrict <- c(combined_df$SUBDISTRICT)
bd_subdistrict <- c(jkt2019$Sub_District)

cat("Unique Aspatial SUBDISTRICT:\n__________________________________________________________\n")
unique(count_subdistrict[!(count_subdistrict %in% bd_subdistrict)])

cat("\nUnique Geospatial Sub_District:\n__________________________________________________________\n")
unique(bd_subdistrict[!(bd_subdistrict %in% count_subdistrict)])
```

The table below compiles the mismatched records of the subdistricts between the two dataframes.

```{r}
# initialise a dataframe of our cases vs bd subdistrict spelling
spelling <- data.frame(
  Aspatial_Count=c("BALE KAMBANG", "HALIM PERDANA KUSUMAH", "JATI PULO", "KALI BARU", "KAMPUNG TENGAH", "KRAMAT JATI", "KERENDANG", "PAL MERIAM", "PINANG RANTI", "RAWA JATI"),
  Geospatial_BD=c("BALEKAMBAG", "HALIM PERDANA KUSUMA", "JATIPULO", "KALIBARU", "TENGAH", "KRAMATJATI", "KRENDANG", "PALMERIAM", "PINANGRANTI", "RAWAJATI")
  )

spelling
```

Next, we need to rectify the mismatched subdistricts names.

```{r}
# where jkt2019 is a mismatched value, replace with the correct value
jkt2019$Sub_District[jkt2019$Sub_District == 'BALEKAMBANG'] <- 'BALE KAMBANG'
jkt2019$Sub_District[jkt2019$Sub_District == 'HALIM PERDANA KUSUMA'] <- 'HALIM PERDANA KUSUMAH'
jkt2019$Sub_District[jkt2019$Sub_District == 'JATIPULO'] <- 'JATI PULO'

jkt2019$Sub_District[jkt2019$Sub_District == 'KALI BARU'] <- 'KALIBARU'

jkt2019$Sub_District[jkt2019$Sub_District == 'TENGAH'] <- 'KAMPUNG TENGAH'
jkt2019$Sub_District[jkt2019$Sub_District == 'KRAMATJATI'] <- 'KRAMAT JATI'
jkt2019$Sub_District[jkt2019$Sub_District == 'KRENDANG'] <- 'KERENDANG'
jkt2019$Sub_District[jkt2019$Sub_District == 'PALMERIAM'] <- 'PAL MERIAM'
jkt2019$Sub_District[jkt2019$Sub_District == 'PINANGRANTI'] <- 'PINANG RANTI'
jkt2019$Sub_District[jkt2019$Sub_District == 'RAWAJATI'] <- 'RAWA JATI'
```

Rejoining the two dataframes

```{r}
combined_jakarta <- left_join(jkt2019, combined_df,
                              by=c("Sub_District"="SUBDISTRICT")
                              )
```

```{r}
# double check of any missing values
sum(is.na(combined_jakarta))
```

Once again, let's visualise it!\

```{r}
updated_target_count = tm_shape(combined_jakarta)+
  tm_fill("TARGET") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title="Updated Target Vacinated Count")

updated_yet_to_be_count = tm_shape(combined_jakarta)+
  tm_fill("YET_TO_BE_VACINATED") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title="Updated TO Be Vaccinated Count")

tmap_arrange(updated_target_count, updated_yet_to_be_count)
```

# 5. Choropleth Mapping and Analysis

## 5.1 Calculation of Monthly Vaccination Rates

To calculate the monthly vaccination rates, I will be using this formula:

$Vaccination Rate (\%) =\frac{Target - Yet To Be Vaccinated}{Target} * 100$

where Target refers to target population to get vaccinated and YetToBeVacinated refers to population who have not been vaccinated yet. Essentially, the vaccination rate is derived from taking count of people who are vaccinated over count of people who are supposed to be vaccinated.

```{r}
# grouping based on the sub-district and time_period

vr <- combined_df %>%
  inner_join(jkt2019, by=c("SUBDISTRICT" = "Sub_District")) %>%
  group_by(SUBDISTRICT, TIME_PERIOD) %>%
  dplyr::summarise(`vaccination_rates` = ((sum(TARGET-YET_TO_BE_VACINATED)/sum(TARGET))*100)) %>%
  
  #afterwards, pivots the table based on the Dates, showing the cumulative death rate
  ungroup() %>% pivot_wider(names_from = TIME_PERIOD,
              values_from = vaccination_rates)
vr
```

The output of the code chunk below shows monthly vaccination rates across each sub-district from a study period of July 2021 to June 2022.

**Convert our dataframes into sf objects**

```{r}
combined_jakarta <- st_as_sf(combined_jakarta)

vr <- vr%>% left_join(jkt2019, by=c("SUBDISTRICT"="Sub_District"))
vr <- st_as_sf(vr)
```

## 5.2 Mapping of Monthly Vaccination Rates

There are different [methods](https://storymaps.arcgis.com/stories/bcb8e509f319436786a2481c722bc218) to classify the vaccination rates in choropleth maps. Initially, I considered Jenks classification method as it can minimize variation in each group which allows map readers to easily witness the trends presented by the map. However, this method is not very suitable for our data as it has low variance.

The code chunk below shows the decreasing variance as time passes through the period of July 2021 to June 2022.

```{r}
var(vr$`2021-07-01`)
var(vr$`2021-10-01`)
var(vr$`2022-02-01`)
var(vr$`2022-06-01`)
```

To get a clearer visualization of a specific month, I decided to use the classification method, cont as it creates a smooth, linear gradient in which the change in values is proportionally related to the change in colors. For the color palette, i decided to go with a sequential scheme from yellow to green as it more suited for ordered data that progress from low to high.

The code chunk below is a map of vaccination rates in July 2021. By clicking on a certain location, it will show us the district name and respective vaccination rates. The darker the green, the higher the vaccination rates.

```{r}
  tmap_mode("view")
  tm_shape(vr)+
    tm_polygons("2021-07-01", 
            n= 6,
            style = "cont",
            title = "Vaccination Rate(%)",
            palette = "YlGn",
            ) +
  
    tm_layout(main.title = "Vaccination Rates in July 2021",
              main.title.position = "center",
              main.title.size = 1,
              #legend.height = 0.5, 
              #legend.width = 0.4,
              
              ) +
    tm_borders(alpha = 0.5) +
    tm_view(set.zoom.limits = c(10, 14),
            view.legend.position = c("right","bottom"))
```

Insights:

1.  The sub-district with the lowest vaccination rate is CIPINANG BESAR UTARA (23.15%).

2.  The sub-district with the highest vaccination rate is HALIM PERDANA KUSUMAH (55.24%).

3.  Sub-districts with lower vaccination rates can try to learn/adopt strategies from sub districts with higher vaccination rates' to increase their own vaccination rates.

The code chunk below is a map of vaccination rates in June 2022.

```{r}
  tmap_mode("view")
  tm_shape(vr)+
    tm_polygons("2022-06-01", 
            n= 6,
            style = "cont",
            title = "Vaccination Rate(%)",
            palette = "YlGn",
            ) +
    
    tm_layout(main.title = "Vaccination Rates in June 2022",
              main.title.position = "center",
              main.title.size = 1,
              #legend.height = 0.5, 
              #legend.width = 0.4,
              ) +
    tm_borders(alpha = 0.5) +
    tm_view(set.zoom.limits = c(10, 14),
            view.legend.position = c("right","bottom"))
```

```{r}
tmap_mode("plot")
```

Insights (as compared to July 2021):

1.  The sub-district with the lowest vaccination rate is no longer CIPINANG BESAR UTARA(81.79%) but KEBON MELATI (78.13%). However, the lowest vaccination rate in June 2022 is still much higher than the the highest vaccination rate in July 2021 (55.24%).

2.  The sub-district with highest vaccination rate is the same, HALIM PERDANA KUSUMAH (89.76%).

3.  The bottom half sub-districts tend to have a higher vaccination rate than the top half sub-districts.

4.  The range of vaccination rates have increased from range 25% to 55% to range 80% to 88%,

5.  Thus, we can infer that within a span of 1 year, the vaccination rates across the sub-districts in Jakarta have increased.

The code chunk below plots the vaccination rate across the our study period of July 2021 to June 2022. The data is classified manually. The breakpoints are 20, 40, 60, 80, 100.

```{r}
rate_cols <- colnames(st_set_geometry(vr[2:13], NULL))

map_list <- vector(mode = "list", length = length(rate_cols))

for (i in 1:length(rate_cols)) {
  cmap <- tm_shape(vr)+
    tm_polygons(rate_cols[[i]], 
            n= 6,
            breaks = c(0, 20, 40, 60, 80, 100),
            #style = "cont",
            title = "Vaccination Rate(%)",
            palette = "YlGn",
            ) +
    #tm_text("SUBDISTRICT")+
    tm_layout(
              panel.show = TRUE,
              panel.labels = rate_cols[[i]],
              panel.label.color = 'gray12',
              panel.label.size = 0.8,
              legend.show = FALSE
              )
  
  map_list[[i]] <- cmap
}

tmap_arrange(map_list, ncol = 4)

```

Insights:

1.  We can actually see Jakarta's vaccination rate increase steadily across the period of July 2021 to February 2022 as more sub-districts shift from a lighter shade of green to a darker one.

2.  However, from February 2022 to June 2022, there is not much difference in the vaccination rates across all sub-districts.

# 6. Local Gi\* Analysis

To detect hot spot areas with high vaccination rates, and cold spot areas with high vaccination rates in Jakarta, we will be using Getis-Ord Gi\* statistics.

For Interpretation of Gi\* values:

$Gi∗>0$ : indicates grouping of areas with values higher than average

$Gi∗<0$ : indicates grouping of areas with values lower than average

A larger magnitude represents a greater intensity of grouping.

For significant Gi\* statistic values, two spatial associations can be inferred:

-   **Hot spot areas:** where $Gi∗>0$, significant and positive, if location is associated with relatively high values of the surrounding locations.

-   **Cold spot areas:** where $Gi∗<0$, significant and negative, if location is associated with relatively low values in surrounding locations

## 6.1 Computing local Gi\* Values of Monthly Vaccination Rates

Before we compute our local Gi\*, I would like to rearange our data for better clarity. The data will be organised in the following format: Date, Sub_District and Vaccination_Rate.

```{r}
#Extract relevant columns
copy_cjkt_df <- combined_jakarta[8:14]

# calculate vaccination rates
copy_cjkt_df <- copy_cjkt_df %>% 
  mutate(
    VACCINATION_RATES = (TARGET - YET_TO_BE_VACINATED)/TARGET *100,
    )

#drop not impt col
copy_cjkt_df = subset(copy_cjkt_df, select = -c(Total_Population,CITY_AREA,DISTRICT,TARGET,YET_TO_BE_VACINATED) )

#for clarity, reorder the columns to data, subdistrict, vaccination rate
copy_cjkt_df<- copy_cjkt_df %>% relocate(VACCINATION_RATES, .after=Sub_District)
copy_cjkt_df<- copy_cjkt_df %>% relocate(TIME_PERIOD)

colnames(copy_cjkt_df) <- c("Date", "Sub_District","Vaccination_Rate","geometry")
```

```{r}
glimpse(copy_cjkt_df)

```

The code chunk below creates a time series cube using [spacetime()](https://cran.r-project.org/web/packages/spacetime/index.html) of sfdep package:

```{r}
vr_st<- as_spacetime(copy_cjkt_df,
                                .loc_col = "Sub_District",
                                .time_col = "Date")
```

By using [is_spacetime_cube()](https://gisgeography.com/space-time-cubes/#:~:text=Space%2Dtime%20cubes%20show%20how,bottom%20cubes%20have%20older%20timestamps.) of sfdep package, we can check if vacc_rate_st is indeed a space-time cube object. If it returns TRUE, vacc_rate_st is a space-time cube object.

```{r}
is_spacetime_cube(vr_st)
```

To compute the local Gi\* statistics, we need to derive the spatial weights first:

```{r}

vr_nb <- vr_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale=1,
                                  alpha=1),
         .before=1) %>%
  set_wts("wt") %>%
  set_nbs("nb")
```

Our df now has neigbours (nb) and weights(wt) for each date.

```{r}
head(vr_nb)
```

Before computing Gi\*, we need to set the seed value so that the results of the simulations will be reproducible and constant. I willbe using seed 123.

```{r}
set.seed(123)
```

The code chunk below computes Gi\* for each location, grouping by Date and using [local_gstar_perm()](https://sfdep.josiahparry.com/reference/local_gstar.html) of sfdep package:

```{r}
gi_stars <- vr_nb %>%
  group_by(Date) %>%
  mutate(gi_star = local_gstar_perm(
    Vaccination_Rate, nb, wt, nsim=99)) %>%
      tidyr::unnest(gi_star)
```

After running 100 simulations, the code chunk below shows our newly created df.

```{r}
gi_stars
```

## 6.2 Mapping of Gi\* and Monthly Vaccination Rates

To map our local gi\*, we first need to include the geometry values by joining copy_cjkt_df with gi_stars.

```{r}
combined_cjkt_gi_stars <- copy_cjkt_df %>%
  left_join(gi_stars)
```

### 6.2.1 Mapping for July 2021

Taking a look at the Gi\* values and its p-value of the vaccination rates of July 2021:

```{r}
tmap_mode("plot")
gi_star_map = tm_shape(filter(combined_cjkt_gi_stars, Date == '2021-07-01')) +
  tm_fill("gi_star") +
  tm_borders(alpha=0.5) +
  tm_layout(main.title = "Gi* values for vaccination rates in July 2021", main.title.size=0.8)

p_sim_map = tm_shape(filter(combined_cjkt_gi_stars, Date == '2021-07-01')) +
  tm_fill("p_sim", breaks = c(0, 0.05, 1)) +
  tm_borders(alpha=0.5) + 
  tm_layout(main.title = "p-values of Gi* for vaccination rates in July 2021", main.title.size=0.8)

tmap_arrange(gi_star_map, p_sim_map)
```

```{r}
psig_july2021 <- combined_cjkt_gi_stars  %>%
  filter(p_sim < 0.05 & Date == '2021-07-01')

tmap_mode("plot")
tm_shape(combined_cjkt_gi_stars) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(psig_july2021) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.4) +
   tm_layout(main.title="Hot and Cold Spot Areas of Vaccination Rates in July 2021", main.title.size = 0.8)


```

### 6.2.1 Mapping for 4 Time Periods

To compare the hot and cold spot over our study period, I will break the period into 4 quarters which are July 2021, October 2021, February 2022, June 2022.

```{r}
# july 2021
psig_july <- combined_cjkt_gi_stars  %>%
  #filter significant values(<0.05) and the according date
  filter(p_sim < 0.05 & Date == '2021-07-01')

#tmap_mode("plot")
hcs_july2021<- tm_shape(combined_cjkt_gi_stars) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(psig_july) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.4) +
   tm_layout(
     main.title="Hot and Cold Spot Areas of Vaccination Rates in July 2021",
     main.title.size = 0.5)

# october 2021
psig_oct <- combined_cjkt_gi_stars  %>%
  #filter significant values(<0.05) and the according date
  filter(p_sim < 0.05 & Date == '2021-10-01')

hcs_oct2021 <-tm_shape(combined_cjkt_gi_stars) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(psig_oct) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.4) +
   tm_layout(
     main.title="Hot and Cold Spot Areas of Vaccination Rates in October 2021",
     main.title.size = 0.5)

#february 2022
psig_feb <- combined_cjkt_gi_stars  %>%
  #filter significant values(<0.05) and the according date
  filter(p_sim < 0.05 & Date == '2022-02-01')

hcs_feb2022<- tm_shape(combined_cjkt_gi_stars) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(psig_feb) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.4) +
   tm_layout(
     main.title="Hot and Cold Spot Areas of Vaccination Rates in February 2022",
     main.title.size = 0.5)

# june 2022
psig_june <- combined_cjkt_gi_stars  %>%
  #filter significant values(<0.05) and the according date
  filter(p_sim < 0.05 & Date == '2022-06-01')

hcs_june2022 <- tm_shape(combined_cjkt_gi_stars) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(psig_june) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.4) +
   tm_layout(
     main.title="Hot and Cold Spot Areas of Vaccination Rates in June 2022",
     main.title.size = 0.5)

tmap_arrange(hcs_july2021,hcs_oct2021,hcs_feb2022,hcs_june2022)
```

The code chunk below creates a function to check the sub-district with the highest and lowest siginificant gi\* value for July 2021, October 2021, February 2022, June 2022.

```{r}
get_significant_locations <- function(date) {
  #print(paste("Sub-Districts with p-value of Gi* < 0.05 in", month))
  
  #sub district name
  sub_d<- filter(combined_cjkt_gi_stars, Date == date & p_sim<0.05)$Sub_District
  
  #gi star value
  gi_star_value <- filter(combined_cjkt_gi_stars, Date == date & p_sim<0.05)$gi_star

  temp <-data.frame(sub_d,gi_star_value)
  #print(temp)
  
  max_gi_sd<- temp$sub_d[temp$gi_star_value ==max(temp$gi_star_value)]
  min_gi_sd<- temp$sub_d[temp$gi_star_value ==min(temp$gi_star_value)]
  
  print(paste("Date: ", date))
  print("For siginificant sub-district with the")
  print(paste("highest gi* value: ",max_gi_sd))
  print(paste("lowest gi* value: ",min_gi_sd))
}

get_significant_locations("2021-07-01")
get_significant_locations("2021-10-01")
get_significant_locations("2022-02-01")
get_significant_locations("2022-06-01")
```

### 6.2.3 Statistical Conclusion

1.  Based on our maps, areas colored "red" are significant hot spots, while areas colored "green" are significant cold spots.

2.  Overall, we can observe that there is spatial clustering in our data. From July 2021 to June 2022, the no. of hot and cold spot areas have changed and that the intensity of Gi\* value has changed. The lowest range of Gi\* value decreased from -4 to -2 to -8 to -6.

3.  In October 2021, there is an unusual large number of hot and cold spots which signifies 5 grouping of hot spot areas with values higher than average and 7 significant groupings of cold spot areas with values lower than average.

4.  Hot Spots Analysis: The distribution of the hots pots started from 6 clusters (most of it was around the eastern side of Jakarta) to only 1 cluster in the middle of Jakarta from July 2021 to June 2022.

5.  Cold Spots Analysis: The distribution of the cold spots appeared randomly and shifted towards the southern area of Jakarta from July 2021 to June 2022.

# 7. Emerging Hot Spot Analysis(EHSA)

## 7.1 Mann-Kendall Test

We will be conducting the Mann-Kendall Test using the spatio-temporal local Gi\* values on 3 Sub-Districts which are:

1.  KEBON KACANG

2.  SRENGSENG SAWAH

3.  HALIM PERDANA KUSUMAH

**What is the Mann-Kendall test?**

In a [Mann-Kendall Test](https://www.statology.org/mann-kendall-trend-test-r/#:~:text=A%20Mann%2DKendall%20Trend%20Test%20is%20used%20to%20determine%20whether,trend%20present%20in%20the%20data.), it is used to determine whether or not a trend exists in time series data. It is a non-parametric test, meaning there is no underlying assumption made about the normality of the data.

The hypotheses for the test are as follows:

-   H0 (null hypothesis): There is no trend present in the data.

-   HA (alternative hypothesis): A trend is present in the data. (This could be a positive or negative trend)

If the p-value of the test is lower than the significance level of 0.05, then there is statistically significant evidence that a trend is present in the time series data.

This method mainly gives 3 type of [information:](https://acp.copernicus.org/preprints/acp-2019-109/acp-2019-109-AC2-supplement.pdf)

-   Tau: varies between -1 and 1; it is positive when the trend increases and negative when the trend decreases

-   The Sen slope: which estimates the overall slope of the time series. This slope corresponds to the median of all the slopes calculated between each pair of points in the series.

-   The significance, which represents the threshold for which the hypothesis that there is no trend is accepted. The trend is statistically significant when the p-value is less than 0.05.

### 7.1.1 For KEBON KACANG

```{r}
KEBON_KACANG <- gi_stars %>% 
  ungroup() %>% 
  filter(Sub_District == "KEBON KACANG") |> 
  select(Sub_District, Date, gi_star)

KEBON_KACANG_plot<- ggplot(data = KEBON_KACANG, 
       aes(x = Date, 
           y = gi_star)) +
  geom_line() +
  theme_light()
  
  plot<- KEBON_KACANG_plot+ggtitle("Local Gi* values of KEBON KACANG")

  ggplotly(plot)  
```

```{r}
KEBON_KACANG %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

Based on the output above, we can observe that the tau is highly negative suggesting that the trend is decreasing. The sl, which represents the p-value, is smaller than our level of significance, 0.05. Hence, we reject the null hypothesis as there is significant evidence that a decreasing trend is present in the time series data.

### 7.1.2 For SRENGSENG SAWAH

```{r}

SRENGSENG_SAWAH <- gi_stars %>% 
  ungroup() %>% 
  filter(Sub_District == "SRENGSENG SAWAH") |> 
  select(Sub_District, Date, gi_star)

SRENGSENG_SAWAH_plot<- ggplot(data = SRENGSENG_SAWAH, 
       aes(x = Date, 
           y = gi_star)) +
  geom_line() +
  theme_light()
  
  plot<- SRENGSENG_SAWAH_plot+ggtitle("Local Gi* values of SRENGSENG SAWAH")

  ggplotly(plot)  

```

```{r}
SRENGSENG_SAWAH %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

Based on the output above, we can observe that the tau is positive suggesting that the trend is increasing. The sl, which represents the p-value, is smaller than our level of significance, 0.05. Hence, we reject the null hypothesis as there is significant evidence that an increasing trend is present in the time series data.

### 7.1.3 For KOJA

```{r}
KOJA <- gi_stars %>% 
  ungroup() %>% 
  filter(Sub_District == "KOJA") |> 
  select(Sub_District, Date, gi_star)

KOJA_plot<- ggplot(data = KOJA, 
       aes(x = Date, 
           y = gi_star)) +
  geom_line() +
  theme_light()

  plot<- KOJA_plot+ggtitle("Local Gi* values of KOJA")

  ggplotly(plot)  
  
```

```{r}
KOJA %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

Based on the output above, we can observe that the tau is slightly positive, suggesting that the trend is increasing. The sl, which represents the p-value, is larger than our level of significance, 0.05. Hence, we do reject the null hypothesis as there is insufficient evidence that a trend is present in the time series data.

# 7.2 Performing Emerging Hotspot Analysis

Before we perform our emerging hotspot anlaysis, we will be using the new columns created earlier on to manually calculate the local Gi\* for each location. We can do this by grouping by Date and using [local_gstar_perm()](https://sfdep.josiahparry.com/reference/local_gstar.html) of sfdep package. After which, we use [unnest()](https://tidyr.tidyverse.org/reference/nest.html) to unnest gi_star column of the newly created gi_start data.frame.

```{r}
ehsa <- gi_stars %>%
  group_by(Sub_District) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)

emerging <- ehsa %>% 
  arrange(sl, abs(tau)) %>% 
  slice(1:5)
```

We will then perform EHSA analysis by using [emerging_hotspot_analysis()](https://pro.arcgis.com/en/pro-app/latest/tool-reference/space-time-pattern-mining/learnmoreemerging.htm) of sfdep package. It takes a spacetime object x (i.e. vr_st), and the quoted name of the variable of interest (i.e. Vaccination_Rate) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed(we will be performing 100).

```{r}
ehsa <- emerging_hotspot_analysis(
  x = vr_st, 
  .var = "Vaccination_Rate", 
  k = 1, 
  nsim = 99
)
```

### 7.2.1 Visualising the distribution of EHSA classes

In the code chunk below, [ggplot2](https://ggplot2.tidyverse.org/) functions is used to reveal the distribution of EHSA classes as a bar chart.

```{r}
ggplot(data = ehsa,
       aes(x = classification)) +
  geom_bar()
```

Below is a table explaining what each class means.

| EHSA Class Name      | Definition                                                                                                                                                                                                                                                                                                                 |
|----------------------|--------------------------------------------------|
| Oscillating coldspot | A statistically significant cold spot for the final time-step interval that has a history of also being a statistically significant hot spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant cold spots.                                                     |
| Oscillating hotspot  | A statistically significant hot spot for the final time-step interval that has a history of also being a statistically significant cold spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant hot spots.                                                      |
| Sporadic cold spot   | A statistically significant cold spot for the final time-step interval with a history of also being an on-again and off-again cold spot. Less than 90 percent of the time-step intervals have been statistically significant cold spots and none of the time-step intervals have been statistically significant hot spots. |

source: [esri](https://pro.arcgis.com/en/pro-app/latest/tool-reference/space-time-pattern-mining/learnmoreemerging.htm#:~:text=Oscillating%20Hot%20Spot,been%20statistically%20significant%20hot%20spots.)

### 7.2.2 Visualising EHSA

Before we can visualise EHSA, we need to join both copy_cjkt_df and ehsa together by using the code chunk below.

```{r}
cjkt_ehsa <- copy_cjkt_df %>%
  left_join(ehsa,
            by = c("Sub_District" = "location"))

```

The code chunk below plots a categorical choropleth map for the significant(p-value\<0.05) locations using tmap functions.

```{r}
ehsa_sig <- cjkt_ehsa  %>%
  filter(p_value < 0.05)

tmap_mode("plot")
tm_shape(cjkt_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4) + 
  tm_layout(
     main.title="Significant Classifications of EHSA",
     main.title.size = 0.7)
```

### 7.2.3 Statistical Conclusion

Based on the graph visualising **the distribution of EHSA classes**, we can observe:

1.  Oscillating hot spots has the highest numbers of sub-districts in Jakarta.

2.  As there is more oscillating hot spots than oscillating cold spots, it suggests that the data has an overall positive trend. This can be explained by the increasing vaccination rates across our study time period.

Based on the **Significant Classifications of EHSA map**, we can observe that

1.  Majority of the Oscillating hot spots tend to be spread around the central area of Jakarta.

2.  Sporadic cold spots are more clustered around the central area of Jakarta.

3.  Oscillating cold spots are quite spread out and tend to represent only 1 sub district

4.  There seem to only be 1 sub-district in the central area where no patterns are detected.

# 8. Acknowledgment

To conclude, I would like to thank Prof. Kam for our IS415 Geospatial Analytics and Applications course materials & resources. I would also like to thank my seniors, [Xiao Rong Wong](https://rpubs.com/xiaorongw/IS415_Take-home_Ex02) and [Megan Sim](https://is415-msty.netlify.app/posts/2021-09-10-take-home-exercise-1/#joining-eda) as I have referenced their codes.
