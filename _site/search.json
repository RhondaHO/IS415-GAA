[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "Welcome to IS415 Geospatial Analytics and Applications.\nThis is the course website of IS415 I study this term. You will find my course work on this website."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "",
    "text": "Show the code\npacman::p_load(sf, tidyverse, funModeling)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#task-2",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#task-2",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "Task 2",
    "text": "Task 2\nUsing appropriate tidyr and dplyr methods, derive the proportion of functional and non-functional water point at LGA level.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`new_georeferenced_column_`)\nwp_nga\n\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)\n\n\n#exclude redundant fields\nNGA <- NGA %>%\n  select(3:4, 8:9)\n\n\n#check for duplicate names\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n\n#lets correct the errors (suppose to manually find)\n\n\n#check frequency count\n\nfreq(data = wp_sf,\n     input = \"status_clean\")\n\n\n#replace NA fills with unknown\nwp_sf_nga <- wp_sf%>%\n  rename(status_clean = 'status_clean') %>%\n    select(status_clean) %>%\n    mutate(status_clean = replace_na(\n           status_clean, \"unknown\"))\n\n\n#filter out the NA values, extract functional water output\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Functional\",\n             \"Functional, needs repair\",\n             \"Functional, not in use\"\n           ))\n\n\n#extract non functional\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Abandoned/Decommissioned\",\n             \"Non-Functional\",\n             \"Non-Functional, dry\"\n           ))\n\n\n#extract unknown\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\n\n# extra step to cross check whether previously extracted correctly\nNGA_wp <- NGA %>%\n  mutate(`total_wp`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_functional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_nonfunctional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_unknown`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  ))\n\n\n#save in rds format (rds allow us to retain the data structure/simple feature with the data properties)\n\nwrite_rds(NGA_wp, \"data/rds/NGA_wp.rds\")\n\n\nst_geometry(geoNGA)\nglimpse(geoNGA)\nhead(geoNGA, n=5)\n\nst_geometry(nga)\nglimpse(nga)\nhead(nga, n=5)\n\nConvert into a simple dataframe feature\n\nwpdx_sf <- st_as_sf(wpdx, \n                       coords = c(\"lat_deg\", \"lon_deg\"),\n                       crs=4326) %>% st_transform(crs = 3414)\n\nTake a look at the dataframe.\n\nglimpse(wpdx_sf)\n\nReplace NA values of status_clean to empty string and then mutate the dataframe so that functional = 1, non-functional = 0.\n\nwpdx_sf <- replace(wpdx_sf['status_clean'], is.na(wpdx_sf['status_clean']), \"\")\n\nwpdx_sf %>%\n select(status_clean) %>%\n mutate(\n  functional = case_when(status_clean!=\"\" ~ 1,\n                         status_clean==\"\" ~ 0\n                         )\n )\n\nrrr\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) +\n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n    color=\"red\",\n    linetype=\"dashed\",\n    \n  )))\n\n\nwpdx_sf$functional<-as.numeric(wpdx_sf$functional)\n#hist(wpdx_sf$`functional`)gg"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "Install and load the required packages.\n\ninstall.packages(\"pacman\")\n\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data\n\nImport data from the following geospatial data into R by using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format\nCyclingPath, a line feature layer in ESRI shapefile format\nPreSchool, a point feature layer in kml file format\n\n\n\nmpsz = st_read(dsn = \"data/geospatial\", \n               layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\RhondaHO\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncyclingpath = st_read(dsn = \"data/geospatial\",\n                      layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\RhondaHO\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2248 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\npreschool = st_read(\"data/geospatial/pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\RhondaHO\\IS415-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nNote that when the input geospatial data is in shapefile format, two arguments will be used, namely: dsn to define the data path and layer to provide the shapefile name. Also note that no extension such as .shp, .dbf, .prj and .shx are needed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#checking-content-of-a-simple-feature-dataframe",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#checking-content-of-a-simple-feature-dataframe",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Checking Content of a Simple Feature Dataframe",
    "text": "Checking Content of a Simple Feature Dataframe\n\nRetrieve information related to the content of a simple feature data frame using methods:\n\nst_geometry()\nglimpse()\nhead()\n\n\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\nhead(mpsz, n=5) \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Plotting the Geospatial Data",
    "text": "Plotting the Geospatial Data\n\nTo visualise the geospatial features, make use of plot() of R Graphic.\n\n\nplot(mpsz)\n\n\n\n\n\nTo plot only the geometry.\n\n\nplot(st_geometry(mpsz))\n\n\nPlot the sf object by using a specific attribute.\n\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#assigning-epsg-code-to-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#assigning-epsg-code-to-a-simple-feature-data-frame",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Assigning EPSG code to a simple feature data frame",
    "text": "Assigning EPSG code to a simple feature data frame\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nThis is an example the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code chunk below.\n\nst_crs(mpsz)\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\nst_crs(mpsz3414)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#transforming-the-projection-of-preschool-from-wgs84-to-svy21",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#transforming-the-projection-of-preschool-from-wgs84-to-svy21",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Transforming the projection of preschool from wgs84 to svy21",
    "text": "Transforming the projection of preschool from wgs84 to svy21\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-and-converting-an-aspatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-and-converting-an-aspatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing and Converting An Aspatial Data",
    "text": "Importing and Converting An Aspatial Data\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\nlistings <- read_csv(\"chap01/data/aspatial/listings.csv\")\nlist(listings)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#creating-a-simple-feature-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#creating-a-simple-feature-data-frame-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Creating a simple feature data frame from an aspatial data frame",
    "text": "Creating a simple feature data frame from an aspatial data frame\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#geoprocessing-with-sf-package",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Geoprocessing with sf package",
    "text": "Geoprocessing with sf package\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths. This is followed by calculating the area of the buffers as shown in the code chunk below. Lastly, sum() of Base R will be used to derive the total land involved.\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\nbuffer_cycling$AREA <- st_area(buffer_cycling)\nsum(buffer_cycling$AREA)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#point-in-polygon-count",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#point-in-polygon-count",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Point-in-polygon count",
    "text": "Point-in-polygon count\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\nsummary(mpsz3414$`PreSch Count`)\ntop_n(mpsz3414, 1, `PreSch Count`)\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#explorotary-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#explorotary-data-analysis-eda",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Explorotary Data Analysis (EDA)",
    "text": "Explorotary Data Analysis (EDA)\n\nHistogram\n\n\nhist(mpsz3414$`PreSch Density`)\n\n\nCustomisable Histogram\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\nScatterplot\n\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#task-3",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#task-3",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "Task 3",
    "text": "Task 3\nCombining the geospatial and aspatial data frame into simple feature data frame."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#task-4",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#task-4",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "Task 4",
    "text": "Task 4\nVisualising the distribution of water point by using appropriate analytical visualisation methods.\n\nplot(nga)\n\n\nplot(geoNGA)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-onEx02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-onEx02.html",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called **tmap** package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-onEx02.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-onEx02.html#getting-started",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-onEx02.html#importing-of-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-onEx02.html#importing-of-data",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "3. Importing of Data",
    "text": "3. Importing of Data\n\n3.1 Importing Geospatial Data into R\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nmpsz\n\n\n\n3.2 Importing Attribute Data into R\n\npopdata <- read_csv(\"data/aspatial/respopagesexfa2011to2020.csv\")\n\n\n\n3.3 Data Preparation\n\n\n3.3.1 Data Wrangling\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\nJoining the attribute data and geospatial data\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\n#left join: join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n#write it to a new file\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-onEx02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-onEx02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "4. Choropleth Mapping Geospatial Data Using tmap",
    "text": "4. Choropleth Mapping Geospatial Data Using tmap\n\n4.1 Plotting a choropleth map quickly by using qtm()\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n4.2 Creating a choropleth map by using tmap’s elements\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n4.2.1 Drawing a base map\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n4.2.2 Drawing a choropleth map using tm_polygons()\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n4.2.3 Drawing a choropleth map using tm_fill() and *tm_border()**\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n# to add boundaries\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n#Beside alpha argument, there are three other arguments for tm_borders(), they are: col = border colour, lwd = border line width. The default is 1, and lty = border line type. The default is “solid”\n\n\n\n\n4.3 Data classification methods of tmap\n\n4.3.1 Plotting choropleth maps with built-in classification methods\n\n\nShow the code\n# quantile data classification\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n# equal data classification\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n# Notice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n\n\n4.3.2 Plotting choropleth map with custome break\n\n\nShow the code\nsummary(mpsz_pop2020$DEPENDENCY)\n\n#With reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n4.4 Colour Scheme\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n# add a - to reverse color scheme\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n4.5 Map Layouts\n\n4.5.1 Map Legend\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n4.5.2 Map style\n\n## classic \ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n4.5.3 Cartographic Furniture\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\ntmap_style(\"white\")\n\n\n\n\n4.6 Drawing Small Multiple Choropleth Maps\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n4.6.1 By assigning multiple values to at least one of the aesthetic arguments\n\n# small multiple choropleth maps are created by defining ncols in tm_fill()\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n# small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n4.6.2 By defining a group-by variable in tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n4.6.3 By creating multiple stand-alone maps with tmap_arrange()\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n4.7 Mappping Spatial Object Meeting a Selection Criterion\nnstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-onEx02.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-onEx02.html#reference",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "5 Reference",
    "text": "5 Reference\nReference"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "",
    "text": "Show the code\npacman::p_load(sf, tidyverse, tmap)\n\nNGA_wp <- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#handling-geospatial-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#handling-geospatial-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "2. Handling Geospatial Data",
    "text": "2. Handling Geospatial Data\n\n2.1 Importing Geospatial\nRead the file from geoBoundaries.\n\n\nShow the code\ngeoNGA = st_read(dsn = \"data/geospatial/\", layer=\"geoBoundaries-NGA-ADM2\")%>%st_transform(crs=26392)\n\n#transforms data from decimal to metres\n\n\nRead the file from Humanitarian Data Exchange.\n\n\nShow the code\nNGA = st_read(dsn = \"data/geospatial/\", layer=\"nga_admbnda_adm2_osgof_20190417\")%>%st_transform(crs=26392)\n\n\n\n\n2.2 Handling Aspatial Data\nRead the file from and filter out where country name is Nigeria\n\n\nShow the code\nwp_nga <- read_csv(\"data/aspatial/wpdx.csv\") %>%\n  filter(clean_country_name %in% c(\"Nigeria\"))\n\n\nConverting an aspatial data into an sf data.frame involves two steps.\nFirst, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`new_georeferenced_column_`)\nwp_nga\n\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\n#Transforming into Nigeria projected coordinate system\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-data-cleaning",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-data-cleaning",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "3. Geospatial Data Cleaning",
    "text": "3. Geospatial Data Cleaning\nNGA sf data.frame consists of many redundent fields. The code chunk below uses select() of dplyr to retain column 3, 4, 8 and 9. Do you know why?\n\n3.1 Exclude redundant fields\n\n#exclude redundant fields\nNGA <- NGA %>%\n  select(3:4, 8:9)\n\n\n\n3.2 Checking for duplicate name\n\n#check for duplicate names\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n\n#lets correct the errors (suppose to manually find)\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nNow, let us rerun the code chunk below to confirm that the duplicated name issue has been addressed.\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-wrangling-for-water-point-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-wrangling-for-water-point-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4. Data Wrangling for Water Point Data",
    "text": "4. Data Wrangling for Water Point Data\nExploratory Data Analysis (EDA) is a popular approach to gain initial understanding of the data. In the code chunk below, freq() of funModeling package is used to reveal the distribution of water point status visually.\n\n#check frequency count\n\nfreq(data = wp_sf,\n     input = \"status_clean\")\n\n\n#replace NA fills with unknown\nwp_sf_nga <- wp_sf%>%\n  rename(status_clean = 'status_clean') %>%\n    select(status_clean) %>%\n    mutate(status_clean = replace_na(\n           status_clean, \"unknown\"))\n\n\n4.1 Extracting Water Point Data\n\n#filter out the NA values, extract functional water output\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Functional\",\n             \"Functional, needs repair\",\n             \"Functional, not in use\"\n           ))\n\n\n#extract non functional\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Abandoned/Decommissioned\",\n             \"Non-Functional\",\n             \"Non-Functional, dry\"\n           ))\n\n\n#extract unknown\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\nNext, the code chunk below is used to perform a quick EDA on the derived sf data.frames.\n\nfreq(data = wp_functional,\n     input = 'status_clean')\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\nfreq(data = wp_unknown,\n     input = 'status_clean')\n\n\n\n4.2 Performing Point-in-Polygon Count\nNext, we want to find out the number of total, functional, nonfunctional and unknown water points in each LGA. This is performed in the following code chunk. First, it identifies the functional water points in each LGA by using st_intersects() of sf package. Next, length() is used to calculate the number of functional water points that fall inside each LGA.\n\n# extra step to cross check whether previously extracted correctly\nNGA_wp <- NGA %>%\n  mutate(`total_wp`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_functional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_nonfunctional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_unknown`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  ))\n\n\n\n4.3 Visualing attributes by using statistical graphs\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n#save in rds format (rds allow us to retain the data structure/simple feature with the data properties)\n\nwrite_rds(NGA_wp, \"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#basic-chloropleth-map",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#basic-chloropleth-map",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "2. Basic Chloropleth Map",
    "text": "2. Basic Chloropleth Map\n\n\nShow the code\np1 <- tm_shape(NGA_wp) + \n  tm_fill(\"wp_functional\",\n          n=10, #10 classes\n          style=\"equal\", #classification method\n          palette=\"Blues\") + #always plural form\n  tm_borders(lwd = 0.1, #border thickness\n             alpha= 1) +\n  tm_layout(main.title = \"Distribution of functional water points\",\n            legend.outside = FALSE)\n\np2 <- tm_shape(NGA_wp) + \n  tm_fill(\"total_wp\",\n          n=10, #10 classes\n          style=\"equal\", #classification method\n          palette=\"Blues\") + #always plural form\n  tm_borders(lwd = 0.1, #border thickness\n             alpha= 1) +\n  tm_layout(main.title = \"Distribution of total water points\",\n            legend.outside = FALSE)\n\n\n\n\nShow the code\ntmap_arrange(p2,p1,nrow=1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#chloropleth-maps-for-rates",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#chloropleth-maps-for-rates",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "3. Chloropleth Maps for Rates",
    "text": "3. Chloropleth Maps for Rates\n\n3.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\n\n\nShow the code\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\n3.2 Plotting map of rate\n\n\nShow the code\ntm_shape(NGA_wp) + \n  tm_fill(\"pct_functional\",\n          n=10, \n          style=\"equal\", \n          palette=\"Reds\") +\n  tm_borders(lwd = 0.1, \n             alpha= 1) +\n  tm_layout(main.title = \"Rate map of functional water points\",\n            legend.outside = FALSE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#extreme-value-maps",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#extreme-value-maps",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "4. Extreme Value Maps",
    "text": "4. Extreme Value Maps\n\n4.1 Percentile Map\n\n\n4.1.1 Data Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\n\nShow the code\n#NGA_wp <- NGA_wp %>%\n  #drop_na()\n\nNGA_wp <- NGA_wp %>%\n  na.omit()\n\n\n\n\n4.1.2 Why writing functions?\nStep 2: Creating customised classification and extracting values\n\n\nShow the code\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n\n\n\n4.1.3 Creating the get.var function\nWriting a function has three big advantages over using copy-and-paste:\nYou can give a function an evocative name that makes your code easier to understand. As requirements change, you only need to update code in one place, instead of many. You eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\n\nShow the code\nget.var <- function(vname,df){\n  v <- df[name] %>%\n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n\n4.1.4 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\n\nShow the code\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\n4.1.5 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\n\nShow the code\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n4.2 Box Map\n\n\nShow the code\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n4.2.1 Creating the boxbreaks function\n\n\nShow the code\nboxbreaks <- function(v,mult=1.5) {\n  qv <- unname(quantile(v))\n  iqr <- qv[4] - qv[2]\n  upfence <- qv[4] + mult * iqr\n  lofence <- qv[2] - mult * iqr\n  # initialize break points vector\n  bb <- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence < qv[1]) {  # no lower outliers\n    bb[1] <- lofence\n    bb[2] <- floor(qv[1])\n  } else {\n    bb[2] <- lofence\n    bb[1] <- qv[1]\n  }\n  if (upfence > qv[5]) { # no upper outliers\n    bb[7] <- upfence\n    bb[6] <- ceiling(qv[5])\n  } else {\n    bb[6] <- upfence\n    bb[7] <- qv[5]\n  }\n  bb[3:5] <- qv[2:4]\n  return(bb)\n}\n\n\n\n\n4.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\narguments: vname: variable name (as character, in quotes) df: name of sf data frame returns: v: vector with values (without a column name)\n\n\nShow the code\nget.var <- function(vname,df) {\n  v <- df[vname] %>% st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n\n4.2.3 Test drive the newly created function\nLet’s test the newly created function.\n\n\nShow the code\nvar <- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n\n\n\n4.2.4 Boxmap function\n\n\nShow the code\nboxmap <- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var <- get.var(vnam,df)\n  bb <- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"< 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"> 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)\n\n\n\n\n4.2.5 Recode zero\nThe code chunk below is used to recode LGAs with zero total water point into NA.\n\n\nShow the code\nNGA_wp <- NGA_wp %>%\n  mutate(wp_functional = na_if(\n    total_wp, total_wp < 0))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#getting-started",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n\nShow the code\npacman::p_load(maptools, sf, raster, spatstat, tmap)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#spatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#spatial-data-wrangling",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "3. Spatial Data Wrangling",
    "text": "3. Spatial Data Wrangling\n\n3.1 Importing the spatial data\n\n\nShow the code\nchildcare_sf <- st_read(\"data/child-care-services-geojson.geojson\") %>%\n  st_transform(crs = 3414)\n\n\n\n\nShow the code\nsg_sf <- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\n\n\n\nShow the code\nmpsz_sf <- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\n\n\n3.2 Mapping the geospatial data sets\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\nShow the code\ntmap_mode('plot')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "4. Geospatial Data wrangling",
    "text": "4. Geospatial Data wrangling\nThe first 3 steps are important for dealing with geospatial data wrangling. Source data must be in sf., source data needs to be in the same projection system as once it is converted, cannot tell.\n\n4.1 Converting sf data frames to sp’s Spatial* class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\n\nShow the code\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\n\n\n4.2 Creating a choropleth map by using tmap’s elements\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nShow the code\nchildcare\n\n\n\n\nShow the code\nmpsz\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.3 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\n\n\nShow the code\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\n\n\nShow the code\nchildcare_sp\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.4 Converting the generic sp format into spatstat’s ppp format\n\n\nShow the code\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nplot(childcare_ppp)\n\n\n\n\nShow the code\nsummary(childcare_ppp)\n\n\n\n\n4.5 Handling duplicated points\n\n\nShow the code\n#check for duplication\nany(duplicated(childcare_ppp))\n\n#count the number of co-indicence point\nmultiplicity(childcare_ppp)\n\n#how many locations have more than one point event\nsum(multiplicity(childcare_ppp) > 1)\n\n\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\nShow the code\ntmap_mode('plot')\n\n\n\n\nShow the code\nchildcare_ppp_jit <- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\n\n\nShow the code\nany(duplicated(childcare_ppp_jit))\n\n\n\n\n4.6 Creating owin object\n\n\nShow the code\nsg_owin <- as(sg_sp, \"owin\")\nplot(sg_owin)\n\n\n\n\nShow the code\nsummary(sg_owin)\n\n\n\n\n4.6 Combining point events object and owin object\nExtract childcare events that are located within Singapore\n\n\nShow the code\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nShow the code\nsummary(childcareSG_ppp)\n\n\n\n\n4.6 First-order Spatial Point Patterns Analysis\n\n4.6.1 Kernel Density Estimation\n\n4.6.1.1 Computing kernel density estimation using automatic bandwidth selection method\n\n\nShow the code\nkde_childcareSG_bw <- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\n\n\n\nShow the code\nplot(kde_childcareSG_bw)\n\n\nRetrieve the bandwidth used to compute the kde layer\n\n\nShow the code\nbw <- bw.diggle(childcareSG_ppp)\nbw\n\n\n\n\n4.6.1.2 Rescalling KDE values\nRescale() is used to covert the unit of measurement from meter to kilometer.\n\n\nShow the code\nchildcareSG_ppp.km <- rescale(childcareSG_ppp, 1000, \"km\")\n\n\n\n\nShow the code\nkde_childcareSG.bw <- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n4.6.2 Working with different automatic badwidth methods\nOther spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\n\n\nShow the code\nbw.CvL(childcareSG_ppp.km)\nbw.scott(childcareSG_ppp.km)\n\n#tends to produce the more appropriate values when the pattern consists predominantly of tight clusters\nbw.ppl(childcareSG_ppp.km)\n\n#Best Method to detect a single tight cluster in the midst of random noise\nbw.diggle(childcareSG_ppp.km)\n\n\nCompare the output of using bw.diggle and bw.ppl methods.\n\n\nShow the code\nkde_childcareSG.ppl <- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n4.6.3 Working with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\n\n\n\n\n4.7 Fixed and Adaptive KDE\n\n4.7.1 Computing KDE by using fixed bandwidth\n\n\nShow the code\nkde_childcareSG_600 <- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n4.7.2 Computing KDE by using adaptive bandwidth\nDerive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\n\nShow the code\nkde_childcareSG_adaptive <- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\n\nShow the code\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n4.7.3 Converting KDE output into grid object\n\n\nShow the code\ngridded_kde_childcareSG_bw <- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n4.7.3.1 Converting gridded output into raster\n\n\nShow the code\n#Convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\nkde_childcareSG_bw_raster <-raster(gridded_kde_childcareSG_bw)\n\nkde_childcareSG_bw_raster\n\n\n\n\n4.7.3.2 Assigning projection systems\n\n\nShow the code\nprojection(kde_childcareSG_bw_raster) <-CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\n\n\n\n4.7.4 Visualising the output in tmap\n\n\nShow the code\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n4.7.5 Comparing Spatial Point Patterns using KDE\nCompare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n4.7.5.1 Extracting study area\n\n\nShow the code\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\n\nPlotting of target planning areas\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")\n\n\n\n\n4.7.5.2 Converting the spatial point data frame into generic sp format\n\n\nShow the code\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\n\n\n\n4.7.5.3 Creating owin object\n\n\nShow the code\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\n\n\n\n\n4.7.5.4 Combining childcare points and the study area\n\n\nShow the code\n#Extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n# Next, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n#The code chunk below is used to plot these four study areas and the locations of the childcare centres.\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n4.7.5.5 Computing KDE\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n4.7.5.6 Computing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth.\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\n\n\n4.8 Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used. #### 4.8.1 Testing spatial point patterns using Clark and Evans Test\n\n\nShow the code\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n\n4.8.2 Clark and Evans Test: Choa Chu Kang planning area\n\n\nShow the code\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n\n\n4.8.3 Clark and Evans Test: Tampines planning area\n\n\nShow the code\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#basic-chloropleth-map",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#basic-chloropleth-map",
    "title": "In-class Exercise 4:",
    "section": "2. Basic Chloropleth Map",
    "text": "2. Basic Chloropleth Map\n\n\nShow the code\np1 <- tm_shape(NGA_wp) + \n  tm_fill(\"wp_functional\",\n          n=10, #10 classes\n          style=\"equal\", #classification method\n          palette=\"Blues\") + #always plural form\n  tm_borders(lwd = 0.1, #border thickness\n             alpha= 1) +\n  tm_layout(main.title = \"Distribution of functional water points\",\n            legend.outside = FALSE)\n\np2 <- tm_shape(NGA_wp) + \n  tm_fill(\"total_wp\",\n          n=10, #10 classes\n          style=\"equal\", #classification method\n          palette=\"Blues\") + #always plural form\n  tm_borders(lwd = 0.1, #border thickness\n             alpha= 1) +\n  tm_layout(main.title = \"Distribution of total water points\",\n            legend.outside = FALSE)\n\n\n\n\nShow the code\ntmap_arrange(p2,p1,nrow=1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#chloropleth-maps-for-rates",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#chloropleth-maps-for-rates",
    "title": "In-class Exercise 4:",
    "section": "3. Chloropleth Maps for Rates",
    "text": "3. Chloropleth Maps for Rates\n\n3.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\n\n\nShow the code\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\n3.2 Plotting map of rate\n\n\nShow the code\ntm_shape(NGA_wp) + \n  tm_fill(\"pct_functional\",\n          n=10, \n          style=\"equal\", \n          palette=\"Reds\") +\n  tm_borders(lwd = 0.1, \n             alpha= 1) +\n  tm_layout(main.title = \"Rate map of functional water points\",\n            legend.outside = FALSE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#extreme-value-maps",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#extreme-value-maps",
    "title": "In-class Exercise 4:",
    "section": "4. Extreme Value Maps",
    "text": "4. Extreme Value Maps\n\n4.1 Percentile Map\n\n4.1.1 Data Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\n\nShow the code\n#NGA_wp <- NGA_wp %>%\n  #drop_na()\n\nNGA_wp <- NGA_wp %>%\n  na.omit()\n\n\n\n\n4.1.2 Why writing functions?\nStep 2: Creating customised classification and extracting values\n\n\nShow the code\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n\n\n\n4.1.3 Creating the get.var function\nWriting a function has three big advantages over using copy-and-paste:\nYou can give a function an evocative name that makes your code easier to understand. As requirements change, you only need to update code in one place, instead of many. You eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\n\nShow the code\nget.var <- function(vname,df){\n  v <- df[name] %>%\n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n\n4.1.4 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\n\nShow the code\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\n4.1.5 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\n\nShow the code\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n4.2 Box Map\n\n\nShow the code\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n4.2.1 Creating the boxbreaks function\n\n\nShow the code\nboxbreaks <- function(v,mult=1.5) {\n  qv <- unname(quantile(v))\n  iqr <- qv[4] - qv[2]\n  upfence <- qv[4] + mult * iqr\n  lofence <- qv[2] - mult * iqr\n  # initialize break points vector\n  bb <- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence < qv[1]) {  # no lower outliers\n    bb[1] <- lofence\n    bb[2] <- floor(qv[1])\n  } else {\n    bb[2] <- lofence\n    bb[1] <- qv[1]\n  }\n  if (upfence > qv[5]) { # no upper outliers\n    bb[7] <- upfence\n    bb[6] <- ceiling(qv[5])\n  } else {\n    bb[6] <- upfence\n    bb[7] <- qv[5]\n  }\n  bb[3:5] <- qv[2:4]\n  return(bb)\n}\n\n\n\n\n4.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\narguments: vname: variable name (as character, in quotes) df: name of sf data frame returns: v: vector with values (without a column name)\n\n\nShow the code\nget.var <- function(vname,df) {\n  v <- df[vname] %>% st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n\n4.2.3 Test drive the newly created function\nLet’s test the newly created function.\n\n\nShow the code\nvar <- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n\n\n\n4.2.4 Boxmap function\n\n\nShow the code\nboxmap <- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var <- get.var(vnam,df)\n  bb <- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"< 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"> 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)\n\n\n\n\n4.2.5 Recode zero\nThe code chunk below is used to recode LGAs with zero total water point into NA.\n\n\nShow the code\nNGA_wp <- NGA_wp %>%\n  mutate(wp_functional = na_if(\n    total_wp, total_wp < 0))"
  },
  {
    "objectID": "Take-home_Assgn/In-class_Ex02_TOEDIT.html",
    "href": "Take-home_Assgn/In-class_Ex02_TOEDIT.html",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "",
    "text": "Show the code\npacman::p_load(sf, tidyverse, funModeling)"
  },
  {
    "objectID": "Take-home_Assgn/In-class_Ex02_TOEDIT.html#handling-geospatial-data",
    "href": "Take-home_Assgn/In-class_Ex02_TOEDIT.html#handling-geospatial-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "2. Handling Geospatial Data",
    "text": "2. Handling Geospatial Data\n\n2.1 Importing Geospatial\nRead the file from geoBoundaries.\n\n\nShow the code\ngeoNGA = st_read(dsn = \"data/geospatial/\", layer=\"geoBoundaries-NGA-ADM2\")%>%st_transform(crs=26392)\n\n#transforms data from decimal to metres\n\n\nRead the file from Humanitarian Data Exchange.\n\n\nShow the code\nNGA = st_read(dsn = \"data/geospatial/\", layer=\"nga_admbnda_adm2_osgof_20190417\")%>%st_transform(crs=26392)\n\n\n\n\n2.2 Handling Aspatial Data\nRead the file from and filter out where country name is Nigeria\n\n\nShow the code\nwp_nga <- read_csv(\"data/aspatial/wpdx.csv\") %>%\n  filter(clean_country_name %in% c(\"Nigeria\"))\n\n\nConverting an aspatial data into an sf data.frame involves two steps.\nFirst, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`new_georeferenced_column_`)\nwp_nga\n\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\n#Transforming into Nigeria projected coordinate system\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "Take-home_Assgn/In-class_Ex02_TOEDIT.html#geospatial-data-cleaning",
    "href": "Take-home_Assgn/In-class_Ex02_TOEDIT.html#geospatial-data-cleaning",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "3. Geospatial Data Cleaning",
    "text": "3. Geospatial Data Cleaning\nNGA sf data.frame consists of many redundent fields. The code chunk below uses select() of dplyr to retain column 3, 4, 8 and 9. Do you know why?\n\n3.1 Exclude redundant fields\n\n#exclude redundant fields\nNGA <- NGA %>%\n  select(3:4, 8:9)\n\n\n\n3.2 Checking for duplicate name\n\n#check for duplicate names\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n\n#lets correct the errors (suppose to manually find)\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nNow, let us rerun the code chunk below to confirm that the duplicated name issue has been addressed.\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]"
  },
  {
    "objectID": "Take-home_Assgn/In-class_Ex02_TOEDIT.html#data-wrangling-for-water-point-data",
    "href": "Take-home_Assgn/In-class_Ex02_TOEDIT.html#data-wrangling-for-water-point-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4. Data Wrangling for Water Point Data",
    "text": "4. Data Wrangling for Water Point Data\nExploratory Data Analysis (EDA) is a popular approach to gain initial understanding of the data. In the code chunk below, freq() of funModeling package is used to reveal the distribution of water point status visually.\n\n#check frequency count\n\nfreq(data = wp_sf,\n     input = \"status_clean\")\n\n\n#replace NA fills with unknown\nwp_sf_nga <- wp_sf%>%\n  rename(status_clean = 'status_clean') %>%\n    select(status_clean) %>%\n    mutate(status_clean = replace_na(\n           status_clean, \"unknown\"))\n\n\n4.1 Extracting Water Point Data\n\n#filter out the NA values, extract functional water output\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Functional\",\n             \"Functional, needs repair\",\n             \"Functional, not in use\"\n           ))\n\n\n#extract non functional\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Abandoned/Decommissioned\",\n             \"Non-Functional\",\n             \"Non-Functional, dry\"\n           ))\n\n\n#extract unknown\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\nNext, the code chunk below is used to perform a quick EDA on the derived sf data.frames.\n\nfreq(data = wp_functional,\n     input = 'status_clean')\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\nfreq(data = wp_unknown,\n     input = 'status_clean')\n\n\n\n4.2 Performing Point-in-Polygon Count\nNext, we want to find out the number of total, functional, nonfunctional and unknown water points in each LGA. This is performed in the following code chunk. First, it identifies the functional water points in each LGA by using st_intersects() of sf package. Next, length() is used to calculate the number of functional water points that fall inside each LGA.\n\n# extra step to cross check whether previously extracted correctly\nNGA_wp <- NGA %>%\n  mutate(`total_wp`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_functional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_nonfunctional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_unknown`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  ))\n\n\n\n4.3 Visualing attributes by using statistical graphs\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n#save in rds format (rds allow us to retain the data structure/simple feature with the data properties)\n\nwrite_rds(NGA_wp, \"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#getting-started",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#getting-started",
    "title": "In-class Exercise 4",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n\nShow the code\npacman::p_load(maptools, sf, raster, spatstat, tmap)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#spatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#spatial-data-wrangling",
    "title": "In-class Exercise 4",
    "section": "3. Spatial Data Wrangling",
    "text": "3. Spatial Data Wrangling\n\n3.1 Importing the spatial data\n\n\nShow the code\nchildcare_sf <- st_read(\"data/child-care-services-geojson.geojson\") %>%\n  st_transform(crs = 3414)\n\n\nReading layer `child-care-services-geojson' from data source \n  `C:\\RhondaHO\\IS415-GAA\\In-class_Ex\\In-class_Ex04\\data\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nShow the code\nsg_sf <- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\n\n\n\nShow the code\nmpsz_sf <- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\RhondaHO\\IS415-GAA\\In-class_Ex\\In-class_Ex04\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n3.2 Mapping the geospatial data sets\nTo adjust tmap view settings refer to here.\n\n\nShow the code\ntmap_mode('view') #by default\n\ntm_shape(childcare_sf) +\n  tm_dots(alph=0.5, # intensity of color\n          size=0.01) + #tm_bubbles alternate method of dots, tend to use if we want to create proportional maps\n  tm_view(\n  set.zoom.limits= c(11,14)) # set zoom limits (zoom out value, zoom in value)\n\n\n\n\n\n\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\nShow the code\ntmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geospatial-data-wrangling",
    "title": "In-class Exercise 4",
    "section": "4. Geospatial Data wrangling",
    "text": "4. Geospatial Data wrangling\nThe first 3 steps are important for dealing with geospatial data wrangling. Source data must be in sf., source data needs to be in the same projection system as once it is converted, cannot tell.\n\n4.1 Converting sf data frames to sp’s Spatial* class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\n\nShow the code\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\n\n\nShow the code\nchildcare\n\n\n\n\nShow the code\nmpsz\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.2 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\n\n\nShow the code\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\n\n\nShow the code\nchildcare_sp\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.3 Converting the generic sp format into spatstat’s ppp format\n\n\nShow the code\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nplot(childcare_ppp)\n\n\n\n\nShow the code\nsummary(childcare_ppp)\n\n\n\n\n4.4 Handling duplicated points\n\n\nShow the code\n#check for duplication\nany(duplicated(childcare_ppp))\n\n#count the number of co-indicence point\nmultiplicity(childcare_ppp)\n\n#how many locations have more than one point event\nsum(multiplicity(childcare_ppp) > 1)\n\n\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\nShow the code\ntmap_mode('plot')\n\n\n\n\nShow the code\nchildcare_ppp_jit <- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\n\n\nShow the code\nany(duplicated(childcare_ppp_jit))\n\n\n\n\n4.5 Creating owin object\n\n\nShow the code\nsg_owin <- as(sg_sp, \"owin\")\nplot(sg_owin)\n\n\n\n\nShow the code\nsummary(sg_owin)\n\n\n\n\n4.6 Combining point events object and owin object\nExtract childcare events that are located within Singapore\n\n\nShow the code\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nShow the code\nsummary(childcareSG_ppp)\n\n\n\n\n4.6 First-order Spatial Point Patterns Analysis\n\n4.6.1 Kernel Density Estimation\n\n4.6.1.1 Computing kernel density estimation using automatic bandwidth selection method\n\n\nShow the code\nkde_childcareSG_bw <- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\n\n\n\nShow the code\nplot(kde_childcareSG_bw)\n\n\nRetrieve the bandwidth used to compute the kde layer\n\n\nShow the code\nbw <- bw.diggle(childcareSG_ppp)\nbw\n\n\n\n\n4.6.1.2 Rescalling KDE values\nRescale() is used to covert the unit of measurement from meter to kilometer.\n\n\nShow the code\nchildcareSG_ppp.km <- rescale(childcareSG_ppp, 1000, \"km\")\n\n\n\n\nShow the code\nkde_childcareSG.bw <- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n4.6.2 Working with different automatic badwidth methods\nOther spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\n\n\nShow the code\nbw.CvL(childcareSG_ppp.km)\nbw.scott(childcareSG_ppp.km)\n\n#tends to produce the more appropriate values when the pattern consists predominantly of tight clusters\nbw.ppl(childcareSG_ppp.km)\n\n#Best Method to detect a single tight cluster in the midst of random noise\nbw.diggle(childcareSG_ppp.km)\n\n\nCompare the output of using bw.diggle and bw.ppl methods.\n\n\nShow the code\nkde_childcareSG.ppl <- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n4.6.3 Working with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\n\n\n\n\n4.7 Fixed and Adaptive KDE\n\n4.7.1 Computing KDE by using fixed bandwidth\n\n\nShow the code\nkde_childcareSG_600 <- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n4.7.2 Computing KDE by using adaptive bandwidth\nDerive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\n\nShow the code\nkde_childcareSG_adaptive <- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\n\nShow the code\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n4.7.3 Converting KDE output into grid object\n\n\nShow the code\ngridded_kde_childcareSG_bw <- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n4.7.3.1 Converting gridded output into raster\n\n\nShow the code\n#Convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\nkde_childcareSG_bw_raster <-raster(gridded_kde_childcareSG_bw)\n\nkde_childcareSG_bw_raster\n\n\n\n\n4.7.3.2 Assigning projection systems\n\n\nShow the code\nprojection(kde_childcareSG_bw_raster) <-CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\n\n\n\n4.7.4 Visualising the output in tmap\n\n\nShow the code\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n4.7.5 Comparing Spatial Point Patterns using KDE\nCompare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n4.7.5.1 Extracting study area\n\n\nShow the code\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\n\nPlotting of target planning areas\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")\n\n\n\n\n4.7.5.2 Converting the spatial point data frame into generic sp format\n\n\nShow the code\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\n\n\n\n4.7.5.3 Creating owin object\n\n\nShow the code\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\n\n\n\n\n4.7.5.4 Combining childcare points and the study area\n\n\nShow the code\n#Extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n# Next, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n#The code chunk below is used to plot these four study areas and the locations of the childcare centres.\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n4.7.5.5 Computing KDE\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n4.7.5.6 Computing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth.\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\n\n\n4.8 Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used. #### 4.8.1 Testing spatial point patterns using Clark and Evans Test\n\n\nShow the code\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n\n4.8.2 Clark and Evans Test: Choa Chu Kang planning area\n\n\nShow the code\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n\n\n4.8.3 Clark and Evans Test: Tampines planning area\n\n\nShow the code\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assignment.html",
    "href": "Take-home_Assgn/Take-home_Assignment.html",
    "title": "Take-home Assignment 1",
    "section": "",
    "text": "Show the code\npacman::p_load(sf, tidyverse, funModeling)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assignment.html#handling-geospatial-data",
    "href": "Take-home_Assgn/Take-home_Assignment.html#handling-geospatial-data",
    "title": "Take-home Assignment 1",
    "section": "2. Handling Geospatial Data",
    "text": "2. Handling Geospatial Data\n\n2.1 Importing Geospatial\nRead the file from geoBoundaries.\n\n\nShow the code\ngeoNGA = st_read(dsn = \"data/geospatial/\", layer=\"geoBoundaries-NGA-ADM2\")%>%st_transform(crs=26392)\n\n#transforms data from decimal to metres\n\n\nRead the file from Humanitarian Data Exchange.\n\n\nShow the code\nNGA = st_read(dsn = \"data/geospatial/\", layer=\"nga_admbnda_adm2_osgof_20190417\")%>%st_transform(crs=26392)\n\n\n\n\n2.2 Handling Aspatial Data\nRead the file from and filter out where country name is Nigeria\n\n\nShow the code\nwp_nga <- read_csv(\"data/aspatial/wpdx.csv\") %>%\n  filter(clean_country_name %in% c(\"Nigeria\"))\n\n\nConverting an aspatial data into an sf data.frame involves two steps.\nFirst, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`new_georeferenced_column_`)\nwp_nga\n\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\n#Transforming into Nigeria projected coordinate system\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assignment.html#geospatial-data-cleaning",
    "href": "Take-home_Assgn/Take-home_Assignment.html#geospatial-data-cleaning",
    "title": "Take-home Assignment 1",
    "section": "3. Geospatial Data Cleaning",
    "text": "3. Geospatial Data Cleaning\nNGA sf data.frame consists of many redundent fields. The code chunk below uses select() of dplyr to retain column 3, 4, 8 and 9. Do you know why?\n\n3.1 Exclude redundant fields\n\n#exclude redundant fields\nNGA <- NGA %>%\n  select(3:4, 8:9)\n\n\n\n3.2 Checking for duplicate name\n\n#check for duplicate names\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n\n#lets correct the errors (suppose to manually find)\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nNow, let us rerun the code chunk below to confirm that the duplicated name issue has been addressed.\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assignment.html#data-wrangling-for-water-point-data",
    "href": "Take-home_Assgn/Take-home_Assignment.html#data-wrangling-for-water-point-data",
    "title": "Take-home Assignment 1",
    "section": "4. Data Wrangling for Water Point Data",
    "text": "4. Data Wrangling for Water Point Data\nExploratory Data Analysis (EDA) is a popular approach to gain initial understanding of the data. In the code chunk below, freq() of funModeling package is used to reveal the distribution of water point status visually.\n\n#check frequency count\n\nfreq(data = wp_sf,\n     input = \"status_clean\")\n\n\n#replace NA fills with unknown\nwp_sf_nga <- wp_sf%>%\n  rename(status_clean = 'status_clean') %>%\n    select(status_clean) %>%\n    mutate(status_clean = replace_na(\n           status_clean, \"unknown\"))\n\n\n4.1 Extracting Water Point Data\n\n#filter out the NA values, extract functional water output\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Functional\",\n             \"Functional, needs repair\",\n             \"Functional, not in use\"\n           ))\n\n\n#extract non functional\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Abandoned/Decommissioned\",\n             \"Non-Functional\",\n             \"Non-Functional, dry\"\n           ))\n\n\n#extract unknown\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\nNext, the code chunk below is used to perform a quick EDA on the derived sf data.frames.\n\nfreq(data = wp_functional,\n     input = 'status_clean')\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\nfreq(data = wp_unknown,\n     input = 'status_clean')\n\n\n\n4.2 Performing Point-in-Polygon Count\nNext, we want to find out the number of total, functional, nonfunctional and unknown water points in each LGA. This is performed in the following code chunk. First, it identifies the functional water points in each LGA by using st_intersects() of sf package. Next, length() is used to calculate the number of functional water points that fall inside each LGA.\n\n# extra step to cross check whether previously extracted correctly\nNGA_wp <- NGA %>%\n  mutate(`total_wp`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_functional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_nonfunctional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_unknown`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  ))\n\n\n\n4.3 Visualing attributes by using statistical graphs\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n#save in rds format (rds allow us to retain the data structure/simple feature with the data properties)\n\nwrite_rds(NGA_wp, \"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1.html",
    "href": "Take-home_Assgn/Take-home_Assgn1.html",
    "title": "Take-home Assignment 1",
    "section": "",
    "text": "Show the code\npacman::p_load(sf, tidyverse, funModeling)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1.html#handling-geospatial-data",
    "href": "Take-home_Assgn/Take-home_Assgn1.html#handling-geospatial-data",
    "title": "Take-home Assignment 1",
    "section": "2. Handling Geospatial Data",
    "text": "2. Handling Geospatial Data\n\n2.1 Importing Geospatial\nRead the file from geoBoundaries.\n\n\nShow the code\ngeoNGA = st_read(dsn = \"data/geospatial/\", layer=\"geoBoundaries-NGA-ADM2\")%>%st_transform(crs=26392)\n\n#transforms data from decimal to metres\n\n\nRead the file from Humanitarian Data Exchange.\n\n\nShow the code\nNGA = st_read(dsn = \"data/geospatial/\", layer=\"nga_admbnda_adm2_osgof_20190417\")%>%st_transform(crs=26392)\n\n\n\n\n2.2 Handling Aspatial Data\nRead the file from and filter out where country name is Nigeria\n\n\nShow the code\nwp_nga <- read_csv(\"data/aspatial/wpdx.csv\") %>%\n  filter(clean_country_name %in% c(\"Nigeria\"))\n\n\nConverting an aspatial data into an sf data.frame involves two steps.\nFirst, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`new_georeferenced_column_`)\nwp_nga\n\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\n#Transforming into Nigeria projected coordinate system\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1.html#geospatial-data-cleaning",
    "href": "Take-home_Assgn/Take-home_Assgn1.html#geospatial-data-cleaning",
    "title": "Take-home Assignment 1",
    "section": "3. Geospatial Data Cleaning",
    "text": "3. Geospatial Data Cleaning\nNGA sf data.frame consists of many redundent fields. The code chunk below uses select() of dplyr to retain column 3, 4, 8 and 9. Do you know why?\n\n3.1 Exclude redundant fields\n\n#exclude redundant fields\nNGA <- NGA %>%\n  select(3:4, 8:9)\n\n\n\n3.2 Checking for duplicate name\n\n#check for duplicate names\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n\n#lets correct the errors (suppose to manually find)\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nNow, let us rerun the code chunk below to confirm that the duplicated name issue has been addressed.\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1.html#data-wrangling-for-water-point-data",
    "href": "Take-home_Assgn/Take-home_Assgn1.html#data-wrangling-for-water-point-data",
    "title": "Take-home Assignment 1",
    "section": "4. Data Wrangling for Water Point Data",
    "text": "4. Data Wrangling for Water Point Data\nExploratory Data Analysis (EDA) is a popular approach to gain initial understanding of the data. In the code chunk below, freq() of funModeling package is used to reveal the distribution of water point status visually.\n\n#check frequency count\n\nfreq(data = wp_sf,\n     input = \"status_clean\")\n\n\n#replace NA fills with unknown\nwp_sf_nga <- wp_sf%>%\n  rename(status_clean = 'status_clean') %>%\n    select(status_clean) %>%\n    mutate(status_clean = replace_na(\n           status_clean, \"unknown\"))\n\n\n4.1 Extracting Water Point Data\n\n#filter out the NA values, extract functional water output\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Functional\",\n             \"Functional, needs repair\",\n             \"Functional, not in use\"\n           ))\n\n\n#extract non functional\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Abandoned/Decommissioned\",\n             \"Non-Functional\",\n             \"Non-Functional, dry\"\n           ))\n\n\n#extract unknown\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\nNext, the code chunk below is used to perform a quick EDA on the derived sf data.frames.\n\nfreq(data = wp_functional,\n     input = 'status_clean')\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\nfreq(data = wp_unknown,\n     input = 'status_clean')\n\n\n\n4.2 Performing Point-in-Polygon Count\nNext, we want to find out the number of total, functional, nonfunctional and unknown water points in each LGA. This is performed in the following code chunk. First, it identifies the functional water points in each LGA by using st_intersects() of sf package. Next, length() is used to calculate the number of functional water points that fall inside each LGA.\n\n# extra step to cross check whether previously extracted correctly\nNGA_wp <- NGA %>%\n  mutate(`total_wp`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_functional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_nonfunctional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_unknown`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  ))\n\n\n\n4.3 Visualing attributes by using statistical graphs\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n#save in rds format (rds allow us to retain the data structure/simple feature with the data properties)\n\nwrite_rds(NGA_wp, \"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "",
    "text": "Hello, this is Rhonda Ho’s Take-home Assignment 1 for IS415 module.\nTo view/hide the code, please click on the “</> code” tab beside the title and select the option to view/hide the code.\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, I am tasked to apply appropriate spatial point patterns analysis methods to discover the geographical distribution of functional and non-function water points and their co-locations if any in Osun State, Nigeria.\n\n\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nExploratory Spatial Data Analysis (ESDA)\n\nDerive kernel density maps of functional and non-functional water points. Using appropriate tmap functions,\nDisplay the kernel density maps on openstreetmap of Osub State, Nigeria.\nDescribe the spatial patterns revealed by the kernel density maps. Highlight the advantage of kernel density map over point map.\n\n\n\n\nSecond-order Spatial Point Patterns Analysis\nWith reference to the spatial point patterns observed in ESDA:\n\nFormulate the null hypothesis and alternative hypothesis and select the confidence level.\nPerform the test by using appropriate Second order spatial point patterns analysis technique.\nWith reference to the analysis results, draw statistical conclusions.\n\nSpatial Correlation Analysis\nIn this section, you are required to confirm statistically if the spatial distribution of functional and non-functional water points are independent from each other.\n\nFormulate the null hypothesis and alternative hypothesis and select the confidence level.\nPerform the test by using appropriate Second order spatial point patterns analysis technique.\nWith reference to the analysis results, draw statistical conclusions."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#handling-geospatial-data",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#handling-geospatial-data",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "2. Handling Geospatial Data",
    "text": "2. Handling Geospatial Data\n\n2.1 Importing Geospatial\nRead the file from geoBoundaries.\n\n\nShow the code\ngeoNGA = st_read(dsn = \"data/geospatial/\", layer=\"geoBoundaries-NGA-ADM2\")%>%st_transform(crs=26392)\n\n#transforms data from decimal to metres\n\n\nRead the file from Humanitarian Data Exchange.\n\n\nShow the code\nNGA = st_read(dsn = \"data/geospatial/\", layer=\"nga_admbnda_adm2_osgof_20190417\")%>%st_transform(crs=26392)\n\n\n\n\n2.2 Handling Aspatial Data\nRead the file from and filter out where country name is Nigeria\n\n\nShow the code\nwp_nga <- read_csv(\"data/aspatial/wpdx.csv\") %>%\n  filter(clean_country_name %in% c(\"Nigeria\"))\n\n\nConverting an aspatial data into an sf data.frame involves two steps.\nFirst, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\n\nCode\nwp_nga$Geometry = st_as_sfc(wp_nga$`new_georeferenced_column_`)\nwp_nga\n\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\n#Transforming into Nigeria projected coordinate system\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#geospatial-data-cleaning",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#geospatial-data-cleaning",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "3. Geospatial Data Cleaning",
    "text": "3. Geospatial Data Cleaning\nNGA sf data.frame consists of many redundent fields. The code chunk below uses select() of dplyr to retain column 3, 4, 8 and 9. Do you know why?\n\n3.1 Exclude redundant fields\n\n\nCode\n#exclude redundant fields\nNGA <- NGA %>%\n  select(3:4, 8:9)\n\n\n\n\n3.2 Checking for duplicate name\n\n\nCode\n#check for duplicate names\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n\n#lets correct the errors (suppose to manually find)\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\n\nNow, let us rerun the code chunk below to confirm that the duplicated name issue has been addressed.\n\n\nCode\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#data-wrangling-for-water-point-data",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#data-wrangling-for-water-point-data",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "4. Data Wrangling for Water Point Data",
    "text": "4. Data Wrangling for Water Point Data\nExploratory Data Analysis (EDA) is a popular approach to gain initial understanding of the data. In the code chunk below, freq() of funModeling package is used to reveal the distribution of water point status visually.\n\n\nCode\n#check frequency count\n\nfreq(data = wp_sf,\n     input = \"status_clean\")\n\n\n\n\nCode\n#replace NA fills with unknown\nwp_sf_nga <- wp_sf%>%\n  rename(status_clean = 'status_clean') %>%\n    select(status_clean) %>%\n    mutate(status_clean = replace_na(\n           status_clean, \"unknown\"))\n\n\n\n4.1 Extracting Water Point Data\n\n\nCode\n#filter out the NA values, extract functional water output\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Functional\",\n             \"Functional, needs repair\",\n             \"Functional, not in use\"\n           ))\n\n\n\n\nCode\n#extract non functional\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Abandoned/Decommissioned\",\n             \"Non-Functional\",\n             \"Non-Functional, dry\"\n           ))\n\n\n\n\nCode\n#extract unknown\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\n\nNext, the code chunk below is used to perform a quick EDA on the derived sf data.frames.\n\n\nCode\nfreq(data = wp_functional,\n     input = 'status_clean')\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\nfreq(data = wp_unknown,\n     input = 'status_clean')\n\n\n\n\n4.2 Performing Point-in-Polygon Count\nNext, we want to find out the number of total, functional, nonfunctional and unknown water points in each LGA. This is performed in the following code chunk. First, it identifies the functional water points in each LGA by using st_intersects() of sf package. Next, length() is used to calculate the number of functional water points that fall inside each LGA.\n\n\nCode\n# extra step to cross check whether previously extracted correctly\nNGA_wp <- NGA %>%\n  mutate(`total_wp`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_functional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_nonfunctional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_unknown`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  ))\n\n\n\n\n4.3 Visualing attributes by using statistical graphs\n\n\nCode\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n\n\nCode\n#save in rds format (rds allow us to retain the data structure/simple feature with the data properties)\n\nwrite_rds(NGA_wp, \"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n\nShow the code\npacman::p_load(maptools, sf, raster, spatstat, tmap)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-data-wrangling",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3. Spatial Data Wrangling",
    "text": "3. Spatial Data Wrangling\n\n3.1 Importing the spatial data\n\n\nShow the code\nchildcare_sf <- st_read(\"data/child-care-services-geojson.geojson\") %>%\n  st_transform(crs = 3414)\n\n\n\n\nShow the code\nsg_sf <- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\n\n\n\nShow the code\nmpsz_sf <- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\n\n\n3.2 Mapping the geospatial data sets\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\nShow the code\ntmap_mode('plot')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "4. Geospatial Data wrangling",
    "text": "4. Geospatial Data wrangling\nThe first 3 steps are important for dealing with geospatial data wrangling. Source data must be in sf., source data needs to be in the same projection system as once it is converted, cannot tell.\n\n4.1 Converting sf data frames to sp’s Spatial* class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\n\nShow the code\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\n\n\nShow the code\nchildcare\n\n\n\n\nShow the code\nmpsz\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.2 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\n\n\nShow the code\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\n\n\nShow the code\nchildcare_sp\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.3 Converting the generic sp format into spatstat’s ppp format\n\n\nShow the code\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nplot(childcare_ppp)\n\n\n\n\nShow the code\nsummary(childcare_ppp)\n\n\n\n\n4.4 Handling duplicated points\n\n\nShow the code\n#check for duplication\nany(duplicated(childcare_ppp))\n\n#count the number of co-indicence point\nmultiplicity(childcare_ppp)\n\n#how many locations have more than one point event\nsum(multiplicity(childcare_ppp) > 1)\n\n\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\nShow the code\ntmap_mode('plot')\n\n\n\n\nShow the code\nchildcare_ppp_jit <- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\n\n\nShow the code\nany(duplicated(childcare_ppp_jit))\n\n\n\n\n4.5 Creating owin object\n\n\nShow the code\nsg_owin <- as(sg_sp, \"owin\")\nplot(sg_owin)\n\n\n\n\nShow the code\nsummary(sg_owin)\n\n\n\n\n4.6 Combining point events object and owin object\nExtract childcare events that are located within Singapore\n\n\nShow the code\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nShow the code\nsummary(childcareSG_ppp)\n\n\n\n4.6.1 Extracting study area\n\n\nShow the code\n#extract planning areas\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\n#plotting target planning areas\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")\n\n\n\n\n4.6.2 Converting the spatial point data frame into generic sp format\n\n\nShow the code\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\n\n\n\n4.6.2 Creating owin object\n\n\nShow the code\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\n\n\n\n4.6.2 Combining childcare points and the study area\n\n\nShow the code\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n\n\n\nShow the code\n#rescale to metres to kilometres\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n\n\n\nShow the code\n#plot the 4 study areas\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#second-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#second-order-spatial-point-patterns-analysis",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5 Second-order Spatial Point Patterns Analysis",
    "text": "5 Second-order Spatial Point Patterns Analysis\n???"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-g-function",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-g-function",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6 Analysing Spatial Point Process Using G-Function",
    "text": "6 Analysing Spatial Point Process Using G-Function\n\n\n6.1 Choa Chu Kang planning area\n\n6.1.2 Computing G-function estimation\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, you will learn how to compute G-function estimation by using Gest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\nShow the code\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n6.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-fucntion\n\n\nShow the code\nG_CK.csr <- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\n\n\n\nShow the code\nplot(G_CK.csr)\n\n\n\n\n\n6.2 Tampiness planning area\n\n6.2.2 Computing G-function estimation\n\n\nShow the code\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n6.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-fucntion\n\n\nShow the code\nG_tm.csr <- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\n\n\n\nShow the code\nplot(G_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-f-function",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-f-function",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "7 Analysing Spatial Point Process Using F-Function",
    "text": "7 Analysing Spatial Point Process Using F-Function\n\n\n7.1 Choa Chu Kang planning area\n\n7.1.1 Computing F-function estimation\n\n\nShow the code\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n7.1.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nF_CK.csr <- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nplot(F_CK.csr)\n\n\n\n\n\n7.2 Tampiness planning area\n\n7.1.1 Computing F-function estimation\n\n\nShow the code\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n7.2.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nF_tm.csr <- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\nplot(F_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-k-function",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-k-function",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "8 Analysing Spatial Point Process Using K-Function",
    "text": "8 Analysing Spatial Point Process Using K-Function\n8.1 Choa Chu Kang planning area\n\n8.1.1 Computing K-function estimate\n\n\nShow the code\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n8.1.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nK_ck.csr <- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n8.2 Tampiness planning area\n\n8.1.1 Computing K-function estimate\n\n\nShow the code\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n8.1.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nK_tm.csr <- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-l-function",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-l-function",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "9 Analysing Spatial Point Process Using L-Function",
    "text": "9 Analysing Spatial Point Process Using L-Function\n9.1 Choa Chu Kang planning area\n\n9.1.1 Computing L Fucntion estimation\n\n\nShow the code\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n9.1.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nL_ck.csr <- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n9.2 Tampiness planning area\n\n\n9.2.1 Computing L Function estimation\n\n\nShow the code\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n9.2.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nL_tm.csr <- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on Exercise 4: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "title": "Hands-on Exercise 4: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n\nShow the code\npacman::p_load(maptools, sf, raster, spatstat, tmap)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-data-wrangling",
    "title": "Hands-on Exercise 4: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "3. Spatial Data Wrangling",
    "text": "3. Spatial Data Wrangling\n\n3.1 Importing the spatial data\n\n\nShow the code\nchildcare_sf <- st_read(\"data/child-care-services-geojson.geojson\") %>%\n  st_transform(crs = 3414)\n\n\n\n\nShow the code\nsg_sf <- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\n\n\n\nShow the code\nmpsz_sf <- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\n\n\n3.2 Mapping the geospatial data sets\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\nShow the code\ntmap_mode('plot')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 4: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "4. Geospatial Data wrangling",
    "text": "4. Geospatial Data wrangling\nThe first 3 steps are important for dealing with geospatial data wrangling. Source data must be in sf., source data needs to be in the same projection system as once it is converted, cannot tell.\n\n4.1 Converting sf data frames to sp’s Spatial* class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\n\nShow the code\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\n\n\n4.2 Creating a choropleth map by using tmap’s elements\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nShow the code\nchildcare\n\n\n\n\nShow the code\nmpsz\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.3 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\n\n\nShow the code\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\n\n\nShow the code\nchildcare_sp\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.4 Converting the generic sp format into spatstat’s ppp format\n\n\nShow the code\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nplot(childcare_ppp)\n\n\n\n\nShow the code\nsummary(childcare_ppp)\n\n\n\n\n4.5 Handling duplicated points\n\n\nShow the code\n#check for duplication\nany(duplicated(childcare_ppp))\n\n#count the number of co-indicence point\nmultiplicity(childcare_ppp)\n\n#how many locations have more than one point event\nsum(multiplicity(childcare_ppp) > 1)\n\n\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\nShow the code\ntmap_mode('plot')\n\n\n\n\nShow the code\nchildcare_ppp_jit <- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\n\n\nShow the code\nany(duplicated(childcare_ppp_jit))\n\n\n\n\n4.6 Creating owin object\n\n\nShow the code\nsg_owin <- as(sg_sp, \"owin\")\nplot(sg_owin)\n\n\n\n\nShow the code\nsummary(sg_owin)\n\n\n\n\n4.6 Combining point events object and owin object\nExtract childcare events that are located within Singapore\n\n\nShow the code\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nShow the code\nsummary(childcareSG_ppp)\n\n\n\n\n4.6 First-order Spatial Point Patterns Analysis\n\n4.6.1 Kernel Density Estimation\n\n4.6.1.1 Computing kernel density estimation using automatic bandwidth selection method\n\n\nShow the code\nkde_childcareSG_bw <- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\n\n\n\nShow the code\nplot(kde_childcareSG_bw, main=\"kde_childcareSG.bw in m^2\")\n\n\nRetrieve the bandwidth used to compute the kde layer\n\n\nShow the code\nbw <- bw.diggle(childcareSG_ppp)\nbw\n\n\n\n\n4.6.1.2 Rescalling KDE values\nRescale() is used to covert the unit of measurement from meter to kilometer.\n\n\nShow the code\nchildcareSG_ppp.km <- rescale(childcareSG_ppp, 1000, \"km\")\n\n\n\n\nShow the code\nkde_childcareSG.bw <- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw, main=\"kde_childcareSG.bw in km\")\n\n\n\n\n\n4.6.2 Working with different automatic badwidth methods\nOther spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\n\n\nShow the code\nbw.CvL(childcareSG_ppp.km)\nbw.scott(childcareSG_ppp.km)\n\n#tends to produce the more appropriate values when the pattern consists predominantly of tight clusters\nbw.ppl(childcareSG_ppp.km)\n\n#Best Method to detect a single tight cluster in the midst of random noise\nbw.diggle(childcareSG_ppp.km)\n\n\nCompare the output of using bw.diggle and bw.ppl methods.\n\n\nShow the code\nkde_childcareSG.ppl <- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n4.6.3 Working with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\n\n\n\n\n4.7 Fixed and Adaptive KDE\n\n4.7.1 Computing KDE by using fixed bandwidth\n\n\nShow the code\nkde_childcareSG_600 <- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n4.7.2 Computing KDE by using adaptive bandwidth\nDerive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\n\nShow the code\nkde_childcareSG_adaptive <- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\n\nShow the code\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n4.7.3 Converting KDE output into grid object\n\n\nShow the code\ngridded_kde_childcareSG_bw <- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n4.7.3.1 Converting gridded output into raster\n\n\nShow the code\n#Convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\nkde_childcareSG_bw_raster <-raster(gridded_kde_childcareSG_bw)\n\nkde_childcareSG_bw_raster\n\n\n\n\n4.7.3.2 Assigning projection systems\n\n\nShow the code\nprojection(kde_childcareSG_bw_raster) <-CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\n\n\n\n4.7.4 Visualising the output in tmap\n\n\nShow the code\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n4.7.5 Comparing Spatial Point Patterns using KDE\nCompare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n4.7.5.1 Extracting study area\n\n\nShow the code\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\n\nPlotting of target planning areas\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")\n\n\n\n\n4.7.5.2 Converting the spatial point data frame into generic sp format\n\n\nShow the code\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\n\n\n\n4.7.5.3 Creating owin object\n\n\nShow the code\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\n\n\n\n\n4.7.5.4 Combining childcare points and the study area\n\n\nShow the code\n#Extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n# Next, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n#The code chunk below is used to plot these four study areas and the locations of the childcare centres.\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n4.7.5.5 Computing KDE\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n4.7.5.6 Computing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth.\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\n\n\n4.8 Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used. #### 4.8.1 Testing spatial point patterns using Clark and Evans Test\n\n\nShow the code\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n\n4.8.2 Clark and Evans Test: Choa Chu Kang planning area\n\n\nShow the code\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n\n\n4.8.3 Clark and Evans Test: Tampines planning area\n\n\nShow the code\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "Exploration of Local Co-Location Quotient."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "title": "IS415-GAA",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n\nShow the code\npacman::p_load(maptools, sf, raster, spatstat, tmap, sfdep,dplyr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#importing-data",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#importing-data",
    "title": "IS415-GAA",
    "section": "3. Importing Data",
    "text": "3. Importing Data\n\n\nShow the code\nstudyArea<-st_read(dsn=\"data\", layer=\"study_area\")%>%\n  st_transform(crs=3829) #convert to local projectory system (epsg)\n\n\n\n\nShow the code\nstores<-st_read(dsn=\"data\", layer=\"stores\")%>%\n  st_transform(crs=3829)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#local-colocation-quotients-lclq",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#local-colocation-quotients-lclq",
    "title": "IS415-GAA",
    "section": "4. Local Colocation Quotients (LCLQ)",
    "text": "4. Local Colocation Quotients (LCLQ)\n\n\nShow the code\ntmap_mode('view') #by default\ntm_shape(studyArea) +\n  tm_polygons() +\ntm_shape(stores) +\n  tm_dots(col=\"Name\",\n          size=0.01,\n          border.col=\"black\",\n          border.lwd=0.5) +\n  tm_view(set.zoom.limits= c(12,16)) \n\n\n\n\nShow the code\ntmap_mode('plot')\n\n\n\n\nShow the code\nnb <- include_self(\n  st_knn(st_geometry(stores),6) # 6 nearest neighbours (should be even as it include self)\n)\n\nwt <- st_kernel_weights(nb,\n                        stores,\n                        \"gaussian\",\n                        adaptive=TRUE)\n\n#nearer target higher weightes\n\nFamilyMart <- stores %>%\n  filter(Name == \"Family Mart\")\n\nA <- FamilyMart$Name\n\nSevenEleven <- stores %>%\n  filter(Name == \"7-Eleven\")\nB <- SevenEleven$Name\n\nLCLQ <- local_colocation(A, B, nb, wt, 49) #will not see p-value\nLCLQ_stores <- cbind(stores, LCLQ)\n\n#plot lclq"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/data/stores.html",
    "href": "In-class_Ex/In-class_Ex05/data/stores.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     \n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/data/study_area.html",
    "href": "In-class_Ex/In-class_Ex05/data/study_area.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#importing-geospatial-data",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#importing-geospatial-data",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "3.1 Importing Geospatial Data",
    "text": "3.1 Importing Geospatial Data\nIn this section, st_read() of sf package will be used to import the 2 geospatial data sets into R.\n\n3.1.1 The Geoboundaries Dataset\n\n\nCode\ngeoNGA <- st_read(\"data/geospatial/\",\n                  layer = \"geoBoundaries-NGA-ADM1\") %>%\n  st_transform(crs = 26392)\n\n\nTo understand more about the columns and data across our dataset, I used the function glimpse() as shown in the code chunk below.\n\n\nCode\nglimpse(geoNGA)\n\n\n\n\n3.1.2 The NGA Dataset\n\n\nCode\nNGA <- st_read(\"data/geospatial/\",\n               layer = \"nga_admbnda_adm2_osgof_20190417\") %>%\n  st_transform(crs = 26392)\n\n\n\n\nCode\nglimpse(NGA)\n\n\nBy examining both of the sf dataframes closely, we can observe that the NGA dataset provides us with more information with regards to the state i.e Osun which we require to perform our tasks. Hence, NGA data.frame will be used for the subsequent processing.\n\n\n3.2 Importing Aspatial Data\nMoving on to the Aspatial data, as it is in an excel format, I decided to use read_csv()function. As the area of study for this task is focused on Osun State, Nigeria, I then filtered out the values accordingly using the filter() function. But how do we know which column to filter by? First, I read up on the description of the metadata in the Aspatial data here and discovered that I needed to filter out the country, Nigeria under the `clean_country_name`.\n\n\nCode\nwp_nga <- read_csv(\"data/aspatial/wpdx.csv\") %>%\n  filter(clean_country_name %in% c(\"Nigeria\"))\n\n\nNext, I took a closer look at the data and discovered that the states of Nigeria are splitted in to 4 divisions. To determine where Osun is, I used the function, any() to check which column Osun belonged to.\n\n\nCode\nany(wp_nga$clean_adm1==\"Osun\")\nany(wp_nga$clean_adm2==\"Osun\")\nany(wp_nga$clean_adm3==\"Osun\")\nany(wp_nga$clean_adm4==\"Osun\")\n\n\nBased on the output above, Osun only exists under the column ‘clean_adm1’, so I filtered out the Osun state in that specific column.\n\n\nCode\nwp_nga <- read_csv(\"data/aspatial/wpdx.csv\") %>%\n  filter(clean_adm1 %in% c(\"Osun\"))\n\n\nNext, we need to convert the aspatial data into sf data.frame.\nTo do so, it requires two steps. First, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\n\n\nCode\nwp_nga$Geometry = st_as_sfc(wp_nga$`new_georeferenced_column_`)\nwp_nga\n\n\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\n\nCode\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\n\nAfterwards, we need to transform the projection from wgs84 to appropriate projected coordinate system of Nigeria i.e 26392.\n\n\nCode\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#exclude-redundant-fields",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#exclude-redundant-fields",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "4.1 Exclude redundant fields",
    "text": "4.1 Exclude redundant fields\nNGA sf data.frame consists of many redundent fields. Thus, I used select() of dplyr to retain the relevant columns which contain the values under the state of Osun.\n\n\nCode\n# method 1: using select function\nNGA <- NGA %>%\n  dplyr::select(c(3:4,8:9))\n\n#dplyr:: is used as there may be library conflicts which prevents me from using the code\n\n# method 2: keeping the column by name\n#keeps <- c(\"ADM2_EN\",\"ADM2_PCODE\",\"ADM1_EN\",\"ADM1_PCODE\")\n#NGA = NGA[keeps]\n\n\nAfterwards, I filtered the column ‘ADM1_EN’ to only include Osun Values.\n\n\nCode\nNGA<- NGA %>%\n  filter(ADM1_EN %in% c(\"Osun\"))"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#check-for-duplicate-names",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#check-for-duplicate-names",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "4.2 Check for duplicate names",
    "text": "4.2 Check for duplicate names\nFor the code chunk below, the function duplicate() is used to check for any duplicated values.\n\n\nCode\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n\nHence, based on the output above, there is no duplicated values."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#getting-started-yet-to-edit",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#getting-started-yet-to-edit",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "Getting Started (Yet to edit)",
    "text": "Getting Started (Yet to edit)\n\n\nShow the code\npacman::p_load(sf, tidyverse, funModeling)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#geospatial-data-cleaning-1",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#geospatial-data-cleaning-1",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "3. Geospatial Data Cleaning",
    "text": "3. Geospatial Data Cleaning\nNGA sf data.frame consists of many redundent fields. The code chunk below uses select() of dplyr to retain column 3, 4, 8 and 9. Do you know why?\n\n3.1 Exclude redundant fields\n\n\nCode\n#exclude redundant fields\nNGA <- NGA %>%\n  select(3:4, 8:9)\n\n\n\n\n3.2 Checking for duplicate name\n\n\nCode\n#check for duplicate names\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n\n#lets correct the errors (suppose to manually find)\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\n\nNow, let us rerun the code chunk below to confirm that the duplicated name issue has been addressed.\n\n\nCode\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-sf-data-frames-to-sps-spatial-class",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-sf-data-frames-to-sps-spatial-class",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "5.1 Converting sf data frames to sp’s Spatial* class",
    "text": "5.1 Converting sf data frames to sp’s Spatial* class\nNext, since the task requires us to perform exploratory spatial data analysis (ESDA), we need to convert simple feature data frame to sp’s Spatial* class using the as_Spatial() function.\n\n\nCode\n#overview of wp in Ossun state\nwp_spatial <- as_Spatial(wp_sf)\n\n#only functional wp in Ossun state\nwp_func_spatial <- as_Spatial(wp_functional)\n\n#only non-functional wp in Ossun state\nwp_nonfunc_spatial <- as_Spatial(wp_nonfunctional)\n\n#NGA dataset\nNGA_spatial <- as_Spatial(NGA)\n\n\nTo further understand our data, we run the code chunk below.\n\n\nCode\nwp_spatial\n\n\n\n\nCode\nNGA_spatial\n\n\nLooking at the output above, we understand that wp_spatial belongs to the SpatialPointsDataFrame while NGA_spatial belongs to SpatialPolygonsDataFrame class. This will help us in the next section which is the conversion of Spatial* class into generic sp format."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-the-spatial-class-into-generic-sp-format",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-the-spatial-class-into-generic-sp-format",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "5.2 Converting the Spatial* class into generic sp format",
    "text": "5.2 Converting the Spatial* class into generic sp format\nAs spatstat requires the analytical data in ppp object form. We need to convert the Spatial classes* into Spatial object first. The codes chunk below converts the Spatial* classes into generic sp objects.\n\n\nCode\nwp_sp <- as(wp_spatial, \"SpatialPoints\")\nwp_func_sp <- as(wp_func_spatial, \"SpatialPoints\")\nwp_nonfunc_sp <- as(wp_nonfunc_spatial, \"SpatialPoints\")\n\nNGA_sp <- as(NGA_spatial, \"SpatialPolygons\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "5.3 Converting the generic sp format into spatstat’s ppp format",
    "text": "5.3 Converting the generic sp format into spatstat’s ppp format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\n\nCode\nwp_ppp <- as(wp_sp, \"ppp\")\nwp_ppp\n\nwp_func_ppp <- as(wp_func_sp, \"ppp\")\n\n\nwp_nonfunc_ppp <- as(wp_nonfunc_sp, \"ppp\")\n\n\nTo further understand our data, let’s look at its summary statistics.\n\n\nCode\nsummary(wp_ppp)\n\n\nBased on the output above, fortunately, we do not see a warning messages about duplicates. The code chunk below shows an alternate method of checking for duplicates. If it return FALSE, that means there is no duplicated values.\n\n\nCode\nany(duplicated(wp_ppp))\nany(duplicated(wp_func_ppp))\nany(duplicated(wp_nonfunc_ppp))"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#creating-owin-object",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#creating-owin-object",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "5.4 Creating owin object",
    "text": "5.4 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area, for example, Nigeria’s boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to convert Nigera, Ossun stats SpatialPolygon object into owin object of spatstat.\n\n\nCode\nNGA_owin <- as(NGA_sp, \"owin\")\nplot(NGA_owin)\nsummary(NGA_owin)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#combining-point-events-object-and-owin-object",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#combining-point-events-object-and-owin-object",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "5.5 Combining point events object and owin object",
    "text": "5.5 Combining point events object and owin object\nIn this last step of geospatial data wrangling, we will extract all waterpoints that are located within Nigeria Ossun by using the code chunk below.\n\n\nCode\nwpNGA_ppp = wp_ppp[NGA_owin]\nsummary(wpNGA_ppp)\nplot(wpNGA_ppp)\n\n\nThe code chunk below extract functional waterpoints that are located within Nigeria, Ossun.\n\n\nCode\nwpfuncNGA_ppp = wp_func_ppp[NGA_owin]\nsummary(wpfuncNGA_ppp)\nplot(wpfuncNGA_ppp)\n\n\nThe code chunk below extract non-functional waterpoints that are located within Nigeria, Ossun.\n\n\nCode\nwpnonfuncNGA_ppp = wp_nonfunc_ppp[NGA_owin]\nsummary(wpnonfuncNGA_ppp)\nplot(wpnonfuncNGA_ppp)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#kernel-density-estimation",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#kernel-density-estimation",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "6.1 Kernel Density Estimation",
    "text": "6.1 Kernel Density Estimation\nIn this section, I will be computing the kernel density estimation (KDE) of waterpoints in Singapore.\n\n6.1.1 Automatic bandwidth selection methods\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nCode\nkde_wpNGA_bw <- density(wpNGA_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\")\n\n\nplot(kde_wpNGA_bw, main=\"KDE of Waterpoints in Nigeria, Ossun using bw.diggle (m^2)\")\n\n\nAs we can observe from the above, the density values of the output range from 0 to 0.00000004 which is way too small to comprehend. This is because the default unit of measurement of WGS 84 is in meter. As a result, the density values computed is in “number of points per square meter”. Thus, for a better visualisation, we need to rescale the KDE values.\n\n\n6.1.2 Rescalling KDE values\nIn the code chunk below, rescale() is used to covert the unit of measurement from meter to kilometer.\n\n\nCode\nwpNGA_ppp.km <- rescale(wpNGA_ppp, 1000, \"km\")\n\n\nNow, we can re-run density() using the resale data set and plot the output kde map.\n\n\nCode\nkde_wpNGA.bw <- density(wpNGA_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_wpNGA.bw, \n     main=\"KDE of Waterpoints in Nigeria, Ossun using bw.diggle (km^2)\")\n\n\nAccording to Baddeley et. (2016), they suggested the use of the bw.ppl() algorithm because in ther experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\nHence, I decided to look at the output for both.\n\n\nCode\nkde_wpNGA.ppl <- density(wpNGA_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_wpNGA.bw, main = \"KDE of All Waterpoints, bw.diggle\")\nplot(kde_wpNGA.ppl, main = \"KDE of All Waterpoints, bw.ppl\")\n\n\n\n\n6.1.3 KDE for Functional and Non-functional Waterpoints in Nigeria, Osun\nNext, I performed the same actions as above the code chunk to derive the KDE for functional and non functional waterpoints respectively.\n\n6.1.3.1 KDE for Functional Waterpoints in Nigeria, Osun\n\n\nCode\n#rescale\nwpfuncNGA_ppp.km <- rescale(wpfuncNGA_ppp, 1000, \"km\")\n\n#kde with bandwith - diggle\nkde_wpfuncNGA.bw <- density(wpfuncNGA_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\n\n#kde with bandwith - ppl\nkde_wpfuncNGA.ppl <- density(wpfuncNGA_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\n\n#plot\nplot(kde_wpfuncNGA.bw, main=\"KDE of Functional Waterpoints (km^2), bw.diggle\")\nplot(kde_wpfuncNGA.ppl, main=\"KDE of Functional Waterpoints (km^2), bw.ppl\")\n\n\n\n\n6.1.3.2 KDE for Non-functional Waterpoints in Nigeria, Osun\n\n\nCode\n#rescale\nwpnonfuncNGA_ppp.km <- rescale(wpnonfuncNGA_ppp, 1000, \"km\")\n\n#kde with bandwith - diggle\nkde_wpnonfuncNGA.bw <- density(wpnonfuncNGA_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\n\n#kde with bandwith - ppl\nkde_wpnonfuncNGA.ppl <- density(wpnonfuncNGA_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\n\n#plot\nplot(kde_wpnonfuncNGA.bw, main=\"KDE of Non-functional Waterpoints (km^2), bw.diggle\")\nplot(kde_wpnonfuncNGA.ppl, main=\"KDE of Non-functional Waterpoints (km^2), bw.ppl\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-kde-output-into-grid-object",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-kde-output-into-grid-object",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "6.2 Converting KDE output into grid object",
    "text": "6.2 Converting KDE output into grid object\nNext, for mapping purposes, I need to convert the KDE output into a grid object.\n\n\nCode\ngridded_kde_wpNGA_bw <- as.SpatialGridDataFrame.im(kde_wpNGA.bw)\nspplot(gridded_kde_wpNGA_bw, main = \"Gridded KDE of All Waterpoints, bw.diggle\")\n\n\n\n6.2.1 Converting gridded output into raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\n\nCode\nkde_wpNGA_bw_raster <- raster(gridded_kde_wpNGA_bw)\nkde_wpNGA_bw_raster\n\n\nBased on the output, the crs property is NA. Hence, we need to assign it.\n\n\n6.2.2 Assigning projection systems\n\n\nCode\nprojection(kde_wpNGA_bw_raster) <- CRS(\"+init=EPSG:26392\")\nkde_wpNGA_bw_raster\n\n\nBased on the output above, the CRS information has been sucessfully added in.\n\n\n6.2.3 Visualising the output in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\n6.2.3.1 Raster of KDE All Waterpoints in Nigeria, Osun\n\n\nCode\ntm_shape(kde_wpNGA_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(main.title=\"Raster of KDE Waterpoints in Nigeria, Osun\", \n            main.title.size=1,\n            legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\nBased on the KDE graphs, most of the waterpoints are clustered around the top middle section. Upon further research on the cities in Nigeria, State of Osun, I discovered that most of waterpoints are clustered around the city called Osogbo where the Osun river resides.\n\n\n6.2.3.2 Raster of KDE Functional Waterpoints in Nigeria, Osun\nTo display raster of KDE of functional waterpoints in Nigeria, Osun, simply repeat the steps in section 6.\n\n\nCode\n# Repeat the same steps in section 6.2\n\n#convert to grid\ngridded_kde_wpfuncNGA_bw <- as.SpatialGridDataFrame.im(kde_wpfuncNGA.bw)\nspplot(gridded_kde_wpfuncNGA_bw, main = \"Gridded KDE of Functional Waterpoints, bw.diggle\")\n\n#create raster\nkde_wpfuncNGA_bw_raster <- raster(gridded_kde_wpfuncNGA_bw)\n\n#assign CRS info\nprojection(kde_wpfuncNGA_bw_raster) <- CRS(\"+init=EPSG:26392\")\n\n\n\n\nCode\ntm_shape(kde_wpfuncNGA_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(main.title=\"Raster of KDE Functional Waterpoints in Nigeria, Osun\", \n            main.title.size=0.8,\n            legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\nLooking at the KDE graph for functional waterpoints in Nigeria, Osun using bw.diggle, we can observe that the functional waterpoints are mainly clustered in 6 areas. Roughly gauging based on the cities in Nigeria, Ossun, the most densely populated area of functional waterpoints area is found in the city of Osogbo, followed by Ikirun/Ota, Isero/Ikonifin and Okeigbo. The less densely populated area of functional waterpoints but we are still able to see a cluster are around the city of Ife and Ikire.\n\n\n6.2.3.3 Raster of KDE Non-functional Waterpoints in Nigeria, Osun\n\n\nCode\n#convert to grid\ngridded_kde_wpnonfuncNGA_bw <- as.SpatialGridDataFrame.im(kde_wpnonfuncNGA.bw)\nspplot(gridded_kde_wpnonfuncNGA_bw, main = \"Gridded KDE of Non-functional Waterpoints, bw.diggle\")\n\n#create raster\nkde_wpnonfuncNGA_bw_raster <- raster(gridded_kde_wpnonfuncNGA_bw)\n\n#assign CRS info\nprojection(kde_wpnonfuncNGA_bw_raster) <- CRS(\"+init=EPSG:26392\")\n\n\n\n\nCode\ntm_shape(kde_wpnonfuncNGA_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(main.title=\"Raster of KDE Non-functional Waterpoints in Nigeria, Osun\",\n            main.title.size=0.7,\n            legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\nLooking at the KDE graph for non-functional waterpoints in Nigeria, Osun using bw.diggle, we can observe that the non-functional waterpoints are roughly clustered in 9 areas. As compared to the functional waterpoints clusters, there are a larger number of non-functional waterpoints clusters but each cluster are much smaller. It also seems that the most densely populated non-functional waterpoints area is quite near, on the right of, the city of Osogbo, where most of the functional waterpoints are.\n\n\n6.2.3.4 Raster of KDE All Waterpoints in Nigeria, Osun (Openstreetmap)\n\n\nCode\ntmap_mode('view')\ntm_basemap(server = \"OpenStreetMap\") +\ntm_shape(kde_wpNGA_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(main.title=\"Raster of KDE Waterpoints in Nigeria, Osun\", \n            main.title.size=1,\n            legend.position = c(\"right\", \"bottom\"), frame = FALSE) +\n  tm_view(set.zoom.limits= c(18,29)) \n\n\n\n\nCode\ntmap_mode('plot')\n\n\n\n\n\n6.2.4 Advantage of Kernel Density map over Point map\nKDE map takes into account the location of features relative to each other while for a point map, it shows the quantity specified by the population field that falls within the identified neighborhood and divide that quantity by the area of the neighborhood.\nThe disadvantages of a point map includes:\n\nOvercrowding of points, when the scale is small, which makes it harder for the user to analyse the map\nSubjected to distortion of shape, distance, direction, scale, and area\n\nThus, KDE map would be a more accurate representation."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#functional-waterpoints-in-nigeria-ossun",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#functional-waterpoints-in-nigeria-ossun",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "7.1 Functional Waterpoints in Nigeria, Ossun",
    "text": "7.1 Functional Waterpoints in Nigeria, Ossun\n\n7.1.1 Hypothesis Test\n\nNull hypothesis, H0: The distribution of functional waterpoints in Nigeria, Osun is randomly distributed.\nAlternative hypothesis, H1: The distribution of functional waterpoints in Nigeria, Osun is not randomly distributed.\nThe hypothesis will be tested at a significance level of 0.05, with a corresponding confidence level of 95%.\n\n\n\n7.1.2 Computing L Function estimation\n\n\nCode\nL_func = Lest(wpfuncNGA_ppp, correction = \"Ripley\")\nplot(L_func, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n7.1.3 Performing Complete Spatial Randomness Test\nNext, we perform the function envelope() to compute simulation envelopes of the summary function i.e L function. The following arguments are used:\n\nnsim : Number of simulated point patterns to be generated when computing the envelopes.\nAs we have chosen the significance level to be 0.05, following this formula from the documentation, significance level alpha = 2 * nrank / (1 + nsim), nsim would be 39.\nrank: Integer. Rank of the envelope value amongst the nsim simulated values. A rank of 1 means that the minimum and maximum simulated values will be used.\nLogical flag indicating whether envelopes should be pointwise (global=FALSE) or simultaneous (global=TRUE).\n\n\n\nCode\n#takes around less than 2 minutes\nL_func.csr <- envelope(wpfuncNGA_ppp, Lest, nsim = 39, nrank = 1, global=TRUE)\n\n\n\n\nCode\nplot(L_func.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\nBased on the graph, we can see that the L function is within the randomisation. Hence, we cannot reject the null hypothesis as there is insufficient evidence that the distribution of functional waterpoints in Nigeria, Osun is randomly distributed at the level of 0.05."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#non-functional-waterpoints-in-nigeria-ossun",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#non-functional-waterpoints-in-nigeria-ossun",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "7.2 Non-functional Waterpoints in Nigeria, Ossun",
    "text": "7.2 Non-functional Waterpoints in Nigeria, Ossun\n\n7.2.1 Hypothesis Test\n\nNull hypothesis, H0: The distribution of non-functional waterpoints in Nigeria, Osun is randomly distributed.\nAlternative hypothesis, H1: The distribution of non-functional waterpoints in Nigeria, Osun is not randomly distributed.\nThe hypothesis will be tested at a significance level of 0.05, with a corresponding confidence level of 95%.\n\n\n\n7.2.2 Computing L Function estimation\n\n\nCode\nL_nonfunc = Lest(wpnonfuncNGA_ppp, correction = \"Ripley\")\nplot(L_nonfunc, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n7.1.3 Performing Complete Spatial Randomness Test\n\n\nCode\n#takes around less than 2 minutes\nL_nonfunc.csr <- envelope(wpnonfuncNGA_ppp, Lest, nsim = 39, nrank = 1, global=TRUE)\n\n\n\n\nCode\nplot(L_nonfunc.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\nBased on the graph, we can see that the L function is within the randomisation. Hence, we cannot reject the null hypothesis as there is insufficient evidence to prove that the distribution of non-functional waterpoints in Nigeria, Osun is randomly distributed at the level of 0.05."
  }
]