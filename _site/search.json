[
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#getting-started",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#getting-started",
    "title": "In-class Ex 9",
    "section": "Getting Started",
    "text": "Getting Started\n\n\nCode\npacman::p_load(sf, spdep, GWmodel, SpatialML, tidyverse,\n               tmap, ggpubr, olsrr, devtools)\n\n#tidymodels, load rsample"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#data-preparation",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#data-preparation",
    "title": "In-class Ex 9",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nReading Aspatial data file to rds\n\n\nCode\nmdata <- read_rds(\"data/aspatial/mdata.rds\")\n\n\n\n\nReading Geospatial data\n\n\nCode\nsubzone <- st_read(dsn = \"data/geospatial\", \n                   layer = \"MP14_SUBZONE_WEB_PL\")\n\n\n\n\nCode\nset.seed(123)\n\nresale_split <- rsample::initial_split(mdata,\n                               prop = 6.5/10,)\ntrain_data <- training(resale_split)\ntest_data <- testing(resale_split)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called **tmap** package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#getting-started",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-of-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-of-data",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "3. Importing of Data",
    "text": "3. Importing of Data\n\n3.1 Importing Geospatial Data into R\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nmpsz\n\n\n\n3.2 Importing Attribute Data into R\n\npopdata <- read_csv(\"data/aspatial/respopagesexfa2011to2020.csv\")\n\n\n\n3.3 Data Preparation\n\n\n3.3.1 Data Wrangling\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\nJoining the attribute data and geospatial data\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\n#left join: join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n#write it to a new file\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "4. Choropleth Mapping Geospatial Data Using tmap",
    "text": "4. Choropleth Mapping Geospatial Data Using tmap\n\n4.1 Plotting a choropleth map quickly by using qtm()\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n4.2 Creating a choropleth map by using tmap’s elements\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n4.2.1 Drawing a base map\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n4.2.2 Drawing a choropleth map using tm_polygons()\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n4.2.3 Drawing a choropleth map using tm_fill() and *tm_border()**\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n# to add boundaries\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n#Beside alpha argument, there are three other arguments for tm_borders(), they are: col = border colour, lwd = border line width. The default is 1, and lty = border line type. The default is “solid”\n\n\n\n\n4.3 Data classification methods of tmap\n\n4.3.1 Plotting choropleth maps with built-in classification methods\n\n\nShow the code\n# quantile data classification\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n# equal data classification\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n# Notice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n\n\n4.3.2 Plotting choropleth map with custome break\n\n\nShow the code\nsummary(mpsz_pop2020$DEPENDENCY)\n\n#With reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n4.4 Colour Scheme\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n# add a - to reverse color scheme\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n4.5 Map Layouts\n\n4.5.1 Map Legend\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n4.5.2 Map style\n\n## classic \ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n4.5.3 Cartographic Furniture\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\ntmap_style(\"white\")\n\n\n\n\n4.6 Drawing Small Multiple Choropleth Maps\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n4.6.1 By assigning multiple values to at least one of the aesthetic arguments\n\n# small multiple choropleth maps are created by defining ncols in tm_fill()\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n# small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n4.6.2 By defining a group-by variable in tm_facets()\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n4.6.3 By creating multiple stand-alone maps with tmap_arrange()\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n4.7 Mappping Spatial Object Meeting a Selection Criterion\nnstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#reference",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "5 Reference",
    "text": "5 Reference\nReference"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on Exercise 4: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#getting-started",
    "title": "Hands-on Exercise 4: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n\nShow the code\npacman::p_load(maptools, sf, raster, spatstat, tmap)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-data-wrangling",
    "title": "Hands-on Exercise 4: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "3. Spatial Data Wrangling",
    "text": "3. Spatial Data Wrangling\n\n3.1 Importing the spatial data\n\n\nShow the code\nchildcare_sf <- st_read(\"data/child-care-services-geojson.geojson\") %>%\n  st_transform(crs = 3414)\n\n\n\n\nShow the code\nsg_sf <- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\n\n\n\nShow the code\nmpsz_sf <- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\n\n\n3.2 Mapping the geospatial data sets\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\nShow the code\ntmap_mode('plot')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 4: 1st Order Spatial Point Patterns Analysis Methods",
    "section": "4. Geospatial Data wrangling",
    "text": "4. Geospatial Data wrangling\nThe first 3 steps are important for dealing with geospatial data wrangling. Source data must be in sf., source data needs to be in the same projection system as once it is converted, cannot tell.\n\n4.1 Converting sf data frames to sp’s Spatial* class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\n\nShow the code\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\n\n\n4.2 Creating a choropleth map by using tmap’s elements\n\n\nShow the code\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nShow the code\nchildcare\n\n\n\n\nShow the code\nmpsz\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.3 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\n\n\nShow the code\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\n\n\nShow the code\nchildcare_sp\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.4 Converting the generic sp format into spatstat’s ppp format\n\n\nShow the code\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nplot(childcare_ppp)\n\n\n\n\nShow the code\nsummary(childcare_ppp)\n\n\n\n\n4.5 Handling duplicated points\n\n\nShow the code\n#check for duplication\nany(duplicated(childcare_ppp))\n\n#count the number of co-indicence point\nmultiplicity(childcare_ppp)\n\n#how many locations have more than one point event\nsum(multiplicity(childcare_ppp) > 1)\n\n\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\nShow the code\ntmap_mode('plot')\n\n\n\n\nShow the code\nchildcare_ppp_jit <- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\n\n\nShow the code\nany(duplicated(childcare_ppp_jit))\n\n\n\n\n4.6 Creating owin object\n\n\nShow the code\nsg_owin <- as(sg_sp, \"owin\")\nplot(sg_owin)\n\n\n\n\nShow the code\nsummary(sg_owin)\n\n\n\n\n4.6 Combining point events object and owin object\nExtract childcare events that are located within Singapore\n\n\nShow the code\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nShow the code\nsummary(childcareSG_ppp)\n\n\n\n\n4.6 First-order Spatial Point Patterns Analysis\n\n4.6.1 Kernel Density Estimation\n\n4.6.1.1 Computing kernel density estimation using automatic bandwidth selection method\n\n\nShow the code\nkde_childcareSG_bw <- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\n\n\n\nShow the code\nplot(kde_childcareSG_bw, main=\"kde_childcareSG.bw in m^2\")\n\n\nRetrieve the bandwidth used to compute the kde layer\n\n\nShow the code\nbw <- bw.diggle(childcareSG_ppp)\nbw\n\n\n\n\n4.6.1.2 Rescalling KDE values\nRescale() is used to covert the unit of measurement from meter to kilometer.\n\n\nShow the code\nchildcareSG_ppp.km <- rescale(childcareSG_ppp, 1000, \"km\")\n\n\n\n\nShow the code\nkde_childcareSG.bw <- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw, main=\"kde_childcareSG.bw in km\")\n\n\n\n\n\n4.6.2 Working with different automatic badwidth methods\nOther spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\n\n\nShow the code\nbw.CvL(childcareSG_ppp.km)\nbw.scott(childcareSG_ppp.km)\n\n#tends to produce the more appropriate values when the pattern consists predominantly of tight clusters\nbw.ppl(childcareSG_ppp.km)\n\n#Best Method to detect a single tight cluster in the midst of random noise\nbw.diggle(childcareSG_ppp.km)\n\n\nCompare the output of using bw.diggle and bw.ppl methods.\n\n\nShow the code\nkde_childcareSG.ppl <- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n4.6.3 Working with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\n\n\n\n\n4.7 Fixed and Adaptive KDE\n\n4.7.1 Computing KDE by using fixed bandwidth\n\n\nShow the code\nkde_childcareSG_600 <- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n4.7.2 Computing KDE by using adaptive bandwidth\nDerive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\n\nShow the code\nkde_childcareSG_adaptive <- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\n\nShow the code\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n4.7.3 Converting KDE output into grid object\n\n\nShow the code\ngridded_kde_childcareSG_bw <- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n4.7.3.1 Converting gridded output into raster\n\n\nShow the code\n#Convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\nkde_childcareSG_bw_raster <-raster(gridded_kde_childcareSG_bw)\n\nkde_childcareSG_bw_raster\n\n\n\n\n4.7.3.2 Assigning projection systems\n\n\nShow the code\nprojection(kde_childcareSG_bw_raster) <-CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\n\n\n\n4.7.4 Visualising the output in tmap\n\n\nShow the code\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n4.7.5 Comparing Spatial Point Patterns using KDE\nCompare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n4.7.5.1 Extracting study area\n\n\nShow the code\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\n\nPlotting of target planning areas\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")\n\n\n\n\n4.7.5.2 Converting the spatial point data frame into generic sp format\n\n\nShow the code\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\n\n\n\n4.7.5.3 Creating owin object\n\n\nShow the code\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\n\n\n\n\n4.7.5.4 Combining childcare points and the study area\n\n\nShow the code\n#Extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n# Next, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n#The code chunk below is used to plot these four study areas and the locations of the childcare centres.\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n4.7.5.5 Computing KDE\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n4.7.5.6 Computing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth.\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\n\n\n4.8 Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used. #### 4.8.1 Testing spatial point patterns using Clark and Evans Test\n\n\nShow the code\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n\n4.8.2 Clark and Evans Test: Choa Chu Kang planning area\n\n\nShow the code\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n\n\n4.8.3 Clark and Evans Test: Tampines planning area\n\n\nShow the code\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#getting-started",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n\nShow the code\npacman::p_load(maptools, sf, raster, spatstat, tmap)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-data-wrangling",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "3. Spatial Data Wrangling",
    "text": "3. Spatial Data Wrangling\n\n3.1 Importing the spatial data\n\n\nShow the code\nchildcare_sf <- st_read(\"data/child-care-services-geojson.geojson\") %>%\n  st_transform(crs = 3414)\n\n\n\n\nShow the code\nsg_sf <- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\n\n\n\nShow the code\nmpsz_sf <- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\n\n\n3.2 Mapping the geospatial data sets\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\nShow the code\ntmap_mode('plot')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "4. Geospatial Data wrangling",
    "text": "4. Geospatial Data wrangling\nThe first 3 steps are important for dealing with geospatial data wrangling. Source data must be in sf., source data needs to be in the same projection system as once it is converted, cannot tell.\n\n4.1 Converting sf data frames to sp’s Spatial* class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\n\nShow the code\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\n\n\nShow the code\nchildcare\n\n\n\n\nShow the code\nmpsz\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.2 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\n\n\nShow the code\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\n\n\nShow the code\nchildcare_sp\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.3 Converting the generic sp format into spatstat’s ppp format\n\n\nShow the code\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nplot(childcare_ppp)\n\n\n\n\nShow the code\nsummary(childcare_ppp)\n\n\n\n\n4.4 Handling duplicated points\n\n\nShow the code\n#check for duplication\nany(duplicated(childcare_ppp))\n\n#count the number of co-indicence point\nmultiplicity(childcare_ppp)\n\n#how many locations have more than one point event\nsum(multiplicity(childcare_ppp) > 1)\n\n\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\nShow the code\ntmap_mode('plot')\n\n\n\n\nShow the code\nchildcare_ppp_jit <- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\n\n\nShow the code\nany(duplicated(childcare_ppp_jit))\n\n\n\n\n4.5 Creating owin object\n\n\nShow the code\nsg_owin <- as(sg_sp, \"owin\")\nplot(sg_owin)\n\n\n\n\nShow the code\nsummary(sg_owin)\n\n\n\n\n4.6 Combining point events object and owin object\nExtract childcare events that are located within Singapore\n\n\nShow the code\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nShow the code\nsummary(childcareSG_ppp)\n\n\n\n4.6.1 Extracting study area\n\n\nShow the code\n#extract planning areas\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\n#plotting target planning areas\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")\n\n\n\n\n4.6.2 Converting the spatial point data frame into generic sp format\n\n\nShow the code\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\n\n\n\n4.6.2 Creating owin object\n\n\nShow the code\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\n\n\n\n4.6.2 Combining childcare points and the study area\n\n\nShow the code\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n\n\n\nShow the code\n#rescale to metres to kilometres\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n\n\n\nShow the code\n#plot the 4 study areas\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#second-order-spatial-point-patterns-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#second-order-spatial-point-patterns-analysis",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "5 Second-order Spatial Point Patterns Analysis",
    "text": "5 Second-order Spatial Point Patterns Analysis\n???"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-g-function",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-g-function",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "6 Analysing Spatial Point Process Using G-Function",
    "text": "6 Analysing Spatial Point Process Using G-Function\n\n\n6.1 Choa Chu Kang planning area\n\n6.1.2 Computing G-function estimation\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, you will learn how to compute G-function estimation by using Gest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\n\nShow the code\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n6.1.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-fucntion\n\n\nShow the code\nG_CK.csr <- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\n\n\n\nShow the code\nplot(G_CK.csr)\n\n\n\n\n\n6.2 Tampiness planning area\n\n6.2.2 Computing G-function estimation\n\n\nShow the code\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n6.2.2 Performing Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-fucntion\n\n\nShow the code\nG_tm.csr <- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\n\n\n\nShow the code\nplot(G_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-f-function",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-f-function",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "7 Analysing Spatial Point Process Using F-Function",
    "text": "7 Analysing Spatial Point Process Using F-Function\n\n\n7.1 Choa Chu Kang planning area\n\n7.1.1 Computing F-function estimation\n\n\nShow the code\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n7.1.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nF_CK.csr <- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nplot(F_CK.csr)\n\n\n\n\n\n7.2 Tampiness planning area\n\n7.1.1 Computing F-function estimation\n\n\nShow the code\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n7.2.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nF_tm.csr <- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\nplot(F_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-k-function",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-k-function",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "8 Analysing Spatial Point Process Using K-Function",
    "text": "8 Analysing Spatial Point Process Using K-Function\n8.1 Choa Chu Kang planning area\n\n8.1.1 Computing K-function estimate\n\n\nShow the code\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n8.1.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nK_ck.csr <- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n8.2 Tampiness planning area\n\n8.1.1 Computing K-function estimate\n\n\nShow the code\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n8.1.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nK_tm.csr <- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-l-function",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#analysing-spatial-point-process-using-l-function",
    "title": "Hands-on Exercise 5: 2nd Order Spatial Point Patterns Analysis Methods",
    "section": "9 Analysing Spatial Point Process Using L-Function",
    "text": "9 Analysing Spatial Point Process Using L-Function\n9.1 Choa Chu Kang planning area\n\n9.1.1 Computing L Fucntion estimation\n\n\nShow the code\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n9.1.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nL_ck.csr <- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n9.2 Tampiness planning area\n\n\n9.2.1 Computing L Function estimation\n\n\nShow the code\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n9.2.2 Performing Complete Spatial Randomness Test\n\n\nShow the code\nL_tm.csr <- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "",
    "text": "Code\npacman::p_load(sf, spdep, tmap, knitr, tidyverse)\n\n#tidyverse advised to be at the back so the dependencies dont overlap\n\n\n\n\n\nImporting shapefile into r environment\n\n\nCode\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nImport csv file into r environment\n\n\nCode\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n#head(hunan,5)\n\n\nPerforming relational join\n\n\nCode\nhunan <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-queen-contiguity-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-queen-contiguity-based-neighbours",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Computing (QUEEN) contiguity based neighbours",
    "text": "Computing (QUEEN) contiguity based neighbours\nconcept based on chess i.e(rook, bishop, queen) shows what it considers as neighbours. For continuity neighbours, there needs to be a shared boundary.\nThe code chunk below compute Queen contiguity weight matrix.\n\n\nCode\nwm_q <- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\n\nDecription: The summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\n\nCode\nwm_q[[1]]\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\n\nCode\nhunan$County[1]\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\n\nCode\nhunan$NAME_3[c(2,3,4,57,85)]\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\n\nCode\nnb1 <- wm_q[[1]]\nnb1 <- hunan$GDPPC[nb1]\nnb1\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str(). Be warned: The output might cut across several pages. Save the trees if you are going to print out the report.\n\n\nCode\nstr(wm_q)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-rook-contiguity-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#creating-rook-contiguity-based-neighbours",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Creating (ROOK) contiguity based neighbours",
    "text": "Creating (ROOK) contiguity based neighbours\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\n\nCode\nwm_r <- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one neighbours."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-contiguity-weights",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#visualising-contiguity-weights",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Visualising contiguity weights",
    "text": "Visualising contiguity weights\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\n\nCode\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\n\n\n\nCode\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\n\n\nCode\ncoords <- cbind(longitude, latitude)\n\n\n\n\nCode\nhead(coords)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-queen-contiguity-based-neighbours-map",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-queen-contiguity-based-neighbours-map",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Plotting Queen contiguity based neighbours map",
    "text": "Plotting Queen contiguity based neighbours map\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-rook-contiguity-based-neighbours-map",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-rook-contiguity-based-neighbours-map",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Plotting Rook contiguity based neighbours map",
    "text": "Plotting Rook contiguity based neighbours map\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-both-queen-and-rook-contiguity-based-neighbours-maps",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-both-queen-and-rook-contiguity-based-neighbours-maps",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Plotting both Queen and Rook contiguity based neighbours maps",
    "text": "Plotting both Queen and Rook contiguity based neighbours maps\n\n\nCode\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\", main=\"Queen Contiguity\")\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\", main=\"Rook Contiguity\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#determine-the-cut-off-distance",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#determine-the-cut-off-distance",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Determine the cut-off distance",
    "text": "Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode\n#coords <- coordinates(hunan)\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\nComputing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\n\nCode\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\n\nQuiz: What is the meaning of “Average number of links: 3.681818” shown above?\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\n\nCode\nstr(wm_d62)\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\n\nCode\ntable(hunan$County, card(wm_d62))\n\n\n\n\nCode\nn_comp <- n.comp.nb(wm_d62)\nn_comp$nc\n\n\n\n\nCode\ntable(n_comp$comp.id)\n\n\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\n\nCode\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08, main=\"1st nearest neighbours\")\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6, main=\"Distance link\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-adaptive-distance-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#computing-adaptive-distance-weight-matrix",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Computing adaptive distance weight matrix",
    "text": "Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n\nCode\nknn6 <- knn2nb(knearneigh(coords, k=6))\nknn6\nstr(knn6)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-distance-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#plotting-distance-based-neighbours",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Plotting distance based neighbours",
    "text": "Plotting distance based neighbours\nWe can plot the weight matrix using the code chunk below.\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#row-standardised-weights-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#row-standardised-weights-matrix",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Row-standardised weights matrix",
    "text": "Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n\nCode\nrswm_q <- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors type:\n\n\nCode\nrswm_q$weights[10]\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.2 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\n\nCode\nrswm_ids <- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\n\n\n\nCode\nrswm_ids$weights[1]\n\n\n\n\nCode\nsummary(unlist(rswm_ids$weights))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-lag-with-row-standardized-weights",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-lag-with-row-standardized-weights",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Spatial lag with row-standardized weights",
    "text": "Spatial lag with row-standardized weights\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\n\nCode\n# had to run this -> hunan$GDPPC on its own first before the code could load, error was hunan$GDPPC not a vector\n\nGDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\n\nCode\nnb1 <- wm_q[[1]]\nnb1 <- hunan$GDPPC[nb1]\nnb1\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\n\nCode\nlag.list <- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res <- as.data.frame(lag.list)\ncolnames(lag.res) <- c(\"NAME_3\", \"lag GDPPC\")\nhunan <- left_join(hunan,lag.res)\n\n\n\n\nCode\nhead(hunan)\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\n\nCode\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_gdppc <- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-lag-as-a-sum-of-neighboring-values",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-lag-as-a-sum-of-neighboring-values",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Spatial lag as a sum of neighboring values",
    "text": "Spatial lag as a sum of neighboring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\n\nCode\nb_weights <- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 <- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\n\n\n\nCode\nlag_sum <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res <- as.data.frame(lag_sum)\ncolnames(lag.res) <- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nlag_sum\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\n\nCode\nhunan <- left_join(hunan, lag.res)\n\n\nNow, We can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\n\nCode\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc <- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-window-average",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-window-average",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Spatial window average",
    "text": "Spatial window average\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\n\nCode\nwm_qs <- include.self(wm_q)\n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below.\n\n\nCode\nwm_qs[[1]]\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\n\nCode\nwm_qs <- nb2listw(wm_qs)\nwm_qs\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\n\nCode\nlag_w_avg_gpdpc <- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\n\nCode\nlag.list.wm_qs <- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res <- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) <- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode\nhunan <- left_join(hunan, lag_wm_qs.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\n\nCode\nhunan %>%\n  select(\"County\", \"lag GDPPC\", \"lag_window_avg GDPPC\") %>%\n  kable()\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\n\nCode\nw_avg_gdppc <- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\nNote: For more effective comparison, it is advicible to use the core tmap mapping functions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-window-sum",
    "href": "Hands-on_Ex/Hands-on_Ex06/Hands-on_Ex06.html#spatial-window-sum",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "Spatial window sum",
    "text": "Spatial window sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\n\nCode\nwm_qs <- include.self(wm_q)\nwm_qs\n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\n\nCode\nb_weights <- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n\nNotice that now [1] has six neighbours instead of five.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\n\nCode\nb_weights2 <- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\n\nCode\nw_sum_gdppc <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\n\nCode\nw_sum_gdppc.res <- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) <- c(\"NAME_3\", \"w_sum GDPPC\")\n\n\nNote: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode\nhunan <- left_join(hunan, w_sum_gdppc.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\n\nCode\nhunan %>%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %>%\n  kable()\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\n\nCode\nw_sum_gdppc <- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html",
    "title": "Hands-on Ex 7: Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.(https://en.wikipedia.org/wiki/Hunan)\n\n\n\n\n\nCode\npacman::p_load(sf, spdep, tmap, tidyverse)\n\n\n\n\n\n\n\n\n\nCode\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\n\n\n\n\n\nCode\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\n\n\nCode\nhunan <- left_join(hunan,hunan2012) %>%\n  select(1:4, 7, 15)\n\n\n\n\n\n\n\nCode\nequal <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\n\n\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\n\nCode\nwm_q <- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours. ### 9.4.2 Row-standardised weights matrix\n\n\nCode\nrswm_q <- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\n\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice. ### 9.4.3 Global Spatial Autocorrelation: Moran’s I\n\n\n\n\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\n\nCode\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\np-value=1.095e-06 less that 0.05, null hypothesis should be rejected\n\n\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\n\nCode\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n\n\n\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\n\nCode\nmean(bperm$res[1:999])\n\n\n\n\nCode\nvar(bperm$res[1:999])\n\n\n\n\nCode\nsummary(bperm$res[1:999])\n\n\n\n\nCode\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\np-value=1.095e-06 less that 0.05, null hypothesis should be rejected\n\n\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\n\nCode\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n\n\n\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\n\nCode\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n\n\n\n\n\nCode\nmean(bperm$res[1:999])\n\n\n\n\nCode\nvar(bperm$res[1:999])\n\n\n\n\nCode\nsummary(bperm$res[1:999])\n\n\n\n\nCode\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\") \n\n\n\n\n\n\n\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\n\nCode\nMI_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\n\nCode\nprint(MI_corr)\n\n\n\n\n\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\n\nCode\nGC_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\nCode\nprint(GC_corr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#cluster-and-outlier-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#cluster-and-outlier-analysis",
    "title": "Hands-on Ex 7: Global and Local Measures of Spatial Autocorrelation",
    "section": "10.6 Cluster and Outlier Analysis",
    "text": "10.6 Cluster and Outlier Analysis\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, you will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n10.6.1 Computing local Moran’s I\n\n\nCode\nfips <- order(hunan$County)\nlocalMI <- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n\nlocalmoran() function returns a matrix of values whose columns are:\nIi: the local Moran’s I statistics E.Ii: the expectation of local moran statistic under the randomisation hypothesis Var.Ii: the variance of local moran statistic under the randomisation hypothesis Z.Ii:the standard deviate of local moran statistic Pr(): the p-value of local moran statistic The code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\n\nCode\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#mapping-the-local-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#mapping-the-local-morans-i",
    "title": "Hands-on Ex 7: Global and Local Measures of Spatial Autocorrelation",
    "section": "10.6.1.1 Mapping the local Moran’s I",
    "text": "10.6.1.1 Mapping the local Moran’s I\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\n\nCode\nhunan.localMI <- cbind(hunan,localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n10.6.1.2 Mapping local Moran’s I values\n\n\nCode\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n10.6.1.3 Mapping local Moran’s I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\n\nCode\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n10.6.1.4 Mapping both local Moran’s I values and p-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\n\nCode\nlocalMI.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-moran-scatterplot",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-moran-scatterplot",
    "title": "Hands-on Ex 7: Global and Local Measures of Spatial Autocorrelation",
    "section": "10.7.1 Plotting Moran scatterplot",
    "text": "10.7.1 Plotting Moran scatterplot\n\n\nCode\nnci <- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-moran-scatterplot-with-standardised-variable",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#plotting-moran-scatterplot-with-standardised-variable",
    "title": "Hands-on Ex 7: Global and Local Measures of Spatial Autocorrelation",
    "section": "10.7.2 Plotting Moran scatterplot with standardised variable",
    "text": "10.7.2 Plotting Moran scatterplot with standardised variable\nscale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\n\n\nCode\nhunan$Z.GDPPC <- scale(hunan$GDPPC) %>% \n  as.vector\n\nnci2 <- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n10.7.3 Preparing LISA map classes\n\n\nCode\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\n\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean\n\n\nCode\nhunan$lag_GDPPC <- lag.listw(rswm_q, hunan$GDPPC)\nDV <- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)   \n\n\nThis is follow by centering the local Moran’s around the mean.\n\n\nCode\nLM_I <- localMI[,1] - mean(localMI[,1])    \n\n\nNext, we will set a statistical significance level for the local Moran.\n\n\nCode\nsignif <- 0.05  \n\n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\n\nCode\nquadrant[DV <0 & LM_I>0] <- 1\nquadrant[DV >0 & LM_I<0] <- 2\nquadrant[DV <0 & LM_I<0] <- 3  \nquadrant[DV >0 & LM_I>0] <- 4      \n\n\nLastly, places non-significant Moran in the category 0.\n\n\nCode\nquadrant[localMI[,5]>signif] <- 0\n\n\nIn fact, we can combined all the steps into one single code chunk as shown below:\n\n\nCode\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC <- lag.listw(rswm_q, hunan$GDPPC)\nDV <- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I <- localMI[,1]   \nsignif <- 0.05       \nquadrant[DV <0 & LM_I>0] <- 1\nquadrant[DV >0 & LM_I<0] <- 2\nquadrant[DV <0 & LM_I<0] <- 3  \nquadrant[DV >0 & LM_I>0] <- 4    \nquadrant[localMI[,5]>signif] <- 0\n\n\n\n\n10.7.4 Plotting LISA map\n\n\nCode\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\n\nCode\ngdppc <- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-on Ex 7: Global and Local Measures of Spatial Autocorrelation",
    "section": "10.8 Hot Spot and Cold Spot Area Analysis",
    "text": "10.8 Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n10.8.1 Getis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n10.8.2 Deriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n10.8.2.1 Deriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\n\nCode\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\n\nCode\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\n\n\nCode\ncoords <- cbind(longitude, latitude)\n\n\n\n\n10.8.2.2 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode\n#coords <- coordinates(hunan)\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n10.8.2.3 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\n\nCode\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\n\nCode\nwm62_lw <- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\n\n\n\n\n10.8.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n\nCode\nknn <- knn2nb(knearneigh(coords, k=8))\nknn\n\n\n\n\nCode\nknn_lw <- nb2listw(knn, style = 'B')\nsummary(knn_lw)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#computing-gi-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex07/Hands-on_Ex07.html#computing-gi-statistics",
    "title": "Hands-on Ex 7: Global and Local Measures of Spatial Autocorrelation",
    "section": "10.9 Computing Gi statistics",
    "text": "10.9 Computing Gi statistics\n\n10.9.1 Gi statistics using fixed distance\n\n\nCode\nfips <- order(hunan$County)\ngi.fixed <- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\n\nCode\nhunan.gi <- cbind(hunan, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename(). ### 10.9.2 Mapping Gi values with fixed distance weights\n\n\nCode\ngdppc <- qtm(hunan, \"GDPPC\")\n\nGimap <-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n10.9.3 Gi statistics using adaptive distance\n\n\nCode\nfips <- order(hunan$County)\ngi.adaptive <- localG(hunan$GDPPC, knn_lw)\nhunan.gi <- cbind(hunan, as.matrix(gi.adaptive)) %>%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n\n10.9.4 Mapping Gi values with adaptive distance weights\n\n\nCode\ngdppc<- qtm(hunan, \"GDPPC\")\n\nGimap <- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html",
    "title": "Hands-on Ex 8: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#the-data",
    "title": "Hands-on Ex 8: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "13.2 The Data",
    "text": "13.2 The Data\nTwo data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#getting-started",
    "title": "Hands-on Ex 8: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "13.3 Getting Started",
    "text": "13.3 Getting Started\n\n\nCode\npacman::p_load(readr,dplyr,vctrs,olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#a-short-note-about-gwmodel",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#a-short-note-about-gwmodel",
    "title": "Hands-on Ex 8: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "13.4 A short note about GWmodel",
    "text": "13.4 A short note about GWmodel\nGWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#geospatial-data-wrangling",
    "title": "Hands-on Ex 8: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "13.5 Geospatial Data Wrangling",
    "text": "13.5 Geospatial Data Wrangling\n\n\nCode\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nUpdate CRS information\n\n\nCode\nmpsz_svy21 <- st_transform(mpsz, 3414)\n\n\n\n\nCode\nst_crs(mpsz_svy21)\n\n\n\n\nCode\nst_bbox(mpsz_svy21) #view extent"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#aspatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#aspatial-data-wrangling",
    "title": "Hands-on Ex 8: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "13.6 Aspatial Data Wrangling",
    "text": "13.6 Aspatial Data Wrangling\n\n13.6.1 Importing the aspatial data\n\n\nCode\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\n\n\nCode\nglimpse(condo_resale)\n\n\n\n\nCode\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\n\n\n\n\nCode\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\n\n\n\n\nCode\nsummary(condo_resale)\n\n\n\n\n13.6.2 Converting aspatial data frame into a sf object\n\n\nCode\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %>%\n  st_transform(crs=3414)\n\n\n\n\nCode\nhead(condo_resale.sf)\n\n\nNotice that the output is in point feature data frame."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#exploratory-data-analysis-eda",
    "title": "Hands-on Ex 8: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "13.7 Exploratory Data Analysis (EDA)",
    "text": "13.7 Exploratory Data Analysis (EDA)\n\n13.7.1 EDA using statistical graphics\n\n\nCode\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nCode\ncondo_resale.sf <- condo_resale.sf %>%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\n\n\n\nCode\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\n13.7.2 Multiple Histogram Plots distribution of variables\n\n\nCode\nAREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n13.7.3 Drawing Statistical Point Map\n\n\nCode\ntmap_mode(\"view\")\n\n\n\n\nCode\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14)) +\n  tmap_options(check.and.fix = TRUE)\n\n\n\n\nCode\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#hedonic-pricing-modelling-in-r",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#hedonic-pricing-modelling-in-r",
    "title": "Hands-on Ex 8: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "13.8 Hedonic Pricing Modelling in R",
    "text": "13.8 Hedonic Pricing Modelling in R\n\n13.8.1 Simple Linear Regression Method\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\n\nCode\ncondo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\n\nlm() returns an object of class “lm” or for multiple responses of class c(“mlm”, “lm”).\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\n\nCode\nsummary(condo.slr)\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n\\[\ny = -258121.1 + 14719x1\n\\]\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\n\nCode\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\n13.8.2 Multiple Linear Regression Method\n\n13.8.2.1 Visualising the relationships of the independent variables\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\n\nCode\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named “AOE”, “FPC”, “hclust”, “alphabet”. In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n13.8.3 Building a hedonic pricing model using multiple linear regression method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\n\nCode\ncondo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\n\n\n13.8.4 Preparing Publication Quality Table: olsrr method\nWith reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.\nNow, we are ready to calibrate the revised model by using the code chunk below.\n\n\nCode\ncondo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n\n\n\n13.8.5 Preparing Publication Quality Table: gtsummary method\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\n\nCode\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\nCode\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %>% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n13.8.5.1 Checking for multicolinearity\n\n\nCode\nols_vif_tol(condo.mlr1)\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n13.8.5.2 Test for Non-Linearity\n\n\nCode\nols_plot_resid_fit(condo.mlr1)\n\n\nThe figure above reveals that most of the data pointsare scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n13.8.5.3 Test for Normality Assumption\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\n\nCode\nols_plot_resid_hist(condo.mlr1)\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\n\nCode\nols_test_normality(condo.mlr1)\n\n\n\n\n13.8.5.4 Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\n\nCode\nmlr.output <- as.data.frame(condo.mlr1$residuals)\n\n\n\n\nCode\ncondo_resale.res.sf <- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %>%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n\n\n\n\nCode\ncondo_resale.sp <- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\n\n\n\nCode\ntmap_mode(\"view\")\n\n\n\n\nCode\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\nCode\nnb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nsummary(nb)\n\n\n\n\nCode\nnb_lw <- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\n\n\n\nCode\nlm.morantest(condo.mlr1, nb_lw)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Hands-on Ex 8: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "13.9 Building Hedonic Pricing Models using GWmodel",
    "text": "13.9 Building Hedonic Pricing Models using GWmodel\n\n13.9.1 Building Fixed Bandwidth GWR Model\n\n13.9.1.1 Computing fixed bandwith\n\n\nCode\nbw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\n\nThe result shows that the recommended bandwidth is 971.3405 metres.\n\n\n13.9.1.2 GWModel method - fixed bandwith\n\n\nCode\ngwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\n\n\n\nCode\ngwr.fixed\n\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n\n\n\n13.9.2 Building Adaptive Bandwidth GWR Model\n\n13.9.2.1 Computing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\n\nCode\nbw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\n13.9.2.2 Constructing the adaptive bandwidth gwr model\n\n\nCode\ngwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\n\n\n\nCode\ngwr.adaptive\n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\n13.9.3 Visualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n\n13.9.4 Converting SDF into sf data.frame\n\n\nCode\ncondo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%\n  st_transform(crs=3414)\n\n\n\n\nCode\ncondo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)\ncondo_resale.sf.adaptive.svy21  \n\n\n\n\nCode\ngwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\n\n\n\nCode\nglimpse(condo_resale.sf.adaptive)\n\n\n\n\nCode\nsummary(gwr.adaptive$SDF$yhat)\n\n\n\n\n13.9.5 Visualising local R2\n\n\nCode\ntmap_mode(\"view\")\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n\n13.9.6 Visualising coefficient estimates\n\n\nCode\ntmap_mode(\"view\")\nAREA_SQM_SE <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\n13.9.6.1 By URA Plannign Region\n\n\nCode\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reference",
    "href": "Hands-on_Ex/Hands-on_Ex08/Hands-on_Ex08.html#reference",
    "title": "Hands-on Ex 8: Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Reference",
    "text": "Reference"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "",
    "text": "Show the code\npacman::p_load(sf, tidyverse, funModeling)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#handling-geospatial-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#handling-geospatial-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "2. Handling Geospatial Data",
    "text": "2. Handling Geospatial Data\n\n2.1 Importing Geospatial\nRead the file from geoBoundaries.\n\n\nShow the code\ngeoNGA = st_read(dsn = \"data/geospatial/\", layer=\"geoBoundaries-NGA-ADM2\")%>%st_transform(crs=26392)\n\n#transforms data from decimal to metres\n\n\nRead the file from Humanitarian Data Exchange.\n\n\nShow the code\nNGA = st_read(dsn = \"data/geospatial/\", layer=\"nga_admbnda_adm2_osgof_20190417\")%>%st_transform(crs=26392)\n\n\n\n\n2.2 Handling Aspatial Data\nRead the file from and filter out where country name is Nigeria\n\n\nShow the code\nwp_nga <- read_csv(\"data/aspatial/wpdx.csv\") %>%\n  filter(clean_country_name %in% c(\"Nigeria\"))\n\n\nConverting an aspatial data into an sf data.frame involves two steps.\nFirst, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`new_georeferenced_column_`)\nwp_nga\n\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\n#Transforming into Nigeria projected coordinate system\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-data-cleaning",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-data-cleaning",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "3. Geospatial Data Cleaning",
    "text": "3. Geospatial Data Cleaning\nNGA sf data.frame consists of many redundent fields. The code chunk below uses select() of dplyr to retain column 3, 4, 8 and 9. Do you know why?\n\n3.1 Exclude redundant fields\n\n#exclude redundant fields\nNGA <- NGA %>%\n  select(3:4, 8:9)\n\n\n\n3.2 Checking for duplicate name\n\n#check for duplicate names\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n\n#lets correct the errors (suppose to manually find)\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nNow, let us rerun the code chunk below to confirm that the duplicated name issue has been addressed.\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-wrangling-for-water-point-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-wrangling-for-water-point-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4. Data Wrangling for Water Point Data",
    "text": "4. Data Wrangling for Water Point Data\nExploratory Data Analysis (EDA) is a popular approach to gain initial understanding of the data. In the code chunk below, freq() of funModeling package is used to reveal the distribution of water point status visually.\n\n#check frequency count\n\nfreq(data = wp_sf,\n     input = \"status_clean\")\n\n\n#replace NA fills with unknown\nwp_sf_nga <- wp_sf%>%\n  rename(status_clean = 'status_clean') %>%\n    select(status_clean) %>%\n    mutate(status_clean = replace_na(\n           status_clean, \"unknown\"))\n\n\n4.1 Extracting Water Point Data\n\n#filter out the NA values, extract functional water output\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Functional\",\n             \"Functional, needs repair\",\n             \"Functional, not in use\"\n           ))\n\n\n#extract non functional\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\n             \"Abandoned/Decommissioned\",\n             \"Non-Functional\",\n             \"Non-Functional, dry\"\n           ))\n\n\n#extract unknown\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\nNext, the code chunk below is used to perform a quick EDA on the derived sf data.frames.\n\nfreq(data = wp_functional,\n     input = 'status_clean')\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\nfreq(data = wp_unknown,\n     input = 'status_clean')\n\n\n\n4.2 Performing Point-in-Polygon Count\nNext, we want to find out the number of total, functional, nonfunctional and unknown water points in each LGA. This is performed in the following code chunk. First, it identifies the functional water points in each LGA by using st_intersects() of sf package. Next, length() is used to calculate the number of functional water points that fall inside each LGA.\n\n# extra step to cross check whether previously extracted correctly\nNGA_wp <- NGA %>%\n  mutate(`total_wp`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_functional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_nonfunctional`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  )) %>%\n  mutate(`wp_unknown`= lengths(\n    st_intersects(NGA, wp_sf_nga)\n  ))\n\n\n\n4.3 Visualing attributes by using statistical graphs\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             linewidth=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n#save in rds format (rds allow us to retain the data structure/simple feature with the data properties)\n\nwrite_rds(NGA_wp, \"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "",
    "text": "Show the code\npacman::p_load(sf, tidyverse, tmap)\n\nNGA_wp <- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#basic-chloropleth-map",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#basic-chloropleth-map",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "2. Basic Chloropleth Map",
    "text": "2. Basic Chloropleth Map\n\n\nShow the code\np1 <- tm_shape(NGA_wp) + \n  tm_fill(\"wp_functional\",\n          n=10, #10 classes\n          style=\"equal\", #classification method\n          palette=\"Blues\") + #always plural form\n  tm_borders(lwd = 0.1, #border thickness\n             alpha= 1) +\n  tm_layout(main.title = \"Distribution of functional water points\",\n            legend.outside = FALSE)\n\np2 <- tm_shape(NGA_wp) + \n  tm_fill(\"total_wp\",\n          n=10, #10 classes\n          style=\"equal\", #classification method\n          palette=\"Blues\") + #always plural form\n  tm_borders(lwd = 0.1, #border thickness\n             alpha= 1) +\n  tm_layout(main.title = \"Distribution of total water points\",\n            legend.outside = FALSE)\n\n\n\n\nShow the code\ntmap_arrange(p2,p1,nrow=1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#chloropleth-maps-for-rates",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#chloropleth-maps-for-rates",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "3. Chloropleth Maps for Rates",
    "text": "3. Chloropleth Maps for Rates\n\n3.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\n\n\nShow the code\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n\n3.2 Plotting map of rate\n\n\nShow the code\ntm_shape(NGA_wp) + \n  tm_fill(\"pct_functional\",\n          n=10, \n          style=\"equal\", \n          palette=\"Reds\") +\n  tm_borders(lwd = 0.1, \n             alpha= 1) +\n  tm_layout(main.title = \"Rate map of functional water points\",\n            legend.outside = FALSE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#extreme-value-maps",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#extreme-value-maps",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "4. Extreme Value Maps",
    "text": "4. Extreme Value Maps\n\n4.1 Percentile Map\n\n\n4.1.1 Data Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\n\nShow the code\n#NGA_wp <- NGA_wp %>%\n  #drop_na()\n\nNGA_wp <- NGA_wp %>%\n  na.omit()\n\n\n\n\n4.1.2 Why writing functions?\nStep 2: Creating customised classification and extracting values\n\n\nShow the code\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n\n\n\n4.1.3 Creating the get.var function\nWriting a function has three big advantages over using copy-and-paste:\nYou can give a function an evocative name that makes your code easier to understand. As requirements change, you only need to update code in one place, instead of many. You eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\n\nShow the code\nget.var <- function(vname,df){\n  v <- df[name] %>%\n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n\n4.1.4 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\n\nShow the code\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n\n4.1.5 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\n\nShow the code\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n4.2 Box Map\n\n\nShow the code\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n4.2.1 Creating the boxbreaks function\n\n\nShow the code\nboxbreaks <- function(v,mult=1.5) {\n  qv <- unname(quantile(v))\n  iqr <- qv[4] - qv[2]\n  upfence <- qv[4] + mult * iqr\n  lofence <- qv[2] - mult * iqr\n  # initialize break points vector\n  bb <- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence < qv[1]) {  # no lower outliers\n    bb[1] <- lofence\n    bb[2] <- floor(qv[1])\n  } else {\n    bb[2] <- lofence\n    bb[1] <- qv[1]\n  }\n  if (upfence > qv[5]) { # no upper outliers\n    bb[7] <- upfence\n    bb[6] <- ceiling(qv[5])\n  } else {\n    bb[6] <- upfence\n    bb[7] <- qv[5]\n  }\n  bb[3:5] <- qv[2:4]\n  return(bb)\n}\n\n\n\n\n4.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\narguments: vname: variable name (as character, in quotes) df: name of sf data frame returns: v: vector with values (without a column name)\n\n\nShow the code\nget.var <- function(vname,df) {\n  v <- df[vname] %>% st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\n\n\n4.2.3 Test drive the newly created function\nLet’s test the newly created function.\n\n\nShow the code\nvar <- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n\n\n\n4.2.4 Boxmap function\n\n\nShow the code\nboxmap <- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var <- get.var(vnam,df)\n  bb <- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"< 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"> 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)\n\n\n\n\n4.2.5 Recode zero\nThe code chunk below is used to recode LGAs with zero total water point into NA.\n\n\nShow the code\nNGA_wp <- NGA_wp %>%\n  mutate(wp_functional = na_if(\n    total_wp, total_wp < 0))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. The point can be location of:\n\nevents such as crime, traffic accident and disease onset, or\nbusiness services (coffee and fastfood outlets) or facilities such as childcare and eldercare.\n\nUsing appropriate functions of spatstat, this hands-on exercise aims to discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#getting-started",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#getting-started",
    "title": "In-class Exercise 4",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n\nShow the code\npacman::p_load(maptools, sf, raster, spatstat, tmap)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#spatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#spatial-data-wrangling",
    "title": "In-class Exercise 4",
    "section": "3. Spatial Data Wrangling",
    "text": "3. Spatial Data Wrangling\n\n3.1 Importing the spatial data\n\n\nShow the code\nchildcare_sf <- st_read(\"data/child-care-services-geojson.geojson\") %>%\n  st_transform(crs = 3414)\n\n\nReading layer `child-care-services-geojson' from data source \n  `C:\\RhondaHO\\IS415-GAA\\In-class_Ex\\In-class_Ex04\\data\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nShow the code\nsg_sf <- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\n\n\n\nShow the code\nmpsz_sf <- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\RhondaHO\\IS415-GAA\\In-class_Ex\\In-class_Ex04\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n3.2 Mapping the geospatial data sets\nTo adjust tmap view settings refer to here.\n\n\nShow the code\ntmap_mode('view') #by default\n\ntm_shape(childcare_sf) +\n  tm_dots(alph=0.5, # intensity of color\n          size=0.01) + #tm_bubbles alternate method of dots, tend to use if we want to create proportional maps\n  tm_view(\n  set.zoom.limits= c(11,14)) # set zoom limits (zoom out value, zoom in value)\n\n\n\n\n\n\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying ecessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\nShow the code\ntmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geospatial-data-wrangling",
    "title": "In-class Exercise 4",
    "section": "4. Geospatial Data wrangling",
    "text": "4. Geospatial Data wrangling\nThe first 3 steps are important for dealing with geospatial data wrangling. Source data must be in sf., source data needs to be in the same projection system as once it is converted, cannot tell.\n\n4.1 Converting sf data frames to sp’s Spatial* class\nThe code chunk below uses as_Spatial() of sf package to convert the three geospatial data from simple feature data frame to sp’s Spatial* class.\n\n\nShow the code\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\n\n\nShow the code\nchildcare\n\n\n\n\nShow the code\nmpsz\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.2 Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\n\n\nShow the code\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\n\n\nShow the code\nchildcare_sp\n\n\n\n\nShow the code\nsg\n\n\n\n\n4.3 Converting the generic sp format into spatstat’s ppp format\n\n\nShow the code\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nplot(childcare_ppp)\n\n\n\n\nShow the code\nsummary(childcare_ppp)\n\n\n\n\n4.4 Handling duplicated points\n\n\nShow the code\n#check for duplication\nany(duplicated(childcare_ppp))\n\n#count the number of co-indicence point\nmultiplicity(childcare_ppp)\n\n#how many locations have more than one point event\nsum(multiplicity(childcare_ppp) > 1)\n\n\n\n\nShow the code\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\nShow the code\ntmap_mode('plot')\n\n\n\n\nShow the code\nchildcare_ppp_jit <- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\n\n\n\nShow the code\nany(duplicated(childcare_ppp_jit))\n\n\n\n\n4.5 Creating owin object\n\n\nShow the code\nsg_owin <- as(sg_sp, \"owin\")\nplot(sg_owin)\n\n\n\n\nShow the code\nsummary(sg_owin)\n\n\n\n\n4.6 Combining point events object and owin object\nExtract childcare events that are located within Singapore\n\n\nShow the code\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nShow the code\nsummary(childcareSG_ppp)\n\n\n\n\n4.6 First-order Spatial Point Patterns Analysis\n\n4.6.1 Kernel Density Estimation\n\n4.6.1.1 Computing kernel density estimation using automatic bandwidth selection method\n\n\nShow the code\nkde_childcareSG_bw <- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\n\n\n\nShow the code\nplot(kde_childcareSG_bw)\n\n\nRetrieve the bandwidth used to compute the kde layer\n\n\nShow the code\nbw <- bw.diggle(childcareSG_ppp)\nbw\n\n\n\n\n4.6.1.2 Rescalling KDE values\nRescale() is used to covert the unit of measurement from meter to kilometer.\n\n\nShow the code\nchildcareSG_ppp.km <- rescale(childcareSG_ppp, 1000, \"km\")\n\n\n\n\nShow the code\nkde_childcareSG.bw <- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\n\n4.6.2 Working with different automatic badwidth methods\nOther spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\n\n\nShow the code\nbw.CvL(childcareSG_ppp.km)\nbw.scott(childcareSG_ppp.km)\n\n#tends to produce the more appropriate values when the pattern consists predominantly of tight clusters\nbw.ppl(childcareSG_ppp.km)\n\n#Best Method to detect a single tight cluster in the midst of random noise\nbw.diggle(childcareSG_ppp.km)\n\n\nCompare the output of using bw.diggle and bw.ppl methods.\n\n\nShow the code\nkde_childcareSG.ppl <- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n4.6.3 Working with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\n\n\n\n\n4.7 Fixed and Adaptive KDE\n\n4.7.1 Computing KDE by using fixed bandwidth\n\n\nShow the code\nkde_childcareSG_600 <- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n4.7.2 Computing KDE by using adaptive bandwidth\nDerive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\n\nShow the code\nkde_childcareSG_adaptive <- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\n\nShow the code\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n4.7.3 Converting KDE output into grid object\n\n\nShow the code\ngridded_kde_childcareSG_bw <- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n4.7.3.1 Converting gridded output into raster\n\n\nShow the code\n#Convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\nkde_childcareSG_bw_raster <-raster(gridded_kde_childcareSG_bw)\n\nkde_childcareSG_bw_raster\n\n\n\n\n4.7.3.2 Assigning projection systems\n\n\nShow the code\nprojection(kde_childcareSG_bw_raster) <-CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\n\n\n\n4.7.4 Visualising the output in tmap\n\n\nShow the code\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n4.7.5 Comparing Spatial Point Patterns using KDE\nCompare KDE of childcare at Ponggol, Tampines, Chua Chu Kang and Jurong West planning areas.\n\n4.7.5.1 Extracting study area\n\n\nShow the code\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\n\nPlotting of target planning areas\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")\n\n\n\n\n4.7.5.2 Converting the spatial point data frame into generic sp format\n\n\nShow the code\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\n\n\n\n4.7.5.3 Creating owin object\n\n\nShow the code\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\n\n\n\n\n4.7.5.4 Combining childcare points and the study area\n\n\nShow the code\n#Extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\n# Next, rescale() function is used to trasnform the unit of measurement from metre to kilometre.\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\n#The code chunk below is used to plot these four study areas and the locations of the childcare centres.\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\n4.7.5.5 Computing KDE\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Jurong West\")\n\n\n\n\n4.7.5.6 Computing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth.\n\n\nShow the code\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\n\n\n\n\n4.8 Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used. #### 4.8.1 Testing spatial point patterns using Clark and Evans Test\n\n\nShow the code\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n\n4.8.2 Clark and Evans Test: Choa Chu Kang planning area\n\n\nShow the code\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n\n\n4.8.3 Clark and Evans Test: Tampines planning area\n\n\nShow the code\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/data/stores.html",
    "href": "In-class_Ex/In-class_Ex05/data/stores.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     \n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/data/study_area.html",
    "href": "In-class_Ex/In-class_Ex05/data/study_area.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "Exploration of Local Co-Location Quotient."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#getting-started",
    "title": "IS415-GAA",
    "section": "2. Getting Started",
    "text": "2. Getting Started\n\n\nShow the code\npacman::p_load(maptools, sf, raster, spatstat, tmap, sfdep,dplyr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#importing-data",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#importing-data",
    "title": "IS415-GAA",
    "section": "3. Importing Data",
    "text": "3. Importing Data\n\n\nShow the code\nstudyArea<-st_read(dsn=\"data\", layer=\"study_area\")%>%\n  st_transform(crs=3829) #convert to local projectory system (epsg)\n\n\n\n\nShow the code\nstores<-st_read(dsn=\"data\", layer=\"stores\")%>%\n  st_transform(crs=3829)\n\n\n\n\nShow the code\nfamily = subset(stores, Name == \"Family Mart\")\nseven = subset(stores, Name == \"7-Eleven\")\n\ncombined <- base::rbind(family, seven)\n\n\n\n\nShow the code\ntmap_mode('view') #by default\ntm_shape(studyArea) +\n  tm_polygons() +\ntm_shape(family) +\n  tm_dots(col=\"Name\",\n          size=0.01,\n          border.col=\"black\",\n          border.lwd=0.5)\ntm_shape(seven) +\n  tm_dots(col=\"Name\",\n          size=0.01,\n          border.col=\"black\",\n          border.lwd=0.5) +\n  tm_view(set.zoom.limits= c(12,16))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#local-colocation-quotients-lclq",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#local-colocation-quotients-lclq",
    "title": "IS415-GAA",
    "section": "4. Local Colocation Quotients (LCLQ)",
    "text": "4. Local Colocation Quotients (LCLQ)\n\n\nShow the code\ntmap_mode('view') #by default\ntm_shape(studyArea) +\n  tm_polygons() +\ntm_shape(stores) +\n  tm_dots(col=\"Name\",\n          size=0.01,\n          border.col=\"black\",\n          border.lwd=0.5) +\n  tm_view(set.zoom.limits= c(12,16)) \n\n\n\n\nShow the code\ntmap_mode('plot')\n\n\n\n\nShow the code\nnb <- include_self(\n  st_knn(st_geometry(stores),6) # 6 nearest neighbours (should be even as it include self)\n)\n\nwt <- st_kernel_weights(nb,\n                        stores,\n                        \"gaussian\",\n                        adaptive=TRUE)\n\n#nearer target higher weightes\n\nFamilyMart <- stores %>%\n  filter(Name == \"Family Mart\")\n\nA <- FamilyMart$Name\n\nSevenEleven <- stores %>%\n  filter(Name == \"7-Eleven\")\nB <- SevenEleven$Name\n\nLCLQ <- local_colocation(A, B, nb, wt, 49) #will not see p-value\nLCLQ_stores <- cbind(stores, LCLQ)\n\n#plot lclq"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06.html",
    "title": "Hands-on Ex 6: Spatial Weights and Applications",
    "section": "",
    "text": "Code\npacman::p_load(sf, sfdep, tmap, tidyverse, knitr)\n\n#inclass uses sfdep, handson uses spdep\n\n\n\n\n\n\n\n\n\nCode\n# st_read from sf package \n\n# task: make it into sf df\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\") \n\n\n\n\n\n\n\nCode\n#tibble df\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n#head(hunan,5) colnames(hunan2012)\n\n\n\n\n\n\n\nCode\n# left join-> missing a unique identifier but it automatically matches county\n\nhunan_GDPC <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)\n\n\n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape (hunan_GDPC) + \n  tm_fill(\"GDPPC\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\ntm_layout ( main.title = \"Distribution of GDP per capita by distribution\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\ntm_borders (alpha = 0.5) +\ntm_compass (type=\"8star\", size = 2) +\ntm_scale_bar() + \ntm_grid(alpha =0.2)\n\n# tmap auto changes from decimal to kilometres using great circle formula\n\n\n\n\n\n\n\nCode\n# using queen's method (default value of function)\n\ncn_queen <- hunan_GDPC %>%\n  mutate(nb= st_contiguity(geometry),\n         .before=1) # put newly created field at the first col\n\n#View(cn_queen)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat about the Rook’s method?\n\n\n\n\nCode\ncn_rook <- hunan_GDPC %>%\n  mutate(nb= st_contiguity(geometry),\n         queen= FALSE,\n         .before=1)\n\n\n\n\nCode\n# sfdep removes the earlier step and conbines 2 function/ it is redundant\n\nwm_q <- hunan_GDPC %>%\n  mutate(nb= st_contiguity(geometry),\n         wt = st_weights(nb),\n         .before=1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07.html",
    "title": "In-class Ex 7",
    "section": "",
    "text": "Getting Started\n\n\nCode\npacman::p_load(sf, sfdep, tmap, tidyverse)\n\n\n\n\nImporting the Data\n\n\nCode\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\") \n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n#head(hunan,5) colnames(hunan2012)\n\nhunan_GDPC <- left_join(hunan,hunan2012)%>%\n  select(1:4, 7, 15)\n\n\n\n\nCode\nwm_q <- hunan_GDPC %>%\n  mutate(nb= st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style=\"W\"), \n                          #W refers to row standardisation\n         .before=1)\n\n\nComputing global moran’I (redundant step)\n\n\nCode\nmoranI <- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\nPerforming global moran’I test -> gives test results and statistics\n\n\nCode\nglobal_moranI <- global_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\nglobal_moranI\n\n\np-value = 1.095e-06 less that 0.05,\n\n\nCode\nglobal_moranI$I\n\nglobal_moranI$K\n\n\nWe should set a seed number to make our results reproducible.\n\n\nCode\nset.seed(1234)\n\n\nPerforming global moran’I test with permutations The code chunk below intends to run 100 simulations (total simulations = 1+ nsim=99).\n\n\nCode\n# with permutations, \n\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim=99)\n\n\nIt shows 2 sided because it is a large sample test.\nComputing Local Moran’I\n\n\nCode\nlisa <- wm_q %>% \n  mutate(local_moran =local_moran(\n    GDPPC, nb, wt, nsim=99),\n    .before=1)%>%\n  unnest(local_moran) \n  #need to unnest the list to be able to use the values\n\nlisa\n\n#z ii is standardised\n# p ii is the permutations\n\n# This step is helpful as we dont hve to manually integrate like we did in the handson exercise (handson ex7 steps 10.7.3 - qunadrant <- vector....)\n\n#general way: mean and pysal(python lib that does the same thing as finding the mean) should be the same\n\n#can use the mean for the takehome assgn , dont hve worry about the median and pysal (feel free to explore)\n\n\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\") + #using the simulations is btr than p_ii cos results will be more stable\n  tm_borders(alpha=0.5)\n  #tm_view(set.zoom.limits=c(6,8))\n\n\n\n\nCode\n# takehome assgn should use gstar instead of lisa \n\nlisa_sig <-lisa %>%\n  filter(p_ii<0.05)\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() + \n  tm_borders(alpha=0.5)+\n\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha=0.4)\n  #tm_view(set.zoom.limits=c(6,8))\n\n\nComputing local Moran’s I\n\n\nCode\nHCSA <- wm_q %>%\n  mutate(local_Gi = local_gstar_perm(\n    GDPPC, nb, wt, nsim=99),\n    .before=1) %>%\n  unnest(local_Gi)\n\nHCSA\n\n\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape(HCSA) +\n  tm_fill(\"p_sim\") +\n  tm_borders(alpha=0.05)\n\n# need to make sure to be able to see the significance level\n\n\n\n\nCode\n# GDPPC_nb <- GDPPC_st %>% \n  #activate(\"geometry\") %>%\n  #mutate(\n   # nb = include_self(st_continuity(geometry)),\n    #wt = st_weights(nb)\n  #) %>%\n  #set_nbs(\"nb\") %>%\n  #set_wts(\"wt\")\n\n\n\n\n\n\n\nCode\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nGDPPC <- read_csv(\"data/aspatial/Hunan_GDPPC.csv\")\n\n\n\n\nCode\nGDPPC_st <- spacetime(GDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n\n\n\n\nCode\nis_spacetime_cube(GDPPC_st)\n\n\n\n\nCode\nglimpse(GDPPC)\nclass(GDPPC$Year)\ntypeof(GDPPC$Year)\n\n\n\n\nCode\nGDPPC_nb <- GDPPC_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n\ngi_stars <- GDPPC_nb %>% \n  group_by(Year) %>% \n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %>% \n  tidyr::unnest(gi_star)\n\n\ncbg <- gi_stars %>% \n  ungroup() %>% \n  filter(County == \"Changsha\") |> \n  select(County, Year, gi_star)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html",
    "title": "In-class Ex 8",
    "section": "",
    "text": "Notes on takehome ex2"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#geospatial-data",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#geospatial-data",
    "title": "In-class Ex 8",
    "section": "Geospatial Data",
    "text": "Geospatial Data\n\n\nCode\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\n#update crs\nmpsz_svy21 <- st_transform(mpsz, 3414)\n\nst_bbox(mpsz_svy21) #view extent"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#aspatial-data",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#aspatial-data",
    "title": "In-class Ex 8",
    "section": "Aspatial Data",
    "text": "Aspatial Data\n\n\nCode\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\n\n\nCode\n# to check for excessive no. of 0, check the spread to determine whether we can use the variable (cannot use if min median mean ALL 0 or ALL same value)\nsummary(condo_resale)\n\n\nConverting aspatial data frame into a sf object\n\n\nCode\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %>%\n  st_transform(crs=3414)\n\n\nSave entire model into condo.mlr . Note that condo.mlr is an lm object (contains all impt variables for least square model).\n\n\nCode\ncondo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex09/data/geospatial/MPSZ-2019.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "",
    "text": "Hello! This is Rhonda Ho’s take-home Assignment 1 for IS415 module.\nTo view/hide all the code at once, please click on the “</> code” tab beside the title of this html document and select the option to view/hide the code.\nThe full details of this assignment can be found here.\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, I am tasked to apply appropriate spatial point patterns analysis methods to discover the geographical distribution of functional and non-function water points and their co-locations if any in Osun State, Nigeria.\n\n\n\n\nApstial data\n\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used. There are two versions of the data. They are: WPdx-Basic and WPdx+. You are required to use WPdx+ data set.\n\nGeospatial data\n\nThis study will focus of Osun State, Nigeria. The state boundary GIS data of Nigeria can be downloaded either from The Humanitarian Data Exchange portal or geoBoundaries.\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nExploratory Spatial Data Analysis (ESDA)\n\nDerive kernel density maps of functional and non-functional water points. Using appropriate tmap functions,\nDisplay the kernel density maps on openstreetmap of Osub State, Nigeria.\nDescribe the spatial patterns revealed by the kernel density maps. Highlight the advantage of kernel density map over point map.\n\nSecond-order Spatial Point Patterns Analysis\nWith reference to the spatial point patterns observed in ESDA:\n\nFormulate the null hypothesis and alternative hypothesis and select the confidence level.\nPerform the test by using appropriate Second order spatial point patterns analysis technique.\nWith reference to the analysis results, draw statistical conclusions.\n\nSpatial Correlation Analysis\nIn this section, you are required to confirm statistically if the spatial distribution of functional and non-functional water points are independent from each other.\n\nFormulate the null hypothesis and alternative hypothesis and select the confidence level.\nPerform the test by using appropriate Second order spatial point patterns analysis technique.\nWith reference to the analysis results, draw statistical conclusions."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#importing-geospatial-data",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#importing-geospatial-data",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "3.1 Importing Geospatial Data",
    "text": "3.1 Importing Geospatial Data\nIn this section, st_read() of sf package will be used to import the 2 geospatial data sets into R.\n\n3.1.1 The Geoboundaries Dataset\nFor Nigeria, there are three Projected Coordinate Systems of Nigeria, which is EPSG: 26391, 26392, and 26303. For this assignment, I will be using EPSG:26392.\n\n\nCode\ngeoNGA <- st_read(\"data/geospatial/\",\n                  layer = \"geoBoundaries-NGA-ADM2\") %>%\n  st_transform(crs = 26392)\n\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\RhondaHO\\IS415-GAA\\Take-home_Assgn\\Take-home_Assgn1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nTo understand more about the columns and data across our dataset, I used the function glimpse() as shown in the code chunk below.\n\n\nCode\nglimpse(geoNGA)\n\n\nRows: 774\nColumns: 6\n$ shapeName  <chr> \"Aba North\", \"Aba South\", \"Abadam\", \"Abaji\", \"Abak\", \"Abaka…\n$ Level      <chr> \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"AD…\n$ shapeID    <chr> \"NGA-ADM2-72505758B79815894\", \"NGA-ADM2-72505758B67905963\",…\n$ shapeGroup <chr> \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NGA\", \"NG…\n$ shapeType  <chr> \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"ADM2\", \"AD…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((548795.5 11..., MULTIPOLYGON (…\n\n\n\n\n3.1.2 The NGA Dataset\n\n\nCode\nNGA <- st_read(\"data/geospatial/\",\n               layer = \"nga_admbnda_adm2_osgof_20190417\") %>%\n  st_transform(crs = 26392)\n\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `C:\\RhondaHO\\IS415-GAA\\Take-home_Assgn\\Take-home_Assgn1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\nglimpse(NGA)\n\n\nRows: 774\nColumns: 17\n$ Shape_Leng <dbl> 0.2370744, 0.2624772, 3.0753158, 2.5379842, 0.6871498, 1.06…\n$ Shape_Area <dbl> 0.0015239210, 0.0035311037, 0.3268678399, 0.0683785064, 0.0…\n$ ADM2_EN    <chr> \"Aba North\", \"Aba South\", \"Abadam\", \"Abaji\", \"Abak\", \"Abaka…\n$ ADM2_PCODE <chr> \"NG001001\", \"NG001002\", \"NG008001\", \"NG015001\", \"NG003001\",…\n$ ADM2_REF   <chr> \"Aba North\", \"Aba South\", \"Abadam\", \"Abaji\", \"Abak\", \"Abaka…\n$ ADM2ALT1EN <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM2ALT2EN <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ ADM1_EN    <chr> \"Abia\", \"Abia\", \"Borno\", \"Federal Capital Territory\", \"Akwa…\n$ ADM1_PCODE <chr> \"NG001\", \"NG001\", \"NG008\", \"NG015\", \"NG003\", \"NG011\", \"NG02…\n$ ADM0_EN    <chr> \"Nigeria\", \"Nigeria\", \"Nigeria\", \"Nigeria\", \"Nigeria\", \"Nig…\n$ ADM0_PCODE <chr> \"NG\", \"NG\", \"NG\", \"NG\", \"NG\", \"NG\", \"NG\", \"NG\", \"NG\", \"NG\",…\n$ date       <date> 2016-11-29, 2016-11-29, 2016-11-29, 2016-11-29, 2016-11-29…\n$ validOn    <date> 2019-04-17, 2019-04-17, 2019-04-17, 2019-04-17, 2019-04-17…\n$ validTo    <date> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ SD_EN      <chr> \"Abia South\", \"Abia South\", \"Borno North\", \"Federal Capital…\n$ SD_PCODE   <chr> \"NG00103\", \"NG00103\", \"NG00802\", \"NG01501\", \"NG00302\", \"NG0…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((548795.5 11..., MULTIPOLYGON (…\n\n\nBy examining both of the sf dataframes closely, we can observe that the NGA dataset provides us with more information with regards to the state i.e Osun which we require to perform our tasks. Hence, NGA data.frame will be used for the subsequent processing.\n\n\n3.2 Importing Aspatial Data\nMoving on to the Aspatial data, as it is in an excel format, I decided to use read_csv()function. As the area of study for this task is focused on Osun State, Nigeria, I then filtered out the values accordingly using the filter() function. But how do we know which column to filter by? First, I read up on the description of the metadata in the Aspatial data here and discovered that I needed to filter out the country, Nigeria under the ‘clean_country_name’.\n\n\nCode\nwp_nga <- read_csv(\"data/aspatial/wpdx.csv\") %>%\n  filter(clean_country_name %in% c(\"Nigeria\"))\n\n\nNext, I took a closer look at the data and discovered that the states of Nigeria are splitted in to 4 divisions. To determine where Osun is, I used the function, any() to check which column Osun belonged to.\n\n\nCode\nany(wp_nga$clean_adm1==\"Osun\")\n\n\n[1] TRUE\n\n\nCode\nany(wp_nga$clean_adm2==\"Osun\")\n\n\n[1] FALSE\n\n\nCode\nany(wp_nga$clean_adm3==\"Osun\")\n\n\n[1] NA\n\n\nCode\nany(wp_nga$clean_adm4==\"Osun\")\n\n\n[1] NA\n\n\nBased on the output above, Osun only exists under the column ‘clean_adm1’, so I filtered out the Osun state in that specific column.\n\n\nCode\nwp_nga <- read_csv(\"data/aspatial/wpdx.csv\") %>%\n  filter(clean_adm1 %in% c(\"Osun\"))\n\n\nNext, we need to convert the aspatial data into sf data.frame.\nTo do so, it requires two steps. First, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\n\n\nCode\nwp_nga$Geometry = st_as_sfc(wp_nga$`new_georeferenced_column_`)\nwp_nga\n\n\n# A tibble: 35 × 75\n   row_id source     lat_deg lon_deg report_date         statu…¹ water…² water…³\n    <dbl> <chr>        <dbl>   <dbl> <dttm>              <chr>   <chr>   <chr>  \n 1 225950 Federal M…    7.43    4.26 2015-05-05 00:00:00 Yes     Boreho… Well   \n 2 225524 Federal M…    7.78    4.56 2015-04-22 00:00:00 Yes     Protec… Well   \n 3 197014 Federal M…    7.49    4.53 2015-04-30 00:00:00 Yes     Boreho… Well   \n 4 225173 Federal M…    7.93    4.73 2015-05-02 00:00:00 Yes     Boreho… Well   \n 5 225843 Federal M…    7.74    4.44 2015-05-08 00:00:00 Yes     Boreho… Well   \n 6 235508 Federal M…    7.15    4.64 2015-04-27 00:00:00 Yes     Protec… Well   \n 7 197708 Federal M…    7.87    4.72 2015-05-13 00:00:00 Yes     Boreho… Well   \n 8 195041 Federal M…    7.73    4.45 2015-06-17 00:00:00 Yes     Protec… Spring \n 9 225222 Federal M…    7.81    4.15 2015-05-14 00:00:00 Yes     Protec… Spring \n10 460770 GRID3         7.4     4.33 2018-06-13 00:00:00 Unknown Boreho… Well   \n# … with 25 more rows, 67 more variables: water_tech_clean <chr>,\n#   `_water_tech_category` <chr>, facility_type <chr>,\n#   clean_country_name <chr>, clean_adm1 <chr>, clean_adm2 <chr>,\n#   clean_adm3 <lgl>, clean_adm4 <lgl>, install_year <dbl>, installer <chr>,\n#   rehab_year <lgl>, rehabilitator <chr>, management_clean <chr>,\n#   status_clean <chr>, pay <chr>, fecal_coliform_presence <lgl>,\n#   fecal_coliform_value <dbl>, subjective_quality <chr>, activity_id <chr>, …\n\n\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\n\nCode\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\n\nSimple feature collection with 35 features and 74 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4.146705 ymin: 7.153846 xmax: 5 ymax: 8.031187\nGeodetic CRS:  WGS 84\n# A tibble: 35 × 75\n   row_id source     lat_deg lon_deg report_date         statu…¹ water…² water…³\n *  <dbl> <chr>        <dbl>   <dbl> <dttm>              <chr>   <chr>   <chr>  \n 1 225950 Federal M…    7.43    4.26 2015-05-05 00:00:00 Yes     Boreho… Well   \n 2 225524 Federal M…    7.78    4.56 2015-04-22 00:00:00 Yes     Protec… Well   \n 3 197014 Federal M…    7.49    4.53 2015-04-30 00:00:00 Yes     Boreho… Well   \n 4 225173 Federal M…    7.93    4.73 2015-05-02 00:00:00 Yes     Boreho… Well   \n 5 225843 Federal M…    7.74    4.44 2015-05-08 00:00:00 Yes     Boreho… Well   \n 6 235508 Federal M…    7.15    4.64 2015-04-27 00:00:00 Yes     Protec… Well   \n 7 197708 Federal M…    7.87    4.72 2015-05-13 00:00:00 Yes     Boreho… Well   \n 8 195041 Federal M…    7.73    4.45 2015-06-17 00:00:00 Yes     Protec… Spring \n 9 225222 Federal M…    7.81    4.15 2015-05-14 00:00:00 Yes     Protec… Spring \n10 460770 GRID3         7.4     4.33 2018-06-13 00:00:00 Unknown Boreho… Well   \n# … with 25 more rows, 67 more variables: water_tech_clean <chr>,\n#   `_water_tech_category` <chr>, facility_type <chr>,\n#   clean_country_name <chr>, clean_adm1 <chr>, clean_adm2 <chr>,\n#   clean_adm3 <lgl>, clean_adm4 <lgl>, install_year <dbl>, installer <chr>,\n#   rehab_year <lgl>, rehabilitator <chr>, management_clean <chr>,\n#   status_clean <chr>, pay <chr>, fecal_coliform_presence <lgl>,\n#   fecal_coliform_value <dbl>, subjective_quality <chr>, activity_id <chr>, …\n\n\nAfterwards, we need to transform the projection from wgs84 to appropriate projected coordinate system of Nigeria i.e 26392.\n\n\nCode\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#exclude-redundant-fields",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#exclude-redundant-fields",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "4.1 Exclude redundant fields",
    "text": "4.1 Exclude redundant fields\nNGA sf data.frame consists of many redundent fields. Thus, I used select() of dplyr to retain the relevant columns which contain the values under the state of Osun.\n\n\nCode\n# method 1: using select function\nNGA <- NGA %>%\n  dplyr::select(c(3:4,8:9))\n\n#dplyr:: is used as there may be library conflicts which prevents me from using the code\n\n# method 2: keeping the column by name\n#keeps <- c(\"ADM2_EN\",\"ADM2_PCODE\",\"ADM1_EN\",\"ADM1_PCODE\")\n#NGA = NGA[keeps]\n\n\nAfterwards, I filtered the column ‘ADM1_EN’ to only include Osun Values.\n\n\nCode\nNGA<- NGA %>%\n  filter(ADM1_EN %in% c(\"Osun\"))"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#check-for-duplicate-names",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#check-for-duplicate-names",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "4.2 Check for duplicate names",
    "text": "4.2 Check for duplicate names\nFor the code chunk below, the function duplicate() is used to check for any duplicated values.\n\n\nCode\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n\ncharacter(0)\n\n\nHence, based on the output above, there is no duplicated values."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-sf-data-frames-to-sps-spatial-class",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-sf-data-frames-to-sps-spatial-class",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "5.1 Converting sf data frames to sp’s Spatial* class",
    "text": "5.1 Converting sf data frames to sp’s Spatial* class\nNext, since the task requires us to perform exploratory spatial data analysis (ESDA), we need to convert simple feature data frame to sp’s Spatial* class using the as_Spatial() function.\n\n\nCode\n#overview of wp in Ossun state\nwp_spatial <- as_Spatial(wp_sf)\n\n#only functional wp in Ossun state\nwp_func_spatial <- as_Spatial(wp_functional)\n\n#only non-functional wp in Ossun state\nwp_nonfunc_spatial <- as_Spatial(wp_nonfunctional)\n\n#NGA dataset\nNGA_spatial <- as_Spatial(NGA)\n\n\nTo further understand our data, we run the code chunk below.\n\n\nCode\nwp_spatial\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 35 \nextent      : 190156.4, 284643.8, 350391.9, 447614.1  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 74\nnames       : row_id,                                       source,   lat_deg,  lon_deg, report_date, status_id,      water_source_clean, water_source_category, water_tech_clean, X_water_tech_category, facility_type, clean_country_name, clean_adm1, clean_adm2, clean_adm3, ... \nmin values  : 194558, Federal Ministry of Water Resources, Nigeria,  7.153846, 4.146705,  1429660800,        No,                Borehole,                Spring,        Hand Pump,             Hand Pump,      Improved,            Nigeria,       Osun,   Aiyedade,         NA, ... \nmax values  : 684595,                                        GRID3, 8.0311867,        5,  1535500800,       Yes, Undefined Hand Dug Well,                  Well,  Mechanized Pump,       Mechanized Pump,      Improved,            Nigeria,       Osun,     Osogbo,         NA, ... \n\n\n\n\nCode\nNGA_spatial\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 30 \nextent      : 176503.2, 291043.8, 331434.7, 454520.1  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 4\nnames       :  ADM2_EN, ADM2_PCODE, ADM1_EN, ADM1_PCODE \nmin values  : Aiyedade,   NG030001,    Osun,      NG030 \nmax values  :   Osogbo,   NG030030,    Osun,      NG030 \n\n\nLooking at the output above, we understand that wp_spatial belongs to the SpatialPointsDataFrame while NGA_spatial belongs to SpatialPolygonsDataFrame class. This will help us in the next section which is the conversion of Spatial* class into generic sp format."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-the-spatial-class-into-generic-sp-format",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-the-spatial-class-into-generic-sp-format",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "5.2 Converting the Spatial* class into generic sp format",
    "text": "5.2 Converting the Spatial* class into generic sp format\nAs spatstat requires the analytical data in ppp object form. We need to convert the Spatial classes* into Spatial object first. The codes chunk below converts the Spatial* classes into generic sp objects.\n\n\nCode\nwp_sp <- as(wp_spatial, \"SpatialPoints\")\nwp_func_sp <- as(wp_func_spatial, \"SpatialPoints\")\nwp_nonfunc_sp <- as(wp_nonfunc_spatial, \"SpatialPoints\")\n\nNGA_sp <- as(NGA_spatial, \"SpatialPolygons\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "5.3 Converting the generic sp format into spatstat’s ppp format",
    "text": "5.3 Converting the generic sp format into spatstat’s ppp format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\n\nCode\nwp_ppp <- as(wp_sp, \"ppp\")\nwp_ppp\n\n\nPlanar point pattern: 35 points\nwindow: rectangle = [190156.37, 284643.78] x [350391.9, 447614.1] units\n\n\nCode\nwp_func_ppp <- as(wp_func_sp, \"ppp\")\n\n\nwp_nonfunc_ppp <- as(wp_nonfunc_sp, \"ppp\")\n\n\nTo further understand our data, let’s look at its summary statistics.\n\n\nCode\nsummary(wp_ppp)\n\n\nPlanar point pattern:  35 points\nAverage intensity 3.810031e-09 points per square unit\n\nCoordinates are given to 2 decimal places\ni.e. rounded to the nearest multiple of 0.01 units\n\nWindow: rectangle = [190156.37, 284643.78] x [350391.9, 447614.1] units\n                    (94490 x 97220 units)\nWindow area = 9186280000 square units\n\n\nBased on the output above, fortunately, we do not see a warning messages about duplicates. The code chunk below shows an alternate method of checking for duplicates. If it return FALSE, that means there is no duplicated values.\n\n\nCode\nany(duplicated(wp_ppp))\n\n\n[1] FALSE\n\n\nCode\nany(duplicated(wp_func_ppp))\n\n\n[1] FALSE\n\n\nCode\nany(duplicated(wp_nonfunc_ppp))\n\n\n[1] FALSE"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#creating-owin-object",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#creating-owin-object",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "5.4 Creating owin object",
    "text": "5.4 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area, for example, Nigeria’s boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to convert Nigera, Ossun stats SpatialPolygon object into owin object of spatstat.\n\n\nCode\nNGA_owin <- as(NGA_sp, \"owin\")\nplot(NGA_owin)\n\n\n\n\n\nCode\nsummary(NGA_owin)\n\n\nWindow: polygonal boundary\n30 separate polygons (no holes)\n            vertices      area relative.area\npolygon 1        204 766084000       0.08870\npolygon 2         81 304399000       0.03520\npolygon 3         97 465688000       0.05390\npolygon 4        124 373051000       0.04320\npolygon 5         60 149473000       0.01730\npolygon 6         84 144820000       0.01680\npolygon 7         50 102243000       0.01180\npolygon 8         72 216002000       0.02500\npolygon 9        112 269897000       0.03130\npolygon 10       125 365142000       0.04230\npolygon 11        83 111191000       0.01290\npolygon 12       126 192557000       0.02230\npolygon 13       219 904397000       0.10500\npolygon 14       174 741131000       0.08580\npolygon 15        81 138742000       0.01610\npolygon 16        65 119452000       0.01380\npolygon 17        90 280205000       0.03240\npolygon 18        69  69814600       0.00808\npolygon 19        69  42727500       0.00495\npolygon 20        49  30458800       0.00353\npolygon 21        62 263505000       0.03050\npolygon 22        93 438930000       0.05080\npolygon 23        87 274127000       0.03170\npolygon 24       105 509979000       0.05910\npolygon 25        98 292058000       0.03380\npolygon 26        64 327765000       0.03800\npolygon 27       133 108945000       0.01260\npolygon 28       122 462169000       0.05350\npolygon 29        94 109715000       0.01270\npolygon 30        95  61239800       0.00709\nenclosing rectangle: [176503.22, 291043.82] x [331434.7, 454520.1] units\n                     (114500 x 123100 units)\nWindow area = 8635910000 square units\nFraction of frame area: 0.613"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#combining-point-events-object-and-owin-object",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#combining-point-events-object-and-owin-object",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "5.5 Combining point events object and owin object",
    "text": "5.5 Combining point events object and owin object\nIn this last step of geospatial data wrangling, we will extract all waterpoints that are located within Nigeria Ossun by using the code chunk below.\n\n\nCode\nwpNGA_ppp = wp_ppp[NGA_owin]\nsummary(wpNGA_ppp)\n\n\nPlanar point pattern:  34 points\nAverage intensity 3.937051e-09 points per square unit\n\nCoordinates are given to 2 decimal places\ni.e. rounded to the nearest multiple of 0.01 units\n\nWindow: polygonal boundary\n30 separate polygons (no holes)\n            vertices      area relative.area\npolygon 1        204 766084000       0.08870\npolygon 2         81 304399000       0.03520\npolygon 3         97 465688000       0.05390\npolygon 4        124 373051000       0.04320\npolygon 5         60 149473000       0.01730\npolygon 6         84 144820000       0.01680\npolygon 7         50 102243000       0.01180\npolygon 8         72 216002000       0.02500\npolygon 9        112 269897000       0.03130\npolygon 10       125 365142000       0.04230\npolygon 11        83 111191000       0.01290\npolygon 12       126 192557000       0.02230\npolygon 13       219 904397000       0.10500\npolygon 14       174 741131000       0.08580\npolygon 15        81 138742000       0.01610\npolygon 16        65 119452000       0.01380\npolygon 17        90 280205000       0.03240\npolygon 18        69  69814600       0.00808\npolygon 19        69  42727500       0.00495\npolygon 20        49  30458800       0.00353\npolygon 21        62 263505000       0.03050\npolygon 22        93 438930000       0.05080\npolygon 23        87 274127000       0.03170\npolygon 24       105 509979000       0.05910\npolygon 25        98 292058000       0.03380\npolygon 26        64 327765000       0.03800\npolygon 27       133 108945000       0.01260\npolygon 28       122 462169000       0.05350\npolygon 29        94 109715000       0.01270\npolygon 30        95  61239800       0.00709\nenclosing rectangle: [176503.22, 291043.82] x [331434.7, 454520.1] units\n                     (114500 x 123100 units)\nWindow area = 8635910000 square units\nFraction of frame area: 0.613\n\n\nCode\nplot(wpNGA_ppp)\n\n\n\n\n\nThe code chunk below extract functional waterpoints that are located within Nigeria, Ossun.\n\n\nCode\nwpfuncNGA_ppp = wp_func_ppp[NGA_owin]\nsummary(wpfuncNGA_ppp)\n\n\nPlanar point pattern:  9 points\nAverage intensity 1.04216e-09 points per square unit\n\nCoordinates are given to 2 decimal places\ni.e. rounded to the nearest multiple of 0.01 units\n\nWindow: polygonal boundary\n30 separate polygons (no holes)\n            vertices      area relative.area\npolygon 1        204 766084000       0.08870\npolygon 2         81 304399000       0.03520\npolygon 3         97 465688000       0.05390\npolygon 4        124 373051000       0.04320\npolygon 5         60 149473000       0.01730\npolygon 6         84 144820000       0.01680\npolygon 7         50 102243000       0.01180\npolygon 8         72 216002000       0.02500\npolygon 9        112 269897000       0.03130\npolygon 10       125 365142000       0.04230\npolygon 11        83 111191000       0.01290\npolygon 12       126 192557000       0.02230\npolygon 13       219 904397000       0.10500\npolygon 14       174 741131000       0.08580\npolygon 15        81 138742000       0.01610\npolygon 16        65 119452000       0.01380\npolygon 17        90 280205000       0.03240\npolygon 18        69  69814600       0.00808\npolygon 19        69  42727500       0.00495\npolygon 20        49  30458800       0.00353\npolygon 21        62 263505000       0.03050\npolygon 22        93 438930000       0.05080\npolygon 23        87 274127000       0.03170\npolygon 24       105 509979000       0.05910\npolygon 25        98 292058000       0.03380\npolygon 26        64 327765000       0.03800\npolygon 27       133 108945000       0.01260\npolygon 28       122 462169000       0.05350\npolygon 29        94 109715000       0.01270\npolygon 30        95  61239800       0.00709\nenclosing rectangle: [176503.22, 291043.82] x [331434.7, 454520.1] units\n                     (114500 x 123100 units)\nWindow area = 8635910000 square units\nFraction of frame area: 0.613\n\n\nCode\nplot(wpfuncNGA_ppp)\n\n\n\n\n\nThe code chunk below extract non-functional waterpoints that are located within Nigeria, Ossun.\n\n\nCode\nwpnonfuncNGA_ppp = wp_nonfunc_ppp[NGA_owin]\nsummary(wpnonfuncNGA_ppp)\n\n\nPlanar point pattern:  14 points\nAverage intensity 1.621139e-09 points per square unit\n\nCoordinates are given to 2 decimal places\ni.e. rounded to the nearest multiple of 0.01 units\n\nWindow: polygonal boundary\n30 separate polygons (no holes)\n            vertices      area relative.area\npolygon 1        204 766084000       0.08870\npolygon 2         81 304399000       0.03520\npolygon 3         97 465688000       0.05390\npolygon 4        124 373051000       0.04320\npolygon 5         60 149473000       0.01730\npolygon 6         84 144820000       0.01680\npolygon 7         50 102243000       0.01180\npolygon 8         72 216002000       0.02500\npolygon 9        112 269897000       0.03130\npolygon 10       125 365142000       0.04230\npolygon 11        83 111191000       0.01290\npolygon 12       126 192557000       0.02230\npolygon 13       219 904397000       0.10500\npolygon 14       174 741131000       0.08580\npolygon 15        81 138742000       0.01610\npolygon 16        65 119452000       0.01380\npolygon 17        90 280205000       0.03240\npolygon 18        69  69814600       0.00808\npolygon 19        69  42727500       0.00495\npolygon 20        49  30458800       0.00353\npolygon 21        62 263505000       0.03050\npolygon 22        93 438930000       0.05080\npolygon 23        87 274127000       0.03170\npolygon 24       105 509979000       0.05910\npolygon 25        98 292058000       0.03380\npolygon 26        64 327765000       0.03800\npolygon 27       133 108945000       0.01260\npolygon 28       122 462169000       0.05350\npolygon 29        94 109715000       0.01270\npolygon 30        95  61239800       0.00709\nenclosing rectangle: [176503.22, 291043.82] x [331434.7, 454520.1] units\n                     (114500 x 123100 units)\nWindow area = 8635910000 square units\nFraction of frame area: 0.613\n\n\nCode\nplot(wpnonfuncNGA_ppp)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#kernel-density-estimation",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#kernel-density-estimation",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "6.1 Kernel Density Estimation",
    "text": "6.1 Kernel Density Estimation\nIn this section, I will be computing the kernel density estimation (KDE) of waterpoints in Singapore.\n\n6.1.1 Automatic bandwidth selection methods\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nCode\nkde_wpNGA_bw <- density(wpNGA_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\")\n\n\nplot(kde_wpNGA_bw, main=\"KDE of Waterpoints in Nigeria, Ossun using bw.diggle (m^2)\")\n\n\n\n\n\nAs we can observe from the above, the density values of the output range from 0 to 0.00000004 which is way too small to comprehend. This is because the default unit of measurement of WGS 84 is in meter. As a result, the density values computed is in “number of points per square meter”. Thus, for a better visualisation, we need to rescale the KDE values.\n\n\n6.1.2 Rescalling KDE values\nIn the code chunk below, rescale() is used to covert the unit of measurement from meter to kilometer.\n\n\nCode\nwpNGA_ppp.km <- rescale(wpNGA_ppp, 1000, \"km\")\n\n\nNow, we can re-run density() using the resale data set and plot the output kde map.\n\n\nCode\nkde_wpNGA.bw <- density(wpNGA_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_wpNGA.bw, \n     main=\"KDE of Waterpoints in Nigeria, Ossun using bw.diggle (km^2)\")\n\n\n\n\n\nAccording to Baddeley et. (2016), they suggested the use of the bw.ppl() algorithm because in ther experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\nHence, I decided to look at the output for both.\n\n\nCode\nkde_wpNGA.ppl <- density(wpNGA_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_wpNGA.bw, main = \"KDE of All Waterpoints, bw.diggle\")\nplot(kde_wpNGA.ppl, main = \"KDE of All Waterpoints, bw.ppl\")\n\n\n\n\n\n\n\n6.1.3 KDE for Functional and Non-functional Waterpoints in Nigeria, Osun\nNext, I performed the same actions as above the code chunk to derive the KDE for functional and non functional waterpoints respectively.\n\n6.1.3.1 KDE for Functional Waterpoints in Nigeria, Osun\n\n\nCode\n#rescale\nwpfuncNGA_ppp.km <- rescale(wpfuncNGA_ppp, 1000, \"km\")\n\n#kde with bandwith - diggle\nkde_wpfuncNGA.bw <- density(wpfuncNGA_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\n\n#kde with bandwith - ppl\nkde_wpfuncNGA.ppl <- density(wpfuncNGA_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\n\n#plot\nplot(kde_wpfuncNGA.bw, main=\"KDE of Functional Waterpoints (km^2), bw.diggle\")\n\n\n\n\n\nCode\nplot(kde_wpfuncNGA.ppl, main=\"KDE of Functional Waterpoints (km^2), bw.ppl\")\n\n\n\n\n\n\n\n6.1.3.2 KDE for Non-functional Waterpoints in Nigeria, Osun\n\n\nCode\n#rescale\nwpnonfuncNGA_ppp.km <- rescale(wpnonfuncNGA_ppp, 1000, \"km\")\n\n#kde with bandwith - diggle\nkde_wpnonfuncNGA.bw <- density(wpnonfuncNGA_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\n\n#kde with bandwith - ppl\nkde_wpnonfuncNGA.ppl <- density(wpnonfuncNGA_ppp.km, sigma=bw.ppl, edge=TRUE, kernel=\"gaussian\")\n\n#plot\nplot(kde_wpnonfuncNGA.bw, main=\"KDE of Non-functional Waterpoints (km^2), bw.diggle\")\n\n\n\n\n\nCode\nplot(kde_wpnonfuncNGA.ppl, main=\"KDE of Non-functional Waterpoints (km^2), bw.ppl\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-kde-output-into-grid-object",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#converting-kde-output-into-grid-object",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "6.2 Converting KDE output into grid object",
    "text": "6.2 Converting KDE output into grid object\nNext, for mapping purposes, I need to convert the KDE output into a grid object.\n\n\nCode\ngridded_kde_wpNGA_bw <- as.SpatialGridDataFrame.im(kde_wpNGA.bw)\nspplot(gridded_kde_wpNGA_bw, main = \"Gridded KDE of All Waterpoints, bw.diggle\")\n\n\n\n\n\n\n6.2.1 Converting gridded output into raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\n\nCode\nkde_wpNGA_bw_raster <- raster(gridded_kde_wpNGA_bw)\nkde_wpNGA_bw_raster\n\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.8948485, 0.9616045  (x, y)\nextent     : 176.5032, 291.0438, 331.4347, 454.5201  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : -3.469473e-18, 0.0596219  (min, max)\n\n\nBased on the output, the crs property is NA. Hence, we need to assign it.\n\n\n6.2.2 Assigning projection systems\n\n\nCode\nprojection(kde_wpNGA_bw_raster) <- CRS(\"+init=EPSG:26392\")\nkde_wpNGA_bw_raster\n\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.8948485, 0.9616045  (x, y)\nextent     : 176.5032, 291.0438, 331.4347, 454.5201  (xmin, xmax, ymin, ymax)\ncrs        : +init=EPSG:26392 \nsource     : memory\nnames      : v \nvalues     : -3.469473e-18, 0.0596219  (min, max)\n\n\nBased on the output above, the CRS information has been sucessfully added in.\n\n\n6.2.3 Visualising the output in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\n6.2.3.1 Raster of KDE All Waterpoints in Nigeria, Osun\n\n\nCode\ntm_shape(kde_wpNGA_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(main.title=\"Raster of KDE Waterpoints in Nigeria, Osun\", \n            main.title.size=1,\n            legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\nBased on the KDE graphs, most of the waterpoints are clustered around the top middle section. Upon further research on the cities in Nigeria, State of Osun, I discovered that most of waterpoints are clustered around the city called Osogbo where the Osun river resides.\n\n\n6.2.3.2 Raster of KDE Functional Waterpoints in Nigeria, Osun\nTo display raster of KDE of functional waterpoints in Nigeria, Osun, simply repeat the steps in section 6.\n\n\nCode\n# Repeat the same steps in section 6.2\n\n#convert to grid\ngridded_kde_wpfuncNGA_bw <- as.SpatialGridDataFrame.im(kde_wpfuncNGA.bw)\nspplot(gridded_kde_wpfuncNGA_bw, main = \"Gridded KDE of Functional Waterpoints, bw.diggle\")\n\n\n\n\n\nCode\n#create raster\nkde_wpfuncNGA_bw_raster <- raster(gridded_kde_wpfuncNGA_bw)\n\n#assign CRS info\nprojection(kde_wpfuncNGA_bw_raster) <- CRS(\"+init=EPSG:26392\")\n\n\n\n\nCode\ntm_shape(kde_wpfuncNGA_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(main.title=\"Raster of KDE Functional Waterpoints in Nigeria, Osun\", \n            main.title.size=0.8,\n            legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\nLooking at the KDE graph for functional waterpoints in Nigeria, Osun using bw.diggle, we can observe that the functional waterpoints are mainly clustered in 6 areas. Roughly gauging based on the cities in Nigeria, Ossun, the most densely populated area of functional waterpoints area is found in the city of Osogbo, followed by Ikirun/Ota, Isero/Ikonifin and Okeigbo. The less densely populated area of functional waterpoints but we are still able to see a cluster are around the city of Ife and Ikire.\n\n\n6.2.3.3 Raster of KDE Non-functional Waterpoints in Nigeria, Osun\n\n\nCode\n#convert to grid\ngridded_kde_wpnonfuncNGA_bw <- as.SpatialGridDataFrame.im(kde_wpnonfuncNGA.bw)\nspplot(gridded_kde_wpnonfuncNGA_bw, main = \"Gridded KDE of Non-functional Waterpoints, bw.diggle\")\n\n\n\n\n\nCode\n#create raster\nkde_wpnonfuncNGA_bw_raster <- raster(gridded_kde_wpnonfuncNGA_bw)\n\n#assign CRS info\nprojection(kde_wpnonfuncNGA_bw_raster) <- CRS(\"+init=EPSG:26392\")\n\n\n\n\nCode\ntm_shape(kde_wpnonfuncNGA_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(main.title=\"Raster of KDE Non-functional Waterpoints in Nigeria, Osun\",\n            main.title.size=0.7,\n            legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\nLooking at the KDE graph for non-functional waterpoints in Nigeria, Osun using bw.diggle, we can observe that the non-functional waterpoints are roughly clustered in 9 areas. As compared to the functional waterpoints clusters, there are a larger number of non-functional waterpoints clusters but each cluster are much smaller. It also seems that the most densely populated non-functional waterpoints area is quite near, on the right of, the city of Osogbo, where most of the functional waterpoints are.\n\n\n6.2.3.4 Raster of KDE All Waterpoints in Nigeria, Osun (Openstreetmap)\n\n\nCode\ntmap_mode('view')\ntm_basemap(server = \"OpenStreetMap\") +\ntm_shape(kde_wpNGA_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(main.title=\"Raster of KDE Waterpoints in Nigeria, Osun\", \n            main.title.size=1,\n            legend.position = c(\"right\", \"bottom\"), frame = FALSE) +\n  tm_view(set.zoom.limits= c(18,29)) \n\n\n\n\n\n\n\n\nCode\ntmap_mode('plot')\n\n\n\n\n\n6.2.4 Advantage of Kernel Density map over Point map\nKDE map takes into account the location of features relative to each other while for a point map, it shows the quantity specified by the population field that falls within the identified neighborhood and divide that quantity by the area of the neighborhood.\nThe disadvantages of a point map includes:\n\nOvercrowding of points, when the scale is small, which makes it harder for the user to analyse the map\nSubjected to distortion of shape, distance, direction, scale, and area\n\nThus, using KDE map would be a more accurate representation than a point map as it tackles the disadvantages of a point map."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#functional-waterpoints-in-nigeria-ossun",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#functional-waterpoints-in-nigeria-ossun",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "7.1 Functional Waterpoints in Nigeria, Ossun",
    "text": "7.1 Functional Waterpoints in Nigeria, Ossun\n\n7.1.1 Hypothesis Test\n\nNull hypothesis, H0: The distribution of functional waterpoints in Nigeria, Osun is randomly distributed.\nAlternative hypothesis, H1: The distribution of functional waterpoints in Nigeria, Osun is not randomly distributed.\nThe hypothesis will be tested at a significance level of 0.05, with a corresponding confidence level of 95%.\n\n\n\n7.1.2 Computing L Function estimation\n\n\nCode\nL_func = Lest(wpfuncNGA_ppp, correction = \"Ripley\")\nplot(L_func, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n7.1.3 Performing Complete Spatial Randomness Test\nNext, we perform the function envelope() to compute simulation envelopes of the summary function i.e L function. The following arguments are used:\n\nnsim : Number of simulated point patterns to be generated when computing the envelopes.\nAs we have chosen the significance level to be 0.05, following this formula from the documentation, significance level alpha = 2 * nrank / (1 + nsim), nsim would be 39.\nrank: Integer. Rank of the envelope value amongst the nsim simulated values. A rank of 1 means that the minimum and maximum simulated values will be used.\nLogical flag indicating whether envelopes should be pointwise (global=FALSE) or simultaneous (global=TRUE).\n\n\n\nCode\n#takes around less than 2 minutes\nL_func.csr <- envelope(wpfuncNGA_ppp, Lest, nsim = 39, nrank = 1, global=TRUE)\n\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,  39.\n\nDone.\n\n\n\n\nCode\nplot(L_func.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\nBased on the graph, we can see that the L function is within the randomisation (greyed area). Hence, we cannot reject the null hypothesis as there is insufficient evidence that the distribution of functional waterpoints in Nigeria, Osun is randomly distributed at the level of 0.05."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#non-functional-waterpoints-in-nigeria-ossun",
    "href": "Take-home_Assgn/Take-home_Assgn1/Take-home_Assgn1.html#non-functional-waterpoints-in-nigeria-ossun",
    "title": "Take-home Assignment 1: Application of Spatial Point Patterns Analysis",
    "section": "7.2 Non-functional Waterpoints in Nigeria, Ossun",
    "text": "7.2 Non-functional Waterpoints in Nigeria, Ossun\n\n7.2.1 Hypothesis Test\n\nNull hypothesis, H0: The distribution of non-functional waterpoints in Nigeria, Osun is randomly distributed.\nAlternative hypothesis, H1: The distribution of non-functional waterpoints in Nigeria, Osun is not randomly distributed.\nThe hypothesis will be tested at a significance level of 0.05, with a corresponding confidence level of 95%.\n\n\n\n7.2.2 Computing L Function estimation\n\n\nCode\nL_nonfunc = Lest(wpnonfuncNGA_ppp, correction = \"Ripley\")\nplot(L_nonfunc, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n7.1.3 Performing Complete Spatial Randomness Test\n\n\nCode\n#takes around less than 2 minutes\nL_nonfunc.csr <- envelope(wpnonfuncNGA_ppp, Lest, nsim = 39, nrank = 1, global=TRUE)\n\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,  39.\n\nDone.\n\n\n\n\nCode\nplot(L_nonfunc.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\nBased on the graph, we can see that the L function is within the randomisation. Hence, we cannot reject the null hypothesis as there is insufficient evidence to prove that the distribution of non-functional waterpoints in Nigeria, Osun is randomly distributed at the level of 0.05."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html",
    "href": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html",
    "title": "Take-home Assignment 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "",
    "text": "Hello! This is Rhonda Ho’s take-home Assignment 2 for IS415 module.\nTo view/hide all the code at once, please click on the “</> code” tab beside the title of this html document and select the option to view/hide the code.\nThe full details of this assignment can be found here.\n\n\nExploratory Spatial Data Analysis (ESDA) hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatio-temporal trends of COVID-19 vaccination in DKI Jakarta.\n\n\n\n\nAspatial data\n\nFor the purpose of this assignment, data from Riwayat File Vaksinasi DKI Jakarta will be used. Daily vaccination data are provided. We are only required to download either the first day of the month or last day of the month of the study period.\n\nGeospatial data\n\nFor the purpose of this study, DKI Jakarta administration boundary 2019 will be used. The data set can be downloaded at Indonesia Geospatial portal, specifically at this page.\n\n\n\n\n\n\nNote\n\n\n\n\nThe national Projected Coordinates Systems of Indonesia is DGN95 / Indonesia TM-3 zone 54.1.\nExclude all the outer islands from the DKI Jakarta sf data frame, and\nRetain the first nine fields in the DKI Jakarta sf data frame. The ninth field JUMLAH_PEN = Total Population.\n\n\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nChoropleth Mapping and Analysis\n\nCompute the monthly vaccination rate from July 2021 to June 2022 at sub-district (also known as kelurahan in Bahasa Indonesia) level,\nPrepare the monthly vaccination rate maps by using appropriate tmap functions,\nDescribe the spatial patterns revealed by the choropleth maps (not more than 200 words).\n\nLocal Gi* Analysis\nWith reference to the vaccination rate maps prepared in ESDA:\n\nCompute local Gi* values of the monthly vaccination rate,\nDisplay the Gi* maps of the monthly vaccination rate. The maps should only display the significant (i.e. p-value < 0.05)\nWith reference to the analysis results, draw statistical conclusions (not more than 250 words).\n\nEmerging Hot Spot Analysis(EHSA)\nWith reference to the local Gi* values of the vaccination rate maps prepared in the previous section:\n\nPerform Mann-Kendall Test by using the spatio-temporal local Gi* values,\nSelect three sub-districts and describe the temporal trends revealed (not more than 250 words), and\nPrepared a EHSA map of the Gi* values of vaccination rate. The maps should only display the significant (i.e. p-value < 0.05).\n\nWith reference to the EHSA map prepared, describe the spatial patterns revealed. (not more than 250 words)."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#geospatial-data",
    "href": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#geospatial-data",
    "title": "Take-home Assignment 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "3.1 Geospatial Data",
    "text": "3.1 Geospatial Data\n\n3.1.1 Import shapefile into r environment\n\n\nCode\njkt2019 <- st_read(dsn = \"data/geospatial\", \n                 layer = \"BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA\") \n\n\nReading layer `BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA' from data source \n  `C:\\RhondaHO\\IS415-GAA\\Take-home_Assgn\\Take-home_Assgn2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 269 features and 161 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 106.3831 ymin: -6.370815 xmax: 106.9728 ymax: -5.184322\nGeodetic CRS:  WGS 84\n\n\nFrom the output, we can observe that:\n\nGeometry type is MULTIPOLYGON\nCRS information is WGS 84 instead of the CRS for Indonesia, Jakarta.\n\n\n\n3.1.2 Identifying Relevant Columns\n\n\nCode\nglimpse(jkt2019)\n\n\nRows: 269\nColumns: 162\n$ OBJECT_ID  <dbl> 25477, 25478, 25397, 25400, 25378, 25379, 25390, 25382, 253…\n$ KODE_DESA  <chr> \"3173031006\", \"3173031007\", \"3171031003\", \"3171031006\", \"31…\n$ DESA       <chr> \"KEAGUNGAN\", \"GLODOK\", \"HARAPAN MULIA\", \"CEMPAKA BARU\", \"PU…\n$ KODE       <dbl> 317303, 317303, 317103, 317103, 310101, 310101, 317102, 310…\n$ PROVINSI   <chr> \"DKI JAKARTA\", \"DKI JAKARTA\", \"DKI JAKARTA\", \"DKI JAKARTA\",…\n$ KAB_KOTA   <chr> \"JAKARTA BARAT\", \"JAKARTA BARAT\", \"JAKARTA PUSAT\", \"JAKARTA…\n$ KECAMATAN  <chr> \"TAMAN SARI\", \"TAMAN SARI\", \"KEMAYORAN\", \"KEMAYORAN\", \"KEPU…\n$ DESA_KELUR <chr> \"KEAGUNGAN\", \"GLODOK\", \"HARAPAN MULIA\", \"CEMPAKA BARU\", \"PU…\n$ JUMLAH_PEN <dbl> 21609, 9069, 29085, 41913, 6947, 7059, 15793, 5891, 33383, …\n$ JUMLAH_KK  <dbl> 7255, 3273, 9217, 13766, 2026, 2056, 5599, 1658, 11276, 128…\n$ LUAS_WILAY <dbl> 0.36, 0.37, 0.53, 0.97, 0.93, 0.95, 1.76, 1.14, 0.47, 1.31,…\n$ KEPADATAN  <dbl> 60504, 24527, 54465, 42993, 7497, 7401, 8971, 5156, 71628, …\n$ PERPINDAHA <dbl> 102, 25, 131, 170, 17, 26, 58, 13, 113, 178, 13, 87, 56, 12…\n$ JUMLAH_MEN <dbl> 68, 52, 104, 151, 14, 32, 36, 10, 60, 92, 5, 83, 21, 70, 93…\n$ PERUBAHAN  <dbl> 20464, 8724, 27497, 38323, 6853, 6993, 15006, 5807, 31014, …\n$ WAJIB_KTP  <dbl> 16027, 7375, 20926, 30264, 4775, 4812, 12559, 3989, 24784, …\n$ SILAM      <dbl> 15735, 1842, 26328, 36813, 6941, 7057, 7401, 5891, 23057, 2…\n$ KRISTEN    <dbl> 2042, 2041, 1710, 3392, 6, 0, 3696, 0, 4058, 5130, 1, 3061,…\n$ KHATOLIK   <dbl> 927, 1460, 531, 1082, 0, 0, 1602, 0, 2100, 2575, 0, 1838, 7…\n$ HINDU      <dbl> 15, 9, 42, 127, 0, 0, 622, 0, 25, 27, 0, 9, 115, 47, 382, 7…\n$ BUDHA      <dbl> 2888, 3716, 469, 495, 0, 2, 2462, 0, 4134, 4740, 5, 1559, 3…\n$ KONGHUCU   <dbl> 2, 1, 5, 1, 0, 0, 10, 0, 9, 10, 0, 4, 1, 1, 4, 0, 2, 0, 0, …\n$ KEPERCAYAA <dbl> 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 22, 0, 3, 3, 0, 0, 0…\n$ PRIA       <dbl> 11049, 4404, 14696, 21063, 3547, 3551, 7833, 2954, 16887, 1…\n$ WANITA     <dbl> 10560, 4665, 14389, 20850, 3400, 3508, 7960, 2937, 16496, 1…\n$ BELUM_KAWI <dbl> 10193, 4240, 14022, 20336, 3366, 3334, 7578, 2836, 15860, 1…\n$ KAWIN      <dbl> 10652, 4364, 13450, 19487, 3224, 3404, 7321, 2791, 15945, 1…\n$ CERAI_HIDU <dbl> 255, 136, 430, 523, 101, 80, 217, 44, 381, 476, 39, 305, 10…\n$ CERAI_MATI <dbl> 509, 329, 1183, 1567, 256, 241, 677, 220, 1197, 993, 79, 90…\n$ U0         <dbl> 1572, 438, 2232, 3092, 640, 648, 802, 585, 2220, 2399, 376,…\n$ U5         <dbl> 1751, 545, 2515, 3657, 645, 684, 995, 588, 2687, 2953, 331,…\n$ U10        <dbl> 1703, 524, 2461, 3501, 620, 630, 1016, 513, 2653, 2754, 309…\n$ U15        <dbl> 1493, 521, 2318, 3486, 669, 671, 1106, 548, 2549, 2666, 328…\n$ U20        <dbl> 1542, 543, 2113, 3098, 619, 609, 1081, 491, 2313, 2515, 290…\n$ U25        <dbl> 1665, 628, 2170, 3024, 639, 582, 1002, 523, 2446, 2725, 325…\n$ U30        <dbl> 1819, 691, 2363, 3188, 564, 592, 1236, 478, 2735, 3122, 329…\n$ U35        <dbl> 1932, 782, 2595, 3662, 590, 572, 1422, 504, 3034, 3385, 317…\n$ U40        <dbl> 1828, 675, 2371, 3507, 480, 486, 1200, 397, 2689, 3037, 250…\n$ U45        <dbl> 1600, 607, 2250, 3391, 421, 457, 1163, 365, 2470, 2597, 206…\n$ U50        <dbl> 1408, 619, 1779, 2696, 346, 369, 1099, 288, 2129, 2282, 134…\n$ U55        <dbl> 1146, 602, 1379, 1909, 252, 318, 979, 235, 1843, 1930, 129,…\n$ U60        <dbl> 836, 614, 1054, 1397, 197, 211, 880, 162, 1386, 1394, 75, 9…\n$ U65        <dbl> 587, 555, 654, 970, 122, 114, 747, 111, 958, 932, 50, 706, …\n$ U70        <dbl> 312, 311, 411, 631, 69, 55, 488, 65, 554, 573, 38, 412, 129…\n$ U75        <dbl> 415, 414, 420, 704, 74, 61, 577, 38, 717, 642, 37, 528, 125…\n$ TIDAK_BELU <dbl> 3426, 1200, 4935, 7328, 1306, 1318, 2121, 973, 5075, 6089, …\n$ BELUM_TAMA <dbl> 1964, 481, 2610, 3763, 730, 676, 1278, 732, 3241, 3184, 383…\n$ TAMAT_SD   <dbl> 2265, 655, 2346, 2950, 1518, 2054, 1169, 1266, 4424, 3620, …\n$ SLTP       <dbl> 3660, 1414, 3167, 5138, 906, 1357, 2236, 852, 5858, 6159, 5…\n$ SLTA       <dbl> 8463, 3734, 12172, 16320, 2040, 1380, 5993, 1570, 12448, 14…\n$ DIPLOMA_I  <dbl> 81, 23, 84, 179, 22, 15, 43, 36, 85, 83, 4, 63, 27, 79, 110…\n$ DIPLOMA_II <dbl> 428, 273, 1121, 1718, 101, 59, 573, 97, 604, 740, 25, 734, …\n$ DIPLOMA_IV <dbl> 1244, 1241, 2477, 4181, 314, 191, 2199, 357, 1582, 1850, 83…\n$ STRATA_II  <dbl> 74, 46, 166, 315, 10, 8, 168, 8, 63, 92, 5, 174, 125, 122, …\n$ STRATA_III <dbl> 4, 2, 7, 21, 0, 1, 13, 0, 3, 9, 0, 16, 8, 7, 75, 49, 65, 14…\n$ BELUM_TIDA <dbl> 3927, 1388, 5335, 8105, 1788, 1627, 2676, 1129, 5985, 6820,…\n$ APARATUR_P <dbl> 81, 10, 513, 931, 246, 75, 156, 160, 132, 79, 23, 145, 369,…\n$ TENAGA_PEN <dbl> 70, 43, 288, 402, 130, 93, 81, 123, 123, 73, 45, 109, 30, 1…\n$ WIRASWASTA <dbl> 8974, 3832, 10662, 14925, 788, 728, 6145, 819, 12968, 14714…\n$ PERTANIAN  <dbl> 1, 0, 1, 3, 2, 2, 1, 3, 2, 5, 1, 1, 0, 0, 2, 5, 2, 1, 13, 4…\n$ NELAYAN    <dbl> 0, 0, 2, 0, 960, 1126, 1, 761, 1, 2, 673, 0, 0, 0, 0, 0, 2,…\n$ AGAMA_DAN  <dbl> 6, 6, 5, 40, 0, 0, 49, 2, 10, 11, 0, 54, 15, 16, 21, 14, 17…\n$ PELAJAR_MA <dbl> 4018, 1701, 6214, 9068, 1342, 1576, 3135, 1501, 6823, 6866,…\n$ TENAGA_KES <dbl> 28, 29, 80, 142, 34, 26, 60, 11, 48, 55, 16, 68, 89, 93, 28…\n$ PENSIUNAN  <dbl> 57, 50, 276, 498, 20, 7, 59, 14, 56, 75, 2, 97, 53, 146, 57…\n$ LAINNYA    <dbl> 4447, 2010, 5709, 7799, 1637, 1799, 3430, 1368, 7235, 7206,…\n$ GENERATED  <chr> \"30 Juni 2019\", \"30 Juni 2019\", \"30 Juni 2019\", \"30 Juni 20…\n$ KODE_DES_1 <chr> \"3173031006\", \"3173031007\", \"3171031003\", \"3171031006\", \"31…\n$ BELUM_     <dbl> 3099, 1032, 4830, 7355, 1663, 1704, 2390, 1213, 5330, 5605,…\n$ MENGUR_    <dbl> 4447, 2026, 5692, 7692, 1576, 1731, 3500, 1323, 7306, 7042,…\n$ PELAJAR_   <dbl> 3254, 1506, 6429, 8957, 1476, 1469, 3185, 1223, 6993, 6858,…\n$ PENSIUNA_1 <dbl> 80, 65, 322, 603, 24, 8, 70, 20, 75, 97, 2, 132, 67, 165, 6…\n$ PEGAWAI_   <dbl> 48, 5, 366, 612, 223, 72, 65, 143, 73, 48, 15, 89, 91, 174,…\n$ TENTARA    <dbl> 4, 0, 41, 57, 3, 0, 74, 1, 20, 12, 2, 11, 90, 340, 41, 52, …\n$ KEPOLISIAN <dbl> 10, 1, 16, 42, 11, 8, 2, 9, 17, 7, 3, 9, 165, 15, 17, 28, 1…\n$ PERDAG_    <dbl> 31, 5, 1, 3, 6, 1, 2, 4, 3, 1, 4, 0, 1, 2, 9, 2, 8, 2, 5, 9…\n$ PETANI     <dbl> 0, 0, 1, 2, 0, 1, 1, 0, 1, 1, 1, 2, 0, 0, 1, 2, 0, 1, 6, 1,…\n$ PETERN_    <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ NELAYAN_1  <dbl> 1, 0, 1, 0, 914, 1071, 0, 794, 0, 1, 663, 0, 0, 0, 0, 0, 2,…\n$ INDUSTR_   <dbl> 7, 3, 4, 3, 1, 3, 0, 0, 1, 7, 0, 0, 2, 2, 1, 3, 12, 1, 8, 4…\n$ KONSTR_    <dbl> 3, 0, 2, 6, 3, 8, 1, 6, 1, 5, 10, 0, 2, 5, 7, 4, 7, 1, 6, 2…\n$ TRANSP_    <dbl> 2, 0, 7, 4, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 6, 3, 2, 1, 2, 5,…\n$ KARYAW_    <dbl> 6735, 3034, 7347, 10185, 237, 264, 4319, 184, 9405, 10844, …\n$ KARYAW1    <dbl> 9, 2, 74, 231, 4, 0, 16, 1, 13, 10, 1, 24, 17, 29, 187, 246…\n$ KARYAW1_1  <dbl> 0, 0, 5, 15, 0, 0, 0, 1, 0, 1, 0, 0, 2, 4, 7, 9, 3, 1, 6, 5…\n$ KARYAW1_12 <dbl> 23, 4, 25, 35, 141, 50, 16, 157, 6, 9, 40, 11, 11, 15, 22, …\n$ BURUH      <dbl> 515, 155, 971, 636, 63, 218, 265, 55, 1085, 652, 17, 357, 2…\n$ BURUH_     <dbl> 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 2, 1, 0, 1, 1,…\n$ BURUH1     <dbl> 0, 0, 1, 0, 1, 25, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ BURUH1_1   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ PEMBANT_   <dbl> 1, 1, 4, 1, 1, 0, 7, 0, 5, 1, 0, 6, 1, 10, 11, 9, 8, 3, 4, …\n$ TUKANG     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ TUKANG_1   <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TUKANG_12  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TUKANG__13 <dbl> 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,…\n$ TUKANG__14 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ TUKANG__15 <dbl> 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,…\n$ TUKANG__16 <dbl> 7, 4, 10, 14, 0, 0, 2, 0, 7, 8, 0, 8, 1, 0, 3, 2, 2, 0, 17,…\n$ TUKANG__17 <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PENATA     <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ PENATA_    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PENATA1_1  <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ MEKANIK    <dbl> 11, 1, 10, 8, 0, 0, 4, 0, 7, 8, 0, 9, 0, 15, 10, 10, 3, 0, …\n$ SENIMAN_   <dbl> 4, 0, 12, 28, 0, 0, 2, 0, 3, 4, 0, 9, 6, 7, 14, 13, 17, 22,…\n$ TABIB      <dbl> 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ PARAJI_    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PERANCA_   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1, 2, 0, 0,…\n$ PENTER_    <dbl> 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0,…\n$ IMAM_M     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PENDETA    <dbl> 2, 4, 5, 33, 0, 0, 20, 0, 10, 8, 0, 30, 14, 14, 18, 12, 1, …\n$ PASTOR     <dbl> 0, 1, 0, 1, 0, 0, 8, 0, 0, 0, 0, 23, 0, 0, 0, 0, 2, 0, 0, 0…\n$ WARTAWAN   <dbl> 7, 1, 16, 27, 0, 0, 4, 0, 8, 6, 0, 9, 5, 9, 26, 30, 11, 7, …\n$ USTADZ     <dbl> 6, 1, 1, 5, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ JURU_M     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0,…\n$ PROMOT     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ANGGOTA_   <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 1, 2, 1, 0, 0,…\n$ ANGGOTA1   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,…\n$ ANGGOTA1_1 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PRESIDEN   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ WAKIL_PRES <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ ANGGOTA1_2 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ ANGGOTA1_3 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ DUTA_B     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ GUBERNUR   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ WAKIL_GUBE <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BUPATI     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ WAKIL_BUPA <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ WALIKOTA   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ WAKIL_WALI <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ ANGGOTA1_4 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 1,…\n$ ANGGOTA1_5 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ DOSEN      <dbl> 3, 2, 23, 36, 1, 2, 11, 0, 3, 5, 0, 14, 6, 28, 69, 73, 58, …\n$ GURU       <dbl> 72, 40, 272, 378, 118, 72, 69, 116, 126, 71, 36, 97, 23, 10…\n$ PILOT      <dbl> 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 1, 0, 0, 0,…\n$ PENGACARA_ <dbl> 4, 1, 8, 22, 0, 0, 5, 0, 5, 4, 0, 4, 3, 12, 24, 26, 40, 13,…\n$ NOTARIS    <dbl> 0, 0, 3, 5, 0, 0, 4, 0, 0, 0, 0, 5, 0, 5, 10, 3, 7, 1, 0, 4…\n$ ARSITEK    <dbl> 1, 0, 2, 3, 0, 0, 2, 0, 0, 0, 0, 4, 1, 2, 7, 3, 9, 2, 0, 4,…\n$ AKUNTA_    <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 2, 0, 3, 1,…\n$ KONSUL_    <dbl> 1, 0, 2, 11, 0, 0, 4, 0, 0, 0, 0, 6, 2, 3, 10, 8, 12, 2, 1,…\n$ DOKTER     <dbl> 16, 32, 35, 68, 0, 1, 63, 0, 27, 32, 1, 63, 48, 60, 236, 12…\n$ BIDAN      <dbl> 3, 1, 9, 18, 12, 8, 1, 3, 3, 3, 7, 3, 10, 10, 7, 2, 3, 2, 1…\n$ PERAWAT    <dbl> 7, 0, 25, 44, 12, 10, 3, 6, 12, 20, 6, 7, 26, 16, 21, 20, 9…\n$ APOTEK_    <dbl> 0, 0, 2, 3, 1, 0, 0, 0, 1, 2, 0, 1, 2, 3, 3, 3, 3, 1, 0, 1,…\n$ PSIKIATER  <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 2, 1, 0, 0, 0,…\n$ PENYIA_    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,…\n$ PENYIA1    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ PELAUT     <dbl> 0, 0, 6, 16, 1, 1, 0, 14, 2, 4, 1, 2, 4, 2, 10, 13, 2, 0, 3…\n$ PENELITI   <dbl> 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 2, 0,…\n$ SOPIR      <dbl> 65, 3, 94, 123, 0, 1, 61, 0, 76, 79, 0, 63, 44, 101, 71, 59…\n$ PIALAN     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PARANORMAL <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ PEDAGA_    <dbl> 379, 126, 321, 562, 11, 10, 412, 15, 202, 225, 0, 271, 212,…\n$ PERANG_    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ KEPALA_    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ BIARAW_    <dbl> 0, 1, 0, 0, 0, 0, 22, 0, 3, 0, 0, 2, 1, 0, 4, 0, 17, 1, 0, …\n$ WIRASWAST_ <dbl> 1370, 611, 1723, 3099, 131, 119, 1128, 259, 2321, 2677, 79,…\n$ LAINNYA_12 <dbl> 94, 57, 82, 122, 12, 10, 41, 6, 89, 158, 24, 37, 15, 94, 12…\n$ LUAS_DESA  <dbl> 25476, 25477, 25396, 25399, 25377, 25378, 25389, 25381, 253…\n$ KODE_DES_3 <chr> \"3173031006\", \"3173031007\", \"3171031003\", \"3171031006\", \"31…\n$ DESA_KEL_1 <chr> \"KEAGUNGAN\", \"GLODOK\", \"HARAPAN MULIA\", \"CEMPAKA BARU\", \"PU…\n$ KODE_12    <dbl> 317303, 317303, 317103, 317103, 310101, 310101, 317102, 310…\n$ geometry   <MULTIPOLYGON [°]> MULTIPOLYGON (((106.8164 -6..., MULTIPOLYGON (…\n\n\nHaving a glimpse at our data, some of relevant column(s) that we need for our task is:\n\nKAB_KOTA : represents the cities in Jakarta\nKECAMATAN : represents the district in Jakarta\nDESA_KELUR : represents the sub-districts in Jakarta.\n\n\n\n3.1.3 Data Pre-Processing\n\n3.1.3.1 Check for Invalid Geometries\n\n\nCode\n#check for invalid geometries\nlength(which(st_is_valid(jkt2019) == FALSE))\n\n\n[1] 0\n\n\nThere is no invalid geometries!\n\n\n3.1.3.2 Check for Missing Values\n\n\nCode\n#check for missing values\nsum(is.na(jkt2019))\n\n\n[1] 14\n\n\nBased on the output, we have discovered that there are 14 missing values in our dataset which may affect our analysis. Thus, I removed all missing values in the sub-districts(a part of our main study focus), using the code chunk below.\n\n\nCode\njkt2019 <- na.omit(jkt2019,c(\"DESA_KELUR\"))\n\n\n\n\n3.1.3.3 Transformation of Coordinates Information\nEarlier on, we discovered that the assigned coordinates system of our data is WGS 84 which is not appropriate for an Indonesian-specific geospatial dataset. Another way of checking it is using the function, st_crs().\n\n\nCode\n#check whether need to transform\nst_crs(jkt2019)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nAs this is an Indonesian-specific geospatial dataset, I will be using EPSG:23845.\n\n\nCode\n# transforms the CRS to DGN95, ESPG code 23845\njkt2019 <- st_transform(jkt2019, 23845)\n\nst_crs(jkt2019)\n\n\nCoordinate Reference System:\n  User input: EPSG:23845 \n  wkt:\nPROJCRS[\"DGN95 / Indonesia TM-3 zone 54.1\",\n    BASEGEOGCRS[\"DGN95\",\n        DATUM[\"Datum Geodesi Nasional 1995\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4755]],\n    CONVERSION[\"Indonesia TM-3 zone 54.1\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",139.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9999,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",200000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",1500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre.\"],\n        AREA[\"Indonesia - onshore east of 138°E.\"],\n        BBOX[-9.19,138,-1.49,141.01]],\n    ID[\"EPSG\",23845]]\n\n\nWe have successfully managed to transform our dataset to the appropriate coordinates system!\n\n\n3.1.3.4 Removal of Outer Islands\nTo start off, let visualise our data!\n\n\nCode\nplot(st_geometry(jkt2019))\n\n\n\n\n\nThe visualisation above shows that our data consist of outer islands beyond Jakarta. As this is not important for our analysis, we should remove it.\nPreviously, we have identified 3 important attributes which represents the divisions of Jakarta. They are KAB_KOTA (City), KECAMATAN (District) and DESA_KELUR (Village). Removing the outer islands by the City would help us to remove all the districts and villages within it, so I proceeded to check the unique values of it.\n\n\nCode\nunique(jkt2019$\"KAB_KOTA\")\n\n\n[1] \"JAKARTA BARAT\"    \"JAKARTA PUSAT\"    \"KEPULAUAN SERIBU\" \"JAKARTA UTARA\"   \n[5] \"JAKARTA TIMUR\"    \"JAKARTA SELATAN\" \n\n\nFrom this, we can observe that all the values has “JAKARTA” in it except “KEPULAUAN SERIBU”(Thousand Island). The code chunk below visualises the different cities in Jakarta.\n\n\nCode\ntm_shape(jkt2019) + \n  tm_polygons(\"KAB_KOTA\")\n\n\n\n\n\nNext, we will need to remove the outer islands by using the filter() function to filter out the outer islands.\n\n\nCode\n# accepts only if the value of KAB_KOTA is NOT KEPULAUAN SERIBU\njkt2019 <- filter(jkt2019, KAB_KOTA != \"KEPULAUAN SERIBU\")\n\n\nFor this assignment, we are required to only retain the first 9 fields.\n\n\nCode\n# filters out other fields by accepting only the first 9 fields\njkt2019 <- jkt2019[, 0:9]\n\n\nFor easier comprehension of our data, I have decided to translate the column headers to English.\n\n\nCode\njkt2019 <- jkt2019 %>% \n  dplyr::rename(\n    Object_ID=OBJECT_ID,\n    Province=PROVINSI, \n    City=KAB_KOTA, \n    District=KECAMATAN, \n    Village_Code=KODE_DESA, \n    Village=DESA, \n    Sub_District=DESA_KELUR,\n    Code=KODE, \n    Total_Population=JUMLAH_PEN\n    )\n\n\nWith that, we are done for our geospatial data!"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#aspatial-data",
    "href": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#aspatial-data",
    "title": "Take-home Assignment 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "3.2 Aspatial Data",
    "text": "3.2 Aspatial Data\n\n3.2.1 Observations of Data Website\nExamining DKI Jakarta Vaccination File History website, some of my observations were:\n\nEntire page is in Indonesian\n\nTo translate to English, simply right click anywhere on the webpage and click “Translate to English” button\n\nThere were two columns for DKI Jakarta Vaccination File History, one for “Vaccination Data Based on Village and District”, another for “RT-Based Vaccination Data”\n\nWe will be focusing on the “Vaccination Data Based on Village and District”column as our task require us to analyse on a sub-district level.\n\nMany instances of missing data!\n\nE.g 1) The hyperlink to March 1 2022 will direct you to March 2 2022 data and March 1 2022 cannot be found\nE.g 2) Certain months such as July 2021, August 2021, February 2022 were missing the record of the date written in the table and its respective data\n\nA few examples of missing records are 19 July 2021, 28 August 2022, 11 September, 28 February 2022, 17 March 2022\nUpon further research and some of my assumptions, missing data can be due to:\n\nA glitch in the system, which prevents the data from being uploaded\nAn update of the system which may prevent records from being uploaded on that day\n\n\n\n\nUnder our task we are required to compute monthly vaccination rate from July 2021 to June 2022 and we are only required to download the first or last day of the month of study period.\nFor this assignment, I will be using data of the first day of each month of the study period with the exception of March 2021, I will be using March 2 instead of March 1 as March 1’s data is missing.\n\n\n3.2.2 Importing Aspatial Data\nIn our ‘data/aspatial’ folder, we have downloaded multiple .xlsx files ranging from 1 July 2021 to 1 June 2022. But before we compile it together, we need to understand each dataset and check for any discrepancies.\nThe purpose of the code chunk below is to reach each excel file under the folder ‘data/aspatial’ , create its dataframe(df) and assign it to a variable name which is in the format of “month_year”. E.g April_2022 . The following functions are used:\n\nlist.files(): creates a list of the files in a directory/folder\nsubstr(): extract the characters present in the data\nstr_trim(): remove whitespace\npaste(): concatenate strings and values into a single element\nread_excel(): to read the data in xlsx format\nnchar(): gets the length of string\n\n\n\nCode\n#gets all files under the folder 'data/aspatial' which is in '.xlsx' format\nstudy_period_data <- list.files('data/aspatial',pattern = \"*.xlsx\")\n\n#instantiate an empty list which will be used later to contain all the df names\ndf_list <-list()\n\nfor (x in study_period_data){\n # eg of x: Data Vaksinasi Berbasis Kelurahan (01 April 2022).xlsx\n \n # extract the year and month from the data file name\n month_yr <- str_trim(substr(x, 38, nchar(x)-6)) #e.g output: April 2022\n \n #split the month and year by the empty space inbetween\n split_month_yr <- strsplit(month_yr,\" \") #e.g output: [[1]][1]'April' '2022'\n \n #split_month_yr[[1]][1] shows the month, split_month_yr[[1]][2] shows the year \n \n #join the month and year with \"_\"\n join_month_yr <- paste(split_month_yr[[1]][1],split_month_yr[[1]][2],sep=\"_\") \n #e.g output: April_2022\n #join_month_yr will be used as the variable name to store the df\n \n #add each month name to list\n df_list<-append(df_list,join_month_yr)\n \n # get filepath that we need to read each month's excel file\n filepath <- paste(\"data/aspatial/\",x,sep=\"\")\n # eg output: \"data/aspatial/Data Vaksinasi Berbasis Kelurahan (01 April 2022).xlsx\"\n   \n #reach each excel file and assign it to its variable name \n assign(join_month_yr,read_excel(filepath))\n \n \n}\n\n\n\n\n3.2.3 Data Pre-processing\n\n3.2.3.1 Check for Duplicated Values\nTo check for any duplicate columns for each dataset, I created a function called duplication_check() which takes in the df filename as a variable.\n\n\nCode\nduplication_check <- function(file_name) {\n  duplicated_columns <- duplicated(as.list(file_name))\n  duplicated_chara <- colnames(file_name[duplicated_columns])\n  return(duplicated_chara)\n}\n\n\nI then made use of df_list which I populated earlier on to contain all of our df names and looped through it. This allows me to check for each time period df, which df has duplicated columns using the function I created.\n\n\nCode\nfor (df in df_list) {\n  duplicated_col <- duplication_check(get(df))\n  if(identical(duplicated_col, character(0))){\n    duplicated_col=\"none\"\n  }\n  \n  print(paste(\"Under df\", df, \"| duplicated col:\", toString(duplicated_col)))\n}\n\n\n[1] \"Under df April_2022 | duplicated col: LANSIA\\r\\nDOSIS 3\"\n[1] \"Under df Desember_2021 | duplicated col: none\"\n[1] \"Under df Februari_2022 | duplicated col: none\"\n[1] \"Under df Januari_2022 | duplicated col: none\"\n[1] \"Under df Juli_2021 | duplicated col: none\"\n[1] \"Under df Juni_2022 | duplicated col: LANSIA\\r\\nDOSIS 3\"\n[1] \"Under df Mei_2022 | duplicated col: LANSIA\\r\\nDOSIS 3\"\n[1] \"Under df November_2021 | duplicated col: none\"\n[1] \"Under df Oktober_2021 | duplicated col: none\"\n[1] \"Under df September_2021 | duplicated col: none\"\n[1] \"Under df Maret_2022 | duplicated col: none\"\n[1] \"Under df Agustus_2021 | duplicated col: none\"\n\n\nBased on the output, we can observe that for df April_2022, Juni_2022 and Mei_2022. There is a duplicated column called LANSIA\\r\\nDOSIS 3.\nDiving deeper into each dataset with a duplicated column with the use of glimpse(), we can observe that column LANSIA\\r\\nDOSIS 3 and column LANSIA\\r\\nDOSIS 2 are exactly the same.\n\n\nCode\nglimpse(April_2022)\n\n\nRows: 268\nColumns: 34\n$ `KODE KELURAHAN`                             <chr> NA, \"3172051003\", \"317304…\n$ `WILAYAH KOTA`                               <chr> NA, \"JAKARTA UTARA\", \"JAK…\n$ KECAMATAN                                    <chr> NA, \"PADEMANGAN\", \"TAMBOR…\n$ KELURAHAN                                    <chr> \"TOTAL\", \"ANCOL\", \"ANGKE\"…\n$ SASARAN                                      <dbl> 8941211, 23947, 29381, 29…\n$ `BELUM VAKSIN`                               <dbl> 1481006, 4518, 5181, 5774…\n$ `JUMLAH\\r\\nDOSIS 1`                          <dbl> 7460205, 19429, 24200, 23…\n$ `JUMLAH\\r\\nDOSIS 2`                          <dbl> 6684941, 16915, 21004, 20…\n$ `JUMLAH\\r\\nDOSIS 3`                          <dbl> 1886300, 4266, 6227, 4237…\n$ `TOTAL VAKSIN\\r\\nDIBERIKAN`                  <dbl> 16031446, 40610, 51431, 4…\n$ `LANSIA\\r\\nDOSIS 1`                          <dbl> 649702, 1574, 2476, 1457,…\n$ `LANSIA\\r\\nDOSIS 2`                          <dbl> 610834, 1433, 2350, 1366,…\n$ `LANSIA\\r\\nDOSIS 3`                          <dbl> 610834, 1433, 2350, 1366,…\n$ `LANSIA TOTAL \\r\\nVAKSIN DIBERIKAN`          <dbl> 1536442, 3562, 6059, 3290…\n$ `PELAYAN PUBLIK\\r\\nDOSIS 1`                  <dbl> 1481384, 3980, 3910, 4605…\n$ `PELAYAN PUBLIK\\r\\nDOSIS 2`                  <dbl> 1375790, 3634, 3524, 4175…\n$ `PELAYAN PUBLIK\\r\\nDOSIS 3`                  <dbl> 210484, 627, 678, 484, 25…\n$ `PELAYAN PUBLIK TOTAL\\r\\nVAKSIN DIBERIKAN`   <dbl> 3067658, 8241, 8112, 9264…\n$ `GOTONG ROYONG\\r\\nDOSIS 1`                   <dbl> 88150, 178, 178, 262, 102…\n$ `GOTONG ROYONG\\r\\nDOSIS 2`                   <dbl> 86122, 173, 179, 260, 99,…\n$ `GOTONG ROYONG\\r\\nDOSIS 3`                   <dbl> 20351, 24, 55, 60, 20, 42…\n$ `GOTONG ROYONG TOTAL\\r\\nVAKSIN DIBERIKAN`    <dbl> 194623, 375, 412, 582, 22…\n$ `TENAGA KESEHATAN\\r\\nDOSIS 1`                <dbl> 115539, 140, 135, 348, 12…\n$ `TENAGA KESEHATAN\\r\\nDOSIS 2`                <dbl> 112036, 130, 130, 331, 12…\n$ `TENAGA KESEHATAN\\r\\nDOSIS 3`                <dbl> 84980, 106, 96, 244, 83, …\n$ `TENAGA KESEHATAN TOTAL\\r\\nVAKSIN DIBERIKAN` <dbl> 312555, 376, 361, 923, 32…\n$ `TAHAPAN 3\\r\\nDOSIS 1`                       <dbl> 4259873, 11254, 14777, 13…\n$ `TAHAPAN 3\\r\\nDOSIS 2`                       <dbl> 3716743, 9507, 12437, 114…\n$ `TAHAPAN 3\\r\\nDOSIS 3`                       <dbl> 1282989, 2927, 4122, 2958…\n$ `TAHAPAN 3 TOTAL\\r\\nVAKSIN DIBERIKAN`        <dbl> 9259605, 23688, 31336, 28…\n$ `REMAJA\\r\\nDOSIS 1`                          <dbl> 865557, 2303, 2724, 2851,…\n$ `REMAJA\\r\\nDOSIS 2`                          <dbl> 783416, 2038, 2384, 2541,…\n$ `REMAJA\\r\\nDOSIS 3`                          <dbl> 11590, 27, 43, 24, 7, 28,…\n$ `REMAJA TOTAL\\r\\nVAKSIN DIBERIKAN`           <dbl> 1660563, 4368, 5151, 5416…\n\n\nBut how does this affect our dataset?\nUpon translation, i discovered that LANSIA\\r\\nDOSIS refers to elderly dosage.\nThis means that in our datasets with duplicated columns, there is 4 related columns - Elderly Dosage 1, Elderly Dosage 2, Elderly Dosage 3, Total Elderly Dosage where Dosage 3 values are a repeat of Dosage 2 values.\nI suspected that the Total elderly vaccine delivered should be the sum of Elderly Dosage 1, 2 and 3. However, according to April_2022 df, total elderly vaccine delivered is 1536442 which is neither the sum of elderly dosage 1, 2 and 3 nor the sum of elderly dosage 1 and 2. This suggests that either the total sum is wrongly added or the values for elderly dosage 3 was input wrongly. Regardless, this reduces the reliability of those data columns.\n\n\n3.2.3.2 Check for Missing Values\nTo check for missing values, I created a function, check_missing_value() and used it to check for each df.\n\n\nCode\ncheck_missing_value <- function(filename){\n  sum(is.na(filename))\n}\n\nfor (df in df_list) {\n  print(check_missing_value(get(df)))\n}\n\n\n[1] 3\n[1] 3\n[1] 3\n[1] 3\n[1] 3\n[1] 3\n[1] 3\n[1] 3\n[1] 3\n[1] 3\n[1] 3\n[1] 3\n\n\nBased on the output, I realised that all the df were missing 3 fields. Looking deeper, I realised it was the same 3 fields (1st row, first 3 columns) for all the df that were blank. The reason why it is empty is due to the fact that is was merged with the top column in the excel and the 1st row of each excel file was meant to represent the total dosages across all districts. Thus, we will be removing it later.\n\n\n3.2.3.3 Combination of every .xlsx Files\nNow, we need to combine the data files for every time period of our study period.\nFirst, I would need to add the date in a new column called Time_Period .\n\n\nCode\nfor (df_str in df_list) {\n  df<-get(df_str)\n  \n  new_df<- df %>%\n    #extract relevant columns\n    select(c('WILAYAH KOTA', # CITY AREA\n             'KECAMATAN', # DISTRICT\n             'KELURAHAN', # SUBDISTRICT\n             'SASARAN', # TARGET population to be vacinated\n             'BELUM VAKSIN' #Yet to be vacinnated\n             )) %>%\n    #add the month_year\n    add_column(Time_Period = df_str)\n  \n  assign(df_str,new_df)\n}\n\ncombined_df <- setNames(data.frame(matrix(ncol = 6, nrow = 0)), \n                        c('WILAYAH KOTA',\n                          'KECAMATAN',\n                          'KELURAHAN',\n                          'SASARAN',\n                          'BELUM VAKSIN',\n                          'Time_Period'))\n\n#combining all tgt\nfor (df_str in df_list) {\n  combined_df <- rbind(combined_df,get(df_str))\n}\n\n\nNext, I translated the Indonesian column headers into English so we can understand it better.\n\n\nCode\n#translate  colnames into eng\ncolnames(combined_df) <- c(\"CITY_AREA\", \"DISTRICT\",\"SUBDISTRICT\", \"TARGET\", \"YET_TO_BE_VACINATED\", \"TIME_PERIOD\")\n\n\nAfterwards, I wanted to convert our time period into the date format yyyy-mm-dd.\n\n\nCode\n#convert into date format yyyy-mm-dd\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Desember_2021'] <- '2021-12-01'\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Februari_2022'] <- '2022-02-01'\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Januari_2022'] <- '2022-01-01'\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Juli_2021'] <- '2021-07-01'\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Juni_2022'] <- '2022-06-01'\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Mei_2022'] <- '2022-05-01'\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Oktober_2021'] <- '2021-10-01'\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Maret_2022'] <- '2022-03-02'\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'Agustus_2021'] <- '2021-08-01'\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'September_2021'] <- '2021-09-01'\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'November_2021'] <- '2021-11-01'\ncombined_df$TIME_PERIOD[combined_df$TIME_PERIOD == 'April_2022'] <- '2022-04-01'\n\n#convert to date format\ncombined_df$TIME_PERIOD <-as.Date(combined_df$TIME_PERIOD, format=\"%Y-%m-%d\")\nclass(combined_df$TIME_PERIOD)\n\n\n[1] \"Date\"\n\n\n\n\nCode\n#check missing values\ncombined_df[rowSums(is.na(combined_df))!=0,]\n\n\n# A tibble: 12 × 6\n   CITY_AREA DISTRICT SUBDISTRICT  TARGET YET_TO_BE_VACINATED TIME_PERIOD\n   <chr>     <chr>    <chr>         <dbl>               <dbl> <date>     \n 1 <NA>      <NA>     TOTAL       8941211             1481006 2022-04-01 \n 2 <NA>      <NA>     TOTAL       8941211             1718787 2021-12-01 \n 3 <NA>      <NA>     TOTAL       8941211             1536737 2022-02-01 \n 4 <NA>      <NA>     TOTAL       8941211             1620250 2022-01-01 \n 5 <NA>      <NA>     TOTAL       7739060             5041111 2021-07-01 \n 6 <NA>      <NA>     TOTAL       8941211             1444901 2022-06-01 \n 7 <NA>      <NA>     TOTAL       8941211             1455001 2022-05-01 \n 8 <NA>      <NA>     TOTAL       8941211             1875655 2021-11-01 \n 9 <NA>      <NA>     TOTAL       8941211             2221074 2021-10-01 \n10 <NA>      <NA>     TOTAL       8941211             3259430 2021-09-01 \n11 <NA>      <NA>     TOTAL       8941211             1516200 2022-03-02 \n12 <NA>      <NA>     TOTAL       8941211             4399496 2021-08-01 \n\n\nAs identified earlier, there are missing values under CITY AREA and DISTRICT where the SUBDISTRICT is “TOTAL”. As our analysis focuses on the Subdistricts of Jakarta, knowing the “Total” is not important. Hence, I decided to remove it.\n\n\nCode\ncombined_df <- na.omit(combined_df)\n\n\n\n\nCode\nsum(is.na(combined_df))\n\n\n[1] 0\n\n\n\n\n3.2.3.4 Removal of Outer Islands\nNext, we need to check whether this data consist of any outer islands and remove it accordingly. The code chunk below follow the steps earlier shown in the removal of outer islands.\n\n\nCode\n#identify unique values of city areas in Jakarta\nunique(combined_df$\"CITY_AREA\")\n\n\n[1] \"JAKARTA UTARA\"      \"JAKARTA BARAT\"      \"JAKARTA TIMUR\"     \n[4] \"JAKARTA SELATAN\"    \"JAKARTA PUSAT\"      \"KAB.ADM.KEP.SERIBU\"\n\n\n\n\nCode\n#remove the outer island i.e KAB.ADM.KEP.SERIBU\ncombined_df <- filter(combined_df, CITY_AREA != \"KAB.ADM.KEP.SERIBU\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#calculation-of-monthly-vaccination-rates",
    "href": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#calculation-of-monthly-vaccination-rates",
    "title": "Take-home Assignment 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "5.1 Calculation of Monthly Vaccination Rates",
    "text": "5.1 Calculation of Monthly Vaccination Rates\nTo calculate the monthly vaccination rates, I will be using this formula:\n\\(Vaccination Rate (\\%) =\\frac{Target - Yet To Be Vaccinated}{Target} * 100\\)\nwhere Target refers to target population to get vaccinated and YetToBeVacinated refers to population who have not been vaccinated yet. Essentially, the vaccination rate is derived from taking count of people who are vaccinated over count of people who are supposed to be vaccinated.\n\n\nCode\n# grouping based on the sub-district and time_period\n\nvr <- combined_df %>%\n  inner_join(jkt2019, by=c(\"SUBDISTRICT\" = \"Sub_District\")) %>%\n  group_by(SUBDISTRICT, TIME_PERIOD) %>%\n  dplyr::summarise(`vaccination_rates` = ((sum(TARGET-YET_TO_BE_VACINATED)/sum(TARGET))*100)) %>%\n  \n  #afterwards, pivots the table based on the Dates, showing the cumulative death rate\n  ungroup() %>% pivot_wider(names_from = TIME_PERIOD,\n              values_from = vaccination_rates)\nvr\n\n\n# A tibble: 261 × 13\n   SUBDISTRICT   2021-…¹ 2021-…² 2021-…³ 2021-…⁴ 2021-…⁵ 2021-…⁶ 2022-…⁷ 2022-…⁸\n   <chr>           <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 ANCOL            34.9    49.2    61.8    72.2    75.1    77.0    78.9    80.6\n 2 ANGKE            36.1    53.3    64.8    74.3    77.7    79.7    80.9    81.7\n 3 BALE KAMBANG     25.1    37.3    57.2    70.2    74.0    76.7    78.3    79.5\n 4 BALI MESTER      33.9    48.9    62.2    74.4    78.3    80.4    81.7    82.8\n 5 BAMBU APUS       32.3    47.8    64.4    77.0    81.0    82.5    83.4    84.5\n 6 BANGKA           34.0    52.6    61.3    73.4    78.1    79.9    80.7    81.5\n 7 BARU             44.4    58.1    67.9    79.6    82.9    84.2    85.0    85.8\n 8 BATU AMPAR       26.7    40.7    58.6    70.7    74.6    77.1    78.8    80.2\n 9 BENDUNGAN HI…    39.5    54.0    62.7    75.8    79.1    80.6    81.4    82.3\n10 BIDARA CINA      27.6    40.9    57.9    71.2    75.3    77.1    78.2    79.2\n# … with 251 more rows, 4 more variables: `2022-03-02` <dbl>,\n#   `2022-04-01` <dbl>, `2022-05-01` <dbl>, `2022-06-01` <dbl>, and abbreviated\n#   variable names ¹​`2021-07-01`, ²​`2021-08-01`, ³​`2021-09-01`, ⁴​`2021-10-01`,\n#   ⁵​`2021-11-01`, ⁶​`2021-12-01`, ⁷​`2022-01-01`, ⁸​`2022-02-01`\n\n\nThe output of the code chunk below shows monthly vaccination rates across each sub-district from a study period of July 2021 to June 2022.\nConvert our dataframes into sf objects\n\n\nCode\ncombined_jakarta <- st_as_sf(combined_jakarta)\n\nvr <- vr%>% left_join(jkt2019, by=c(\"SUBDISTRICT\"=\"Sub_District\"))\nvr <- st_as_sf(vr)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#mapping-of-monthly-vaccination-rates",
    "href": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#mapping-of-monthly-vaccination-rates",
    "title": "Take-home Assignment 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "5.2 Mapping of Monthly Vaccination Rates",
    "text": "5.2 Mapping of Monthly Vaccination Rates\nThere are different methods to classify the vaccination rates in choropleth maps. Initially, I considered Jenks classification method as it can minimize variation in each group which allows map readers to easily witness the trends presented by the map. However, this method is not very suitable for our data as it has low variance.\nThe code chunk below shows the decreasing variance as time passes through the period of July 2021 to June 2022.\n\n\nCode\nvar(vr$`2021-07-01`)\n\n\n[1] 36.34884\n\n\nCode\nvar(vr$`2021-10-01`)\n\n\n[1] 6.146801\n\n\nCode\nvar(vr$`2022-02-01`)\n\n\n[1] 2.971211\n\n\nCode\nvar(vr$`2022-06-01`)\n\n\n[1] 2.652066\n\n\nTo get a clearer visualization of a specific month, I decided to use the classification method, cont as it creates a smooth, linear gradient in which the change in values is proportionally related to the change in colors. For the color palette, i decided to go with a sequential scheme from yellow to green as it more suited for ordered data that progress from low to high.\nThe code chunk below is a map of vaccination rates in July 2021. By clicking on a certain location, it will show us the district name and respective vaccination rates. The darker the green, the higher the vaccination rates.\n\n\nCode\n  tmap_mode(\"view\")\n  tm_shape(vr)+\n    tm_polygons(\"2021-07-01\", \n            n= 6,\n            style = \"cont\",\n            title = \"Vaccination Rate(%)\",\n            palette = \"YlGn\",\n            ) +\n  \n    tm_layout(main.title = \"Vaccination Rates in July 2021\",\n              main.title.position = \"center\",\n              main.title.size = 1,\n              #legend.height = 0.5, \n              #legend.width = 0.4,\n              \n              ) +\n    tm_borders(alpha = 0.5) +\n    tm_view(set.zoom.limits = c(10, 14),\n            view.legend.position = c(\"right\",\"bottom\"))\n\n\n\n\n\n\n\nInsights:\n\nThe sub-district with the lowest vaccination rate is CIPINANG BESAR UTARA (23.15%).\nThe sub-district with the highest vaccination rate is HALIM PERDANA KUSUMAH (55.24%).\nSub-districts with lower vaccination rates can try to learn/adopt strategies from sub districts with higher vaccination rates’ to increase their own vaccination rates.\n\nThe code chunk below is a map of vaccination rates in June 2022.\n\n\nCode\n  tmap_mode(\"view\")\n  tm_shape(vr)+\n    tm_polygons(\"2022-06-01\", \n            n= 6,\n            style = \"cont\",\n            title = \"Vaccination Rate(%)\",\n            palette = \"YlGn\",\n            ) +\n    \n    tm_layout(main.title = \"Vaccination Rates in June 2022\",\n              main.title.position = \"center\",\n              main.title.size = 1,\n              #legend.height = 0.5, \n              #legend.width = 0.4,\n              ) +\n    tm_borders(alpha = 0.5) +\n    tm_view(set.zoom.limits = c(10, 14),\n            view.legend.position = c(\"right\",\"bottom\"))\n\n\n\n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\nInsights (as compared to July 2021):\n\nThe sub-district with the lowest vaccination rate is no longer CIPINANG BESAR UTARA(81.79%) but KEBON MELATI (78.13%). However, the lowest vaccination rate in June 2022 is still much higher than the the highest vaccination rate in July 2021 (55.24%).\nThe sub-district with highest vaccination rate is the same, HALIM PERDANA KUSUMAH (89.76%).\nThe bottom half sub-districts tend to have a higher vaccination rate than the top half sub-districts.\nThe range of vaccination rates have increased from range 25% to 55% to range 80% to 88%,\nThus, we can infer that within a span of 1 year, the vaccination rates across the sub-districts in Jakarta have increased.\n\nThe code chunk below plots the vaccination rate across the our study period of July 2021 to June 2022. The data is classified manually. The breakpoints are 20, 40, 60, 80, 100.\n\n\nCode\nrate_cols <- colnames(st_set_geometry(vr[2:13], NULL))\n\nmap_list <- vector(mode = \"list\", length = length(rate_cols))\n\nfor (i in 1:length(rate_cols)) {\n  cmap <- tm_shape(vr)+\n    tm_polygons(rate_cols[[i]], \n            n= 6,\n            breaks = c(0, 20, 40, 60, 80, 100),\n            #style = \"cont\",\n            title = \"Vaccination Rate(%)\",\n            palette = \"YlGn\",\n            ) +\n    #tm_text(\"SUBDISTRICT\")+\n    tm_layout(\n              panel.show = TRUE,\n              panel.labels = rate_cols[[i]],\n              panel.label.color = 'gray12',\n              panel.label.size = 0.8,\n              legend.show = FALSE\n              )\n  \n  map_list[[i]] <- cmap\n}\n\ntmap_arrange(map_list, ncol = 4)\n\n\n\n\n\nInsights:\n\nWe can actually see Jakarta’s vaccination rate increase steadily across the period of July 2021 to February 2022 as more sub-districts shift from a lighter shade of green to a darker one. This could be due to Indonesia’s efforts in encouraging their citizens to get vaccinated.\nHowever, from February 2022 to June 2022, there is not much difference in the vaccination rates across all sub-districts."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#computing-local-gi-values-of-monthly-vaccination-rates",
    "href": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#computing-local-gi-values-of-monthly-vaccination-rates",
    "title": "Take-home Assignment 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "6.1 Computing local Gi* Values of Monthly Vaccination Rates",
    "text": "6.1 Computing local Gi* Values of Monthly Vaccination Rates\nBefore we compute our local Gi*, I would like to rearange our data for better clarity. The data will be organised in the following format: Date, Sub_District and Vaccination_Rate.\n\n\nCode\n#Extract relevant columns\ncopy_cjkt_df <- combined_jakarta[8:14]\n\n# calculate vaccination rates\ncopy_cjkt_df <- copy_cjkt_df %>% \n  mutate(\n    VACCINATION_RATES = (TARGET - YET_TO_BE_VACINATED)/TARGET *100,\n    )\n\n#drop not impt col\ncopy_cjkt_df = subset(copy_cjkt_df, select = -c(Total_Population,CITY_AREA,DISTRICT,TARGET,YET_TO_BE_VACINATED) )\n\n#for clarity, reorder the columns to data, subdistrict, vaccination rate\ncopy_cjkt_df<- copy_cjkt_df %>% relocate(VACCINATION_RATES, .after=Sub_District)\ncopy_cjkt_df<- copy_cjkt_df %>% relocate(TIME_PERIOD)\n\ncolnames(copy_cjkt_df) <- c(\"Date\", \"Sub_District\",\"Vaccination_Rate\",\"geometry\")\n\n\n\n\nCode\nglimpse(copy_cjkt_df)\n\n\nRows: 3,132\nColumns: 4\n$ Date             <date> 2022-04-01, 2021-12-01, 2022-02-01, 2022-01-01, 2021…\n$ Sub_District     <chr> \"KEAGUNGAN\", \"KEAGUNGAN\", \"KEAGUNGAN\", \"KEAGUNGAN\", \"…\n$ Vaccination_Rate <dbl> 84.62069, 82.22810, 84.01104, 83.22310, 32.67593, 85.…\n$ geometry         <MULTIPOLYGON [m]> MULTIPOLYGON (((-3626874 69..., MULTIPOL…\n\n\nThe code chunk below creates a time series cube using spacetime() of sfdep package:\n\n\nCode\nvr_st<- as_spacetime(copy_cjkt_df,\n                                .loc_col = \"Sub_District\",\n                                .time_col = \"Date\")\n\n\nBy using is_spacetime_cube() of sfdep package, we can check if vr_st is indeed a space-time cube object. If it returns TRUE, vr_st is a space-time cube object.\n\n\nCode\nis_spacetime_cube(vr_st)\n\n\n[1] TRUE\n\n\nTo compute the local Gi* statistics, we need to derive the spatial weights first:\n\n\nCode\nvr_nb <- vr_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale=1,\n                                  alpha=1),\n         .before=1) %>%\n  set_wts(\"wt\") %>%\n  set_nbs(\"nb\")\n\n\nOur df now has neigbours (nb) and weights(wt) for each date.\n\n\nCode\nhead(vr_nb)\n\n\n         Date  Sub_District Vaccination_Rate\n5  2021-07-01     KEAGUNGAN         32.67593\n17 2021-07-01        GLODOK         50.48665\n29 2021-07-01 HARAPAN MULIA         31.84070\n41 2021-07-01  CEMPAKA BARU         32.61251\n53 2021-07-01    PASAR BARU         47.94955\n65 2021-07-01  KARANG ANYAR         34.76752\n                                                                                                                             wt\n5                                                  0.000000000, 0.001071983, 0.001039284, 0.001417870, 0.001110612, 0.001297268\n17                             0.0010719832, 0.0000000000, 0.0015104128, 0.0009296520, 0.0017907632, 0.0007307585, 0.0008494813\n29                                           0.0000000000, 0.0008142118, 0.0009393129, 0.0013989989, 0.0012870298, 0.0006120168\n41                             0.0008142118, 0.0000000000, 0.0007692215, 0.0007293181, 0.0007540881, 0.0009920877, 0.0006784241\n53 0.0000000000, 0.0005532172, 0.0007069914, 0.0010234291, 0.0007360076, 0.0005753173, 0.0004691258, 0.0006728587, 0.0004157941\n65                             0.0005532172, 0.0000000000, 0.0006268231, 0.0018409190, 0.0014996188, 0.0008237842, 0.0007788561\n                                      nb\n5                1, 2, 39, 152, 158, 166\n17          1, 2, 39, 162, 163, 166, 171\n29               3, 4, 10, 110, 140, 141\n41          3, 4, 11, 110, 116, 118, 130\n53 5, 6, 9, 117, 119, 121, 122, 123, 158\n65           5, 6, 7, 121, 151, 158, 159\n\n\nBefore computing Gi*, we need to set the seed value so that the results of the simulations will be reproducible and constant. I will be using seed 123.\n\n\nCode\nset.seed(123)\n\n\nThe code chunk below computes Gi* for each location, grouping by Date and using local_gstar_perm() of sfdep package:\n\n\nCode\ngi_stars <- vr_nb %>%\n  group_by(Date) %>%\n  mutate(gi_star = local_gstar_perm(\n    Vaccination_Rate, nb, wt, nsim=99)) %>%\n      tidyr::unnest(gi_star)\n\n\nAfter running 100 simulations, the code chunk below shows our newly created df.\n\n\nCode\ngi_stars\n\n\n# A tibble: 3,132 × 13\n# Groups:   Date [12]\n   Date       Sub_Di…¹ Vacci…² wt    nb    gi_star    e_gi  var_gi p_value p_sim\n   <date>     <chr>      <dbl> <lis> <lis>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n 1 2021-07-01 KEAGUNG…    32.7 <dbl> <int>   1.33  0.00385 6.66e-8 1.83e-1  0.26\n 2 2021-07-01 GLODOK      50.5 <dbl> <int>   3.81  0.00380 4.47e-8 1.38e-4  0.02\n 3 2021-07-01 HARAPAN…    31.8 <dbl> <int>  -0.846 0.00386 5.54e-8 3.97e-1  0.38\n 4 2021-07-01 CEMPAKA…    32.6 <dbl> <int>  -1.15  0.00386 6.01e-8 2.50e-1  0.2 \n 5 2021-07-01 PASAR B…    47.9 <dbl> <int>   2.72  0.00382 4.15e-8 6.43e-3  0.02\n 6 2021-07-01 KARANG …    34.8 <dbl> <int>   0.926 0.00387 4.84e-8 3.55e-1  0.34\n 7 2021-07-01 MANGGA …    35.1 <dbl> <int>   0.831 0.00383 4.12e-8 4.06e-1  0.44\n 8 2021-07-01 PETOJO …    34.7 <dbl> <int>  -0.105 0.00385 6.23e-8 9.16e-1  0.88\n 9 2021-07-01 SENEN       39.6 <dbl> <int>   0.862 0.00386 3.45e-8 3.89e-1  0.48\n10 2021-07-01 BUNGUR      37.4 <dbl> <int>  -0.150 0.00383 4.53e-8 8.81e-1  0.92\n# … with 3,122 more rows, 3 more variables: p_folded_sim <dbl>, skewness <dbl>,\n#   kurtosis <dbl>, and abbreviated variable names ¹​Sub_District,\n#   ²​Vaccination_Rate"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#mapping-of-gi-and-monthly-vaccination-rates",
    "href": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#mapping-of-gi-and-monthly-vaccination-rates",
    "title": "Take-home Assignment 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "6.2 Mapping of Gi* and Monthly Vaccination Rates",
    "text": "6.2 Mapping of Gi* and Monthly Vaccination Rates\nTo map our local gi*, we first need to include the geometry values by joining copy_cjkt_df with gi_stars.\n\n\nCode\ncombined_cjkt_gi_stars <- copy_cjkt_df %>%\n  left_join(gi_stars)\n\n\n\n6.2.1 Mapping for July 2021\nTaking a look at the Gi* values and its p-value of the vaccination rates of July 2021:\n\n\nCode\ntmap_mode(\"plot\")\ngi_star_map = tm_shape(filter(combined_cjkt_gi_stars, Date == '2021-07-01')) +\n  tm_fill(\"gi_star\") +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"Gi* values for vaccination rates in July 2021\", main.title.size=0.8)\n\np_sim_map = tm_shape(filter(combined_cjkt_gi_stars, Date == '2021-07-01')) +\n  tm_fill(\"p_sim\", breaks = c(0, 0.05, 1)) +\n  tm_borders(alpha=0.5) + \n  tm_layout(main.title = \"p-values of Gi* for vaccination rates in July 2021\", main.title.size=0.8)\n\ntmap_arrange(gi_star_map, p_sim_map)\n\n\n\n\n\n\n\nCode\npsig_july2021 <- combined_cjkt_gi_stars  %>%\n  filter(p_sim < 0.05 & Date == '2021-07-01')\n\ntmap_mode(\"plot\")\ntm_shape(combined_cjkt_gi_stars) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(psig_july2021) +\n  tm_fill(\"gi_star\") + \n  tm_borders(alpha = 0.4) +\n   tm_layout(main.title=\"Hot and Cold Spot Areas of Vaccination Rates in July 2021\", main.title.size = 0.8)\n\n\n\n\n\n\n\n6.2.1 Mapping for All Time Periods\nThe code chunk below creates a function called gi_plot() which takes in a date value and displays the respective maps for that specific date with the gi* values that is significant.\n\n\nCode\ntmap_mode(\"plot\")\n\ngi_plot <- function(date) {\n  psig <- combined_cjkt_gi_stars  %>%\n  #filter significant values(<0.05) and the according date\n  filter(p_sim < 0.05 & Date == date)\n\n    \n    hcs<- tm_shape(combined_cjkt_gi_stars) +\n      tm_polygons() +\n      tm_borders(alpha = 0.5) +\n    tm_shape(psig) +\n    tm_fill(\"gi_star\") + \n    tm_borders(alpha = 0.4) +\n     tm_layout(\n       main.title=paste(\"Hot and Cold Spot Areas of Vaccination Rates in\",date),\n       main.title.size = 0.6)\n\n}\n\n\n\n\nCode\ntmap_arrange(gi_plot(\"2021-07-01\"), gi_plot(\"2021-08-01\"))\n\n\n\n\n\n\n\nCode\ntmap_arrange(gi_plot(\"2021-09-01\"), gi_plot(\"2021-10-01\"))\n\n\n\n\n\nCode\ntmap_arrange(gi_plot(\"2021-11-01\"), gi_plot(\"2021-12-01\"))\n\n\n\n\n\nCode\ntmap_arrange(gi_plot(\"2022-01-01\"), gi_plot(\"2022-02-01\"))\n\n\n\n\n\nCode\ntmap_arrange(gi_plot(\"2022-03-02\"), gi_plot(\"2022-04-01\"))\n\n\n\n\n\nCode\ntmap_arrange(gi_plot(\"2022-05-01\"), gi_plot(\"2022-06-01\"))\n\n\n\n\n\nThe code chunk below creates a function to check the sub-district with the highest and lowest siginificant gi* value for July 2021, October 2021, February 2022, June 2022.\n\n\nCode\nget_significant_locations <- function(date) {\n  #print(paste(\"Sub-Districts with p-value of Gi* < 0.05 in\", month))\n  \n  #sub district name\n  sub_d<- filter(combined_cjkt_gi_stars, Date == date & p_sim<0.05)$Sub_District\n  \n  #gi star value\n  gi_star_value <- filter(combined_cjkt_gi_stars, Date == date & p_sim<0.05)$gi_star\n\n  temp <-data.frame(sub_d,gi_star_value)\n  #print(temp)\n  \n  #sub districts with significant p-values in date\n  sig_sd<- temp$sub_d\n  \n  #max gi star value in date\n  max_gi_sd<- temp$sub_d[temp$gi_star_value ==max(temp$gi_star_value)]\n  \n  # min gi star value in date\n  min_gi_sd<- temp$sub_d[temp$gi_star_value ==min(temp$gi_star_value)]\n  \n  print(paste(\"Date: \", date))\n  print(\"For siginificant sub-district with the\")\n  print(paste(\"highest gi* value: \",max_gi_sd))\n  print(paste(\"lowest gi* value: \",min_gi_sd))\n  print(paste(\"Number of Significant sub_districts:\",length(sig_sd)))\n  \n}\n\nget_significant_locations(\"2021-07-01\")\n\n\n[1] \"Date:  2021-07-01\"\n[1] \"For siginificant sub-district with the\"\n[1] \"highest gi* value:  MELAWAI\"\n[1] \"lowest gi* value:  CIPINANG BESAR UTARA\"\n[1] \"Number of Significant sub_districts: 52\"\n\n\nCode\nget_significant_locations(\"2021-10-01\")\n\n\n[1] \"Date:  2021-10-01\"\n[1] \"For siginificant sub-district with the\"\n[1] \"highest gi* value:  KELAPA GADING TIMUR\"\n[1] \"lowest gi* value:  BATU AMPAR\"\n[1] \"Number of Significant sub_districts: 55\"\n\n\nCode\nget_significant_locations(\"2022-02-01\")\n\n\n[1] \"Date:  2022-02-01\"\n[1] \"For siginificant sub-district with the\"\n[1] \"highest gi* value:  SRENGSENG SAWAH\"\n[1] \"lowest gi* value:  KEBON KACANG\"\n[1] \"Number of Significant sub_districts: 52\"\n\n\nCode\nget_significant_locations(\"2022-06-01\")\n\n\n[1] \"Date:  2022-06-01\"\n[1] \"For siginificant sub-district with the\"\n[1] \"highest gi* value:  SRENGSENG SAWAH\"\n[1] \"lowest gi* value:  KEBON KACANG\"\n[1] \"Number of Significant sub_districts: 47\"\n\n\n\n\n6.2.3 Statistical Conclusion\n\nBased on our maps, areas colored greenish are significant hot spots, while areas colored orange to reddish are significant cold spots.\nOverall, we can observe that there is spatial clustering in our data. From July 2021 to June 2022, the intensity of Gi* value has changed. The lowest range of Gi* value decreased from range -4 to -2 to range -8 to -6.\nIn October 2021, there are unusually highly significant 7 hot and 5 cold spots.\nHot Spots Analysis: From July 2021 to June 2022, the distribution of the hot spots started from 6 clusters (most of it was around the northern central side of Jakarta) and remained with 6 clusters at the end but it shifted downwards to the southern side of Jakarta. By June 2022, the most significant cluster was the the south-west section of Jakarta.\nCold Spots Analysis: The distribution of the cold spots started from 6 clusters and ended with 4 clusters. Besides the most significant cluster which mainly stayed in the middle of Jakarta, the other clusters started to shift from central area to the outer corners of Jakarta.\nWe can also see that the number of significant sub-districts increased from 52 (July 2021) to 55 (October 2021) and then decreased to 47 (June 2021) and this aligns with the changes in the number of hot and cold spot areas."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#mann-kendall-test",
    "href": "Take-home_Assgn/Take-home_Assgn2/Take-home_Assgn2.html#mann-kendall-test",
    "title": "Take-home Assignment 2: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta",
    "section": "7.1 Mann-Kendall Test",
    "text": "7.1 Mann-Kendall Test\nWe will be conducting the Mann-Kendall Test using the spatio-temporal local Gi* values on 3 Sub-Districts which are:\n\nKEBON KACANG\nSRENGSENG SAWAH\nKOJA\n\nWhat is the Mann-Kendall test?\nIn a Mann-Kendall Test, it is used to determine whether or not a trend exists in time series data. It is a non-parametric test, meaning there is no underlying assumption made about the normality of the data.\nThe hypotheses for the test are as follows:\n\nH0 (null hypothesis): There is no trend present in the data.\nHA (alternative hypothesis): A trend is present in the data. (This could be a positive or negative trend)\n\nIf the p-value of the test is lower than the significance level of 0.05, then there is statistically significant evidence that a trend is present in the time series data.\nThis method mainly gives 3 type of information:\n\nTau: varies between -1 and 1; it is positive when the trend increases and negative when the trend decreases\nThe Sen slope: which estimates the overall slope of the time series. This slope corresponds to the median of all the slopes calculated between each pair of points in the series.\nThe significance, which represents the threshold for which the hypothesis that there is no trend is accepted. The trend is statistically significant when the p-value is less than 0.05.\n\n\n7.1.1 For KEBON KACANG\n\n\nCode\nKEBON_KACANG <- gi_stars %>% \n  ungroup() %>% \n  filter(Sub_District == \"KEBON KACANG\") |> \n  select(Sub_District, Date, gi_star)\n\nKEBON_KACANG_plot<- ggplot(data = KEBON_KACANG, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n  \n  plot<- KEBON_KACANG_plot+ggtitle(\"Local Gi* values of KEBON KACANG\")\n\n  ggplotly(plot)  \n\n\n\n\n\n\n\n\nCode\nKEBON_KACANG %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n\n# A tibble: 1 × 5\n     tau       sl     S     D  varS\n   <dbl>    <dbl> <dbl> <dbl> <dbl>\n1 -0.758 0.000779   -50  66.0  213.\n\n\nBased on the output above, we can observe that the tau (-0.758) is highly negative suggesting that the trend is decreasing. The sl (0.000779), which represents the p-value, is smaller than our level of significance(0.05). Hence, we reject the null hypothesis as there is significant evidence that a decreasing trend is present in the time series data.\n\n\n7.1.2 For SRENGSENG SAWAH\n\n\nCode\nSRENGSENG_SAWAH <- gi_stars %>% \n  ungroup() %>% \n  filter(Sub_District == \"SRENGSENG SAWAH\") |> \n  select(Sub_District, Date, gi_star)\n\nSRENGSENG_SAWAH_plot<- ggplot(data = SRENGSENG_SAWAH, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n  \n  plot<- SRENGSENG_SAWAH_plot+ggtitle(\"Local Gi* values of SRENGSENG SAWAH\")\n\n  ggplotly(plot)  \n\n\n\n\n\n\n\n\nCode\nSRENGSENG_SAWAH %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n\n# A tibble: 1 × 5\n    tau      sl     S     D  varS\n  <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 0.697 0.00203    46  66.0  213.\n\n\nBased on the output above, we can observe that the tau(0.697) is positive suggesting that the trend is increasing. The sl(0.00203), which represents the p-value, is smaller than our level of significance(0.05). Hence, we reject the null hypothesis as there is significant evidence that an increasing trend is present in the time series data.\n\n\n7.1.3 For KOJA\n\n\nCode\nKOJA <- gi_stars %>% \n  ungroup() %>% \n  filter(Sub_District == \"KOJA\") |> \n  select(Sub_District, Date, gi_star)\n\nKOJA_plot<- ggplot(data = KOJA, \n       aes(x = Date, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\n  plot<- KOJA_plot+ggtitle(\"Local Gi* values of KOJA\")\n\n  ggplotly(plot)  \n\n\n\n\n\n\n\n\nCode\nKOJA %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n\n\n# A tibble: 1 × 5\n      tau    sl     S     D  varS\n    <dbl> <dbl> <dbl> <dbl> <dbl>\n1 -0.0606 0.837    -4  66.0  213.\n\n\nBased on the output above, we can observe that the tau(-0.0606) is slightly negative, suggesting that the trend is increasing. The sl(0.837), which represents the p-value, is larger than our level of significance(0.05). Hence, we do not reject the null hypothesis as there is insufficient evidence that a trend is present in the time series data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#section-1",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#section-1",
    "title": "In-class Ex 9",
    "section": "",
    "text": "Code\nwrite_rds(train_data,'data/model/train_data.rds')\nwrite_rds(test_data,'data/model/test_data.rds')\n\n\n\n\nCode\nprice_mlr <- lm(resale_price ~ floor_area_sqm + storey_order +\n                  remaining_lease_mths + PROX_CBD +PROX_ELDERLYCARE+\n                  PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_data) #impt for predictive model\n\n\nsummary(price_mlr)\n\n#for predictive model, important parts:\n#Residual standard error: 61390 on 10320 degrees of freedom\n#Multiple R-squared:  0.7427,   Adjusted R-squared:  0.7424 \n\n\n\n\nCode\nwrite_rds(price_mlr,\"data/model/price_mlr.rds\")\n\n\nConvert simple feature to spatial\n\n\nCode\ntrain_data_sp <- as_Spatial(train_data)\ntrain_data_sp\n\n\nTake note: should check that data is always training data\nRanger does not know how to handle simple feature (cannot understand the list in geometry table). Thus, we need to prepare coordinates data.\n\n\nCode\n#extracts geometry info\ncoords <- st_coordinates(mdata)\ncoords_train <- st_coordinates(train_data)\ncoords_test<- st_coordinates(test_data)\n\n\n\n\nCode\n#writes to rds file\ncoords_train <- write_rds(coords_train, \"data/model/coords_train.rds\")\ncoords_test <- write_rds(coords_test, \"data/model/coords_test.rds\")\n\n\n\n\nCode\n#drop geomtry\ntrain_data <- train_data %>%\n  st_drop_geometry()\n\n\n\n\nCode\nset.seed(123)\n\nrf <- ranger(resale_price ~ floor_area_sqm + storey_order +\n                  remaining_lease_mths + PROX_CBD +PROX_ELDERLYCARE+\n                  PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_data)\n\n\n\n\nCode\nprint(rf)\n\n\nOOB prediction error (MSE): 695049755 is the mean sqr value, if we want to use MSE needs to square root it first, compared with residual standard error.\n\n\nCode\n#should not be ore than 10 mins\nset.seed(1234)\n\ngwRF_adaptive<- grf(formula = resale_price ~ floor_area_sqm +\n                      storey_order +\n                  remaining_lease_mths + PROX_CBD +PROX_ELDERLYCARE+\n                  PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL +\n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  dframe= train_data,\n                  bw=55, #no. of observations/closest neighbour\n                  kernel=\"adaptive\", #if calibrate as fixed, bw is diff,\n                  coords=coords_train #put the coords back in \n                  )\n\n\nsave model result\n\n\nCode\nwrite_rds(gwRF_adaptive,'data/model/gwRF_adaptive.rds')\n\n\ngwRF_adaptive$Global.Model\ngwRF_adaptive$Global.Model\n$variable.importance\n\n\nCode\ntest_data <- cbind(test_data, coords_test) %>%\n  st_drop_geometry()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/data/geospatial/ELDERCARE.html",
    "href": "In-class_Ex/In-class_Ex10/data/geospatial/ELDERCARE.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>  ELDERCARE  ENG dataset\n\nELDERCARE\n\n                 0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/data/geospatial/hexagons.html",
    "href": "In-class_Ex/In-class_Ex10/data/geospatial/hexagons.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n                 0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html",
    "title": "In-class Ex 10",
    "section": "",
    "text": "Code\npacman::p_load(tmap, SpatialAcc, sf, readr, tidyr, fca,\n               ggstatsplot, reshape2,\n               tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#geospatial-data-wrangling",
    "title": "In-class Ex 10",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\n\n\nCode\nmpsz <- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_NO_SEA_PL\")\n\nhexagons <- st_read(dsn = \"data/geospatial\", layer = \"hexagons\") \n\neldercare <- st_read(dsn = \"data/geospatial\", layer = \"ELDERCARE\") \n\n\n\nUpdating CRS information\n\n\nCode\nmpsz <- st_transform(mpsz, 3414)\neldercare <- st_transform(eldercare, 3414)\nhexagons <- st_transform(hexagons, 3414)\n\n\n\n\nCode\nst_crs(mpsz)\n\n\n\n\nCleaning and updating attribute fields of the geospatial data\n\n\nCode\neldercare <- eldercare %>%\n  select(fid, ADDRESSPOS) %>%\n  rename(destination_id=fid,\n         postal_code= ADDRESSPOS) %>%\n  mutate(capacity = 100)\n\nhexagons <- hexagons %>%\n  select(fid) %>%\n  rename(origin_id=fid) %>%\n  mutate(demand = 100)\n\n\nNotice that for the purpose of this hands-on exercise, a constant value of 100 is used. In practice, actual demand of the hexagon and capacity of the eldercare centre should be used."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#apsaital-data-handling-and-wrangling",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#apsaital-data-handling-and-wrangling",
    "title": "In-class Ex 10",
    "section": "Apsaital Data Handling and Wrangling",
    "text": "Apsaital Data Handling and Wrangling\n\nImporting Distance Matrix\n\n\nCode\nODMatrix <- read_csv(\"data/aspatial/OD_Matrix.csv\", skip = 0)\n\n\n\n\nCode\n#row is origin, col is destination\n# for fca, it is the opposite\n\ndistmat <- ODMatrix %>%\n  select(origin_id, destination_id, total_cost) %>%\n  spread(destination_id, total_cost)%>%\n  select(c(-c('origin_id')))\n\n\n\n\nCode\n#from metres to km\ndistmat_km <- as.matrix(distmat/1000)\n\n\n\n\nComputing Distance Matrix (Optional)\n\n\nCode\neldercare_coord<- st_coordinates(eldercare)\nhexagon_coord <- st_coordinates(hexagons\n                                )\n\n\n\n\nCode\nEucMatrix <- SpatialAcc::distance(hexagon_coord,\n                                  eldercare_coord,\n                                  type=\"euclidean\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#modelling-and-visualising-accessibility-using-hansen-method",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#modelling-and-visualising-accessibility-using-hansen-method",
    "title": "In-class Ex 10",
    "section": "Modelling and Visualising Accessibility using Hansen Method",
    "text": "Modelling and Visualising Accessibility using Hansen Method\n\nComputing Hansen’s accessibility\nThe code chunk below calculates Hansen’s accessibility using ac() of SpatialAcc and data.frame() is used to save the output in a data frame called acc_Handsen.\n\n\nCode\nacc_Hansen <- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 2, \n                            family = \"Hansen\"))\n\n\nThe default field name is very messy, we will rename it to accHansen by using the code chunk below.\n\n\nCode\ncolnames(acc_Hansen) <- \"accHansen\"\n\n\nNext, we will convert the data table into tibble format by using the code chunk below.\n\n\nCode\n#Warning: `tbl_df()` was deprecated in dplyr 1.0.0.\nacc_Hansen <- tibble::as_tibble(acc_Hansen)\n\n\nLastly, bind_cols() of dplyr will be used to join the acc_Hansen tibble data frame with the hexagons simple feature data frame. The output is called hexagon_Hansen.\n\n\nCode\nhexagon_Hansen <- bind_cols(hexagons, acc_Hansen)\n\n\nThe steps above can be perform by using a single code chunk as shown below.\n\n\nCode\nacc_Hansen <- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 0.5, \n                            family = \"Hansen\"))\n\ncolnames(acc_Hansen) <- \"accHansen\"\nacc_Hansen <- tibble::as_tibble(acc_Hansen)\nhexagon_Hansen <- bind_cols(hexagons, acc_Hansen)\n\n\n\n\nVisualising Hansen’s accessibility\n\n\nCode\nmapex <- st_bbox(hexagons)\n\n\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape(hexagon_Hansen,\n         bbox = mapex) + \n  tm_fill(col = \"accHansen\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: Hansen method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)\n\n\n\n\nStatistical graphic visualisation\n\n\nCode\nhexagon_Hansen <- st_join(hexagon_Hansen, mpsz, \n                          join = st_intersects)\n\n\n\n\nCode\nggplot(data=hexagon_Hansen, \n       aes(y = log(accHansen), \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#modelling-and-visualising-accessibility-using-kd2sfca-method",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#modelling-and-visualising-accessibility-using-kd2sfca-method",
    "title": "In-class Ex 10",
    "section": "Modelling and Visualising Accessibility using KD2SFCA Method",
    "text": "Modelling and Visualising Accessibility using KD2SFCA Method\n\nComputing KD2SFCA’s accessibility\n\n\nCode\nacc_KD2SFCA <- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            d0 = 50,\n                            power = 2, \n                            family = \"KD2SFCA\"))\n\ncolnames(acc_KD2SFCA) <- \"accKD2SFCA\"\nacc_KD2SFCA <- tbl_df(acc_KD2SFCA)\nhexagon_KD2SFCA <- bind_cols(hexagons, acc_KD2SFCA)\n\n\n\n\nVisualising KD2SFCA’s accessibility\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape(hexagon_KD2SFCA,\n         bbox = mapex) + \n  tm_fill(col = \"accKD2SFCA\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: KD2SFCA method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 6),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)\n\n\n\n\nStatistical graphic visualisation\n\n\nCode\nhexagon_KD2SFCA <- st_join(hexagon_KD2SFCA, mpsz, \n                          join = st_intersects)\n\n\n\n\nCode\nggplot(data=hexagon_KD2SFCA, \n       aes(y = accKD2SFCA, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#modelling-and-visualising-accessibility-using-spatial-accessibility-measure-sam-method",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#modelling-and-visualising-accessibility-using-spatial-accessibility-measure-sam-method",
    "title": "In-class Ex 10",
    "section": "Modelling and Visualising Accessibility using Spatial Accessibility Measure (SAM) Method",
    "text": "Modelling and Visualising Accessibility using Spatial Accessibility Measure (SAM) Method\n\nComputing SAM accessibility\n\n\nCode\nacc_SAM <- data.frame(ac(hexagons$demand,\n                         eldercare$capacity,\n                         distmat_km, \n                         d0 = 50,\n                         power = 2, \n                         family = \"SAM\"))\n\ncolnames(acc_SAM) <- \"accSAM\"\nacc_SAM <- tbl_df(acc_SAM)\nhexagon_SAM <- bind_cols(hexagons, acc_SAM)\n\n\n\n\nVisualising SAM’s accessibility\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape(hexagon_SAM,\n         bbox = mapex) + \n  tm_fill(col = \"accSAM\",\n          n = 10,\n          style = \"quantile\",\n          border.col = \"black\",\n          border.lwd = 1) +\ntm_shape(eldercare) +\n  tm_symbols(size = 0.1) +\n  tm_layout(main.title = \"Accessibility to eldercare: SAM method\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.outside = FALSE,\n            legend.height = 0.45, \n            legend.width = 3.0,\n            legend.format = list(digits = 3),\n            legend.position = c(\"right\", \"top\"),\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.5)\n\n\n\n\nStatistical graphic visualisation\n\n\nCode\nhexagon_SAM <- st_join(hexagon_SAM, mpsz, \n                       join = st_intersects)\n\n\n\n\nCode\nggplot(data=hexagon_SAM, \n       aes(y = accSAM, \n           x= REGION_N)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             colour =\"red\", \n             size=2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "Welcome to IS415 Geospatial Analytics and Applications.\nThis is the course website of IS415 I study this term. You will find my course work on this website."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html",
    "title": "IS415-GAA",
    "section": "",
    "text": "Hello! This is Rhonda Ho’s take-home Assignment 3 for IS415 module.\nTo view/hide all the code at once, please click on the “</> code” tab beside the title of this html document and select the option to view/hide the code.\nThe full details of this assignment can be found here.\n\n\nIn this take-home exercise, I am tasked to predict HDB resale prices at the sub-market level (i.e. HDB 3-room, HDB 4-room and HDB 5-room) for the month of January and February 2023 in Singapore. The predictive models must be built by using by using conventional OLS method and GWR methods. I am also required to compare the performance of the conventional OLS method versus the geographical weighted methods.\n\n\n\nBelow is the list of datasets i have collected.\n\n\n\nType\nDataset\nFile Format\n\n\n\n\nAspatial\nResale Flat Prices\n.csv\n\n\nGeospatial\nMaster Plan 2014 Subzone Boundary (Web)\n.kml\n\n\nGeospatial\nBus Stop Location\n.kml\n.shp\n\n\nGeospatial\nTrain Station\n.kml\n.shp\n\n\nGeospatial\nSchool Directory and Information\n.csv\n\n\nGeospatial-Extracted\nChild Care Services\n.shp\n\n\nGeospatial-Extracted\nEldercare Services\n.rds\n\n\nGeospatial-Extracted\nHawker Centres\n.rds\n\n\nGeospatial-Extracted\nKindergarterns\n.rds\n\n\nGeospatial-Extracted\nParks\n.rds\n\n\nGeospatial\nSupermarket\n.geoson\n\n\nGeospatial\nShopping Malls\n.csv"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#geospatial-data",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#geospatial-data",
    "title": "IS415-GAA",
    "section": "Geospatial Data",
    "text": "Geospatial Data\n\nMaster Plan 2019 Boundary\nThe code chunk below retrieves the geospatial polygon data for Singapore’s region in 2019.\n\nmpsz <- st_read(dsn=\"data/geospatial/MP14_SUBZONE_WEB_PL.kml\") %>%\n  st_transform(crs = 3414)\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\RhondaHO\\IS415-GAA\\Take-home_Assgn\\Take-home_Assgn3\\data\\geospatial\\MP14_SUBZONE_WEB_PL.kml' \n  using driver `KML'\nSimple feature collection with 323 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nLocational Factors Extracted via onemapAPI token\nFor some of the locational factors, I will be utilising onemapAPI to retrieve its geometry data.\nOne method I discovered was the usage of a token to retrieve geometry of locational factors based on its related theme. But before we can proceed on, we need to register for account here and retrieve the token.\n\ntoken <- get_token(\"user@example.com\", \"password\")\n\nThe code chunk below helps us view the available themes. As a token is needed, I first saved the output in an rds file.\n\n#retrieve available themes that we can refer to\navail_themes <-search_themes(token)\nwrite_rds(avail_themes, \"data/rds/available_themes.rds\")\n\nBy reading the according file, I sorted the themes in alphabetical order, for easier reference.\n\n#read the file for available themes\navail_themes<-readRDS(\"data/rds/available_themes.rds\")\n\n#sort by alphabetical order\navail_themes<-avail_themes[order(avail_themes$THEMENAME),]\navail_themes\n\n# A tibble: 193 × 5\n   THEMENAME                                     QUERYNAME ICON  CATEG…¹ THEME…²\n   <chr>                                         <chr>     <chr> <chr>   <chr>  \n 1 ABC Waters Completed                          abcwater… abcl… Enviro… PUBLIC…\n 2 ABC Waters Construction                       abcwater… tend… Enviro… PUBLIC…\n 3 ABC Waters Sites                              abcwater… abcl… Enviro… PUBLIC…\n 4 Active Cemeteries                             activece… circ… Enviro… NATION…\n 5 Aedes Mosquito Breeding Habitats - Central    breeding… Pict… Health  NATION…\n 6 Aedes Mosquito Breeding Habitats - North East breeding… Pict… Health  NATION…\n 7 Aedes Mosquito Breeding Habitats - North West breeding… Pict… Health  NATION…\n 8 Aedes Mosquito Breeding Habitats - South East breeding… Pict… Health  NATION…\n 9 Aedes Mosquito Breeding Habitats - South West breeding… Pict… Health  NATION…\n10 After Death Facilities                        afterdea… coff… Enviro… NATION…\n# … with 183 more rows, and abbreviated variable names ¹​CATEGORY, ²​THEME_OWNER\n\n\nLooking through the available themes from onemap API, some of the themes relevant to our tasks is:\n\n\n\n\n\n\n\nTheme Name\nQuery Name\n\n\n\n\nChild Care Services\nchildcare\n\n\nEldercare Services\neldercare\n\n\nHawker Centres_New (Note: there are other similar themes such as Hawker Centres and Healthier Hawker Centres)\nhawkercentre_new\n\n\nKindergartens\nkindergartens\n\n\nParks (Note: there are other similar themes such as Parks@SG and NParks Parks and Nature Reserves)\nnationalparks\n\n\n\nFor the following code chunks, I created a shapefile for each locational factor I am interested in.\nThe steps taken are:\n\nRetrieve data such as the geometry and name of the place/amenity by using onemap’s get_theme() function which takes in a theme (i.e query name) and the store the data in a df\nConvert the df to a sf object by using the st_as_sf() function\nTransform crs information to Singapore coordinates system i.e 3414 by using st_transform() function\nWrite to a shapefile using st_write() function\n\n\nChildcareEldercareHawker CentresKindergartensParks\n\n\n#| eval: false\n#theme: childcare\n\n#retrieve the data such as the geometry and name  accordingly to the theme\nchildcare_tibble <- get_theme(token, \"childcare\")\n\n# to convert a data frame of coordinates to an sf object and transform the crs information and create a shapefile for it\nchildcare_sf <- st_as_sf(childcare_tibble, coords=c(\"Lng\", \"Lat\"), \n                        crs=4326) %>% \n  st_transform(crs = 3414)\nst_write(obj = childcare_sf,\n         dsn = \"data/geospatial\",\n         layer = \"childcare\",\n         driver = \"ESRI Shapefile\",\n         append=FALSE)\n\n\n#| eval: false\n#theme: eldercare\n\n#retrieve the data such as the geometry and name based accordingly to the theme\neldercare_tibble <- get_theme(token, \"eldercare\")\n\n# to convert a data frame of coordinates to an sf object and transform the crs information and create a shapefile for it\neldercare_sf <- st_as_sf(eldercare_tibble, coords=c(\"Lng\", \"Lat\"), \n                        crs=4326) %>% \n  st_transform(crs = 3414)\n\nwrite_rds(eldercare_sf, \"data/rds/eldercare.rds\")\n\n\n#| eval: false\n#theme: new hawker centres\n\n#retrieve the data such as the geometry and name based accordingly to the theme\n#hawkercentre_new_tibble <- get_theme(token, \"hawkercentre_new\")\n\n# to convert a data frame of coordinates to an sf object and transform the crs information and create a shapefile for it\nhawkercentre_new_sf <- st_as_sf(hawkercentre_new_tibble, coords=c(\"Lng\", \"Lat\"), \n                        crs=4326) %>% \n  st_transform(crs = 3414)\n\nwrite_rds(hawkercentre_new_sf, \"data/rds/hawker_new.rds\")\n\n#issues writing in this manner\n#st_write(obj = hawkercentre_new_sf,\n#         dsn = \"data/geospatial\",\n#         layer = \"hawkercentre_new\",\n#        driver = \"ESRI Shapefile\",\n#         append=FALSE)\n\n\n#| eval: false\n#theme: kindergartens\n\n#retrieve the data such as the geometry and name based accordingly to the theme\nkindergartens_tibble <- get_theme(token, \"kindergartens\")\n\n# to convert a data frame of coordinates to an sf object and transform the crs information and create a shapefile for it\nkindergartens_sf <- st_as_sf(kindergartens_tibble, coords=c(\"Lng\", \"Lat\"), \n                        crs=4326) %>% \n  st_transform(crs = 3414)\n\nwrite_rds(kindergartens_sf, \"data/rds/kindergartens.rds\")\n\n\n#| eval: false\n#theme: parks\n\n#retrieve the data such as the geometry and name based accordingly to the theme\nnationalparks_tibble <- get_theme(token, \"nationalparks\")\n\n# to convert a data frame of coordinates to an sf object and transform the crs information and create a shapefile for it\nnationalparks_sf <- st_as_sf(nationalparks_tibble, coords=c(\"Lng\", \"Lat\"), \n                        crs=4326) %>% \n  st_transform(crs = 3414)\n\nwrite_rds(nationalparks_sf, \"data/rds/nationalparks.rds\")\n\n\n\nImport the following to be used for our tasks later.\n\nchildcare_sf <-st_read(\"data/geospatial\", layer=\"childcare\")\n\nReading layer `childcare' from data source \n  `C:\\RhondaHO\\IS415-GAA\\Take-home_Assgn\\Take-home_Assgn3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1925 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nProjected CRS: SVY21 / Singapore TM\n\neldercare_sf<- readRDS(\"data/rds/eldercare.rds\")\nhawkercentre_new_sf <- readRDS(\"data/rds/hawker_new.rds\")\nkindergartens_sf <- readRDS(\"data/rds/kindergartens.rds\")\nnationalparks_sf <- readRDS(\"data/rds/nationalparks.rds\")\n\n\n\nCBD Area\nFor this assignment, I consider the CBD area to be in the Downtown Core so I will be using the coordinates of Downtown Core .\n\nlat= c(1.287953)\nlng= c(103.851784)\n\ncbd_sf <- data.frame(lat, lng) %>%\n  st_as_sf(coords = c(\"lng\", \"lat\"), crs=4326) %>%\n  st_transform(crs=3414)\n\n\n\nSupermarket\n\nsupermarket_sf <- st_read(\"data/geospatial/supermarkets-geojson.geojson\") \n\nReading layer `supermarkets-geojson' from data source \n  `C:\\RhondaHO\\IS415-GAA\\Take-home_Assgn\\Take-home_Assgn3\\data\\geospatial\\supermarkets-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 526 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6258 ymin: 1.24715 xmax: 104.0036 ymax: 1.461526\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nsupermarket_sf <- supermarket_sf %>%\n  st_transform(crs = 3414)\n\n\n\nBus Stop\n\nbus_stop<- st_read(dsn = \"data/geospatial\", layer = \"BusStop\")\n\nReading layer `BusStop' from data source \n  `C:\\RhondaHO\\IS415-GAA\\Take-home_Assgn\\Take-home_Assgn3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5159 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\nbus_stop_sf <- bus_stop %>%\n  st_transform(crs = 3414)\n\n\n\nMRT/LRT\n\nmrt = st_read(dsn = \"data/geospatial/\", layer = \"Train_Station_Exit_Layer\")\n\nReading layer `Train_Station_Exit_Layer' from data source \n  `C:\\RhondaHO\\IS415-GAA\\Take-home_Assgn\\Take-home_Assgn3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 562 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 6134.086 ymin: 27499.7 xmax: 45356.36 ymax: 47865.92\nProjected CRS: SVY21\n\nmrt_sf <- mrt %>%\n  st_transform(crs = 3414)\n\n\n\nPrimary School\nWhile searching for a dataset for primary school locations, i’ve crossed upon this dataset from data.gov.sg which has the general information of schools in Singapore. By filtering out themainlevel_code which represents the different types of schools to be Primary, I will be able to extract out the address and postal codes of primary schools in Singapore.\n\nprimary_school <- read_csv(\"data/geospatial/general-information-of-schools.csv\")\n\nRows: 346 Columns: 31\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (31): school_name, url_address, address, postal_code, telephone_no, tele...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprimary_school <- primary_school %>%\n  filter(mainlevel_code == \"PRIMARY\") %>%\n  select(school_name, address, postal_code)\n\nglimpse(primary_school)\n\nRows: 183\nColumns: 3\n$ school_name <chr> \"ADMIRALTY PRIMARY SCHOOL\", \"AHMAD IBRAHIM PRIMARY SCHOOL\"…\n$ address     <chr> \"11   WOODLANDS CIRCLE\", \"10   YISHUN STREET 11\", \"100  Br…\n$ postal_code <chr> \"738907\", \"768643\", \"579646\", \"159016\", \"544969\", \"569785\"…\n\n\nBased on the output, we can observe that there are 183 primary schools in Singapore. However, in 2022, the primary school, Juying Primary School was merged together with Pioneer Primary School and it cannot be found via the API . Thus, I decided to remove it out of our data.\n\nprimary_school<-primary_school %>%  \n  filter(school_name!='JUYING PRIMARY SCHOOL')\n\nOnce again, we can use geocode() function we created earlier to help us extract the geometry of each primary school by its school name.\n\nprimary_school$LATITUDE <- 0\nprimary_school$LONGITUDE <- 0\n\nfor (i in 1:nrow(primary_school)){\n  temp_output <- geocode(primary_school[i, 1],\"\")\n\n  primary_school$LATITUDE[i] <- temp_output$results.LATITUDE\n  primary_school$LONGITUDE[i] <- temp_output$results.LONGITUDE\n}\n\nTo reuse the primary school data without recalling the API, I saved it in an rds file.\n\nwrite_rds(primary_school, \"data/rds/primary_school.rds\")\n\n\nprimary_school_rds<-readRDS(\"data/rds/primary_school.rds\")\n\nNext, we can convert our df into a sf.\n\nprimary_school_sf <- st_as_sf(primary_school_rds,\n                    coords = c(\"LONGITUDE\", \n                               \"LATITUDE\"),\n                    crs=4326) %>%\n  st_transform(crs = 3414)\n\nHowever, besides this, we need to determine what is a good primary school (which is necessary for 1 of our tasks). Based on the assumption that a good primary school is a popular one, I picked out the top 10 popular primary schools, referencing its popularity from this website.\n\npopular_primary_schools <-c(\"Pei Hwa Presbyterian Primary School\",\n                            \"Gongshang Primary School\",\n                            \"Riverside Primary School\",\n                            \"Red Swastika School\",\n                            \"Punggol Green Primary School\",\n                            \"Princess Elizabeth Primary School\",\n                            \"Westwood Primary School\",\n                            \"St. Hilda’s Primary School\",\n                            \"Catholic High School (Primary Section)\",\n                            \"Ai Tong School\")\npopular_primary_schools\n\n [1] \"Pei Hwa Presbyterian Primary School\"   \n [2] \"Gongshang Primary School\"              \n [3] \"Riverside Primary School\"              \n [4] \"Red Swastika School\"                   \n [5] \"Punggol Green Primary School\"          \n [6] \"Princess Elizabeth Primary School\"     \n [7] \"Westwood Primary School\"               \n [8] \"St. Hilda’s Primary School\"            \n [9] \"Catholic High School (Primary Section)\"\n[10] \"Ai Tong School\"                        \n\n\nNext, I need to check whether the top 10 most popular primary schools can be found in the primary school data i extracted earlier. But before that, to make things consistent, I used lapply() function and make the schools names I picked out from the website to be all in uppercase.\n\n#make school names all uppercase\npopular_primary_schools <- lapply(popular_primary_schools, toupper) \n\n# to check both primary school datasets matches\npopular_primary_schools_sf <- primary_school_sf %>%\n  filter(school_name %in% popular_primary_schools)\n\n\nnrow(popular_primary_schools_sf)\n\n[1] 8\n\n\nBased on the output above, out of the 10 primary schools, only 8 can be found. The code chunk below tells us which primary schools are missing.\n\nunique(popular_primary_schools[!(popular_primary_schools %in% popular_primary_schools_sf$school_name)])\n\n[[1]]\n[1] \"ST. HILDA’S PRIMARY SCHOOL\"\n\n[[2]]\n[1] \"CATHOLIC HIGH SCHOOL (PRIMARY SECTION)\"\n\n\nLooking further into our dataset, I found out that in the original primary school dataset, the schools names of the output above is written different. For eg, CANOSSA CATHOLIC PRIMARY SCHOOL is the same as CATHOLIC HIGH SCHOOL (PRIMARY SECTION). Thus, I decided to use rbind() function to manually add both records to popular_primary_schools_sf.\n\npopular_primary_schools_sf <- popular_primary_schools_sf %>%\n  rbind(primary_school_sf %>% filter(school_name == \"CANOSSA CATHOLIC PRIMARY SCHOOL\"))\n\npopular_primary_schools_sf <- popular_primary_schools_sf %>%\n  rbind(primary_school_sf %>% filter(school_name == \"ST. HILDA'S PRIMARY SCHOOL\"))\n\n\nnrow(popular_primary_schools_sf)\n\n[1] 10\n\n\n\n\nShopping Mall\nFor shopping malls, I used the dataset extracted by Valery Lim who webscrapped the shopping malls data from its wikipedia in 2019.\n\nshopping_mall <- read.csv(\"data/geospatial/mall_coordinates_updated.csv\")\n\nshopping_mall <- shopping_mall %>%\n  select(name, latitude, longitude)\n\nglimpse(shopping_mall)\n\nRows: 184\nColumns: 3\n$ name      <chr> \"100 AM\", \"112 KATONG\", \"313@SOMERSET\", \"321 CLEMENTI\", \"600…\n$ latitude  <dbl> 1.274588, 1.305087, 1.301385, 1.312025, 1.334042, 1.437131, …\n$ longitude <dbl> 103.8435, 103.9051, 103.8377, 103.7650, 103.8510, 103.7953, …\n\n\nTaking a glimpse in our data, there is a total of 184 shopping malls in 2019.\nNext, the code chunk below transforms our data with the correct Singapore coordinates system.\n\nshopping_mall_sf <- st_as_sf(shopping_mall,\n                        coords = c(\"longitude\",\n                                   \"latitude\"),\n                        crs = 4326) %>%\n  st_transform(crs = 3414)"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#aspatialdata",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#aspatialdata",
    "title": "Take-home Assignment 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "AspatialData",
    "text": "AspatialData\n\n\nCode\nresale <- read_csv(\"data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv\")\n\nglimpse(resale)\n\n\nHaving a glimpse of our data, we can observe that this dataset has a total of 11 columns and 149071 rows. The columns consist of month, town, flat_type, block, street_name, storey_range, floor_area_sqm, flat_model, lease_commence_date, remaining_lease, resale_price.\nFor this assignment, the dataset will focus on:\n\nTransaction period: October 2022 to February 2023\nTraining dataset period: October 2022 to December 2023\nTest dataset period: October 2022 to December 2023\nType of rook flat: 5-room flats\n\nThe code chunk filters our dataset accordingly.\n\n\nCode\nresale<- resale %>% \n  filter(flat_type == \"5 ROOM\") %>%\n  filter(month >= \"2022-10\" & month < \"2023-02\")\n\n\nBased on my senior’s experience, “ST.” is usually written as “SAINT” instead - for example, St. Luke’s Primary School is written as Saint Luke’s Primary School. To address, this, we’ll replace such occurrences:\n\n\nCode\nresale$street_name <- gsub(\"ST\\\\.\", \"SAINT\", resale$street_name)\n\n\nSubsequently, as our dataset is missing the coordinates for the location, I created a function, geocode(), which calls onemapAPI to retrieve the geometry of each location.\n\n\nCode\n#library(httr)\ngeocode <- function(block, streetname) {\n  base_url <- \"https://developers.onemap.sg/commonapi/search\"\n  address <- paste(block, streetname, sep = \" \")\n  query <- list(\"searchVal\" = address, \n                \"returnGeom\" = \"Y\",\n                \"getAddrDetails\" = \"N\",\n                \"pageNum\" = \"1\")\n  \n  res <- GET(base_url, query = query)\n  restext<-content(res, as=\"text\")\n  \n  output <- fromJSON(restext)  %>% \n    as.data.frame %>%\n    select(results.LATITUDE, results.LONGITUDE)\n\n  return(output)\n}\n\n\nWhile exploring the API, I realised that the best search parameter would be to combine the column block with its street_name. Thus, the function geocode() takes in both the block and street name .\n\n\nCode\nresale$LATITUDE <- 0\nresale$LONGITUDE <- 0\n\nfor (i in 1:nrow(resale)){\n  temp_output <- geocode(resale[i, 4], resale[i, 5])\n  \n  resale$LATITUDE[i] <- temp_output$results.LATITUDE\n  resale$LONGITUDE[i] <- temp_output$results.LONGITUDE\n}"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#structural-data",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#structural-data",
    "title": "Take-home Assignment 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "Structural Data",
    "text": "Structural Data\n\nFloor Level\nlet’s first take a look at the storey_range values\n\n\nCode\nunique(resale$storey_range)\n\n\nTo use this in our model, we need to perform dummy coding on it. Based on the unique values, there are 17 storey range categories. pivot_wider() is then used to create duplicate variables representing every storey range, with the value being 1 if the observation belongs in said storey range, and 0 if otherwise.\n\n\nCode\nresale <- resale %>%\n  pivot_wider(names_from = storey_range, values_from = storey_range, \n              values_fn = list(storey_range = ~1), values_fill = 0) \n\n\n\n\nRemaining Lease\nCurrently, the remaining_lease is in string format - we need it to be numeric. How we’ll do it is to split the string, taking in the values of the month and year - and replace the original values with the calculated value of the remaining lease in years.\n\n\nCode\n#e.g of resale$remaining_lease - 53 years 10 months\nstr_list <- str_split(resale$remaining_lease, \" \")\n\n#after spliting, [53, years, 10, months]\nfor (i in 1:length(str_list)){\n  if (length(unlist(str_list[i])) > 2) {\n      year <- as.numeric(unlist(str_list[i])[1])\n      month <- as.numeric(unlist(str_list[i])[3])\n      resale$remaining_lease[i] <- year + round(month/12, 2)\n  }\n  else {\n    year <- as.numeric(unlist(str_list[i])[1])\n    resale$remaining_lease[i] <- year\n  }\n}\n\n\n\n\nAge of Unit\nTo get the age of the unit, I decided to take the current year minus the lease commence date year of the the unit.\n\n\nCode\nstr_list <- resale$lease_commence_date\n\nfor (i in 1:length(str_list)){\n    resale$lease_commence_date[i] <- 2023 -\n      resale$lease_commence_date[i]\n}"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#aspatial-data",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#aspatial-data",
    "title": "IS415-GAA",
    "section": "Aspatial Data",
    "text": "Aspatial Data\n\nResale Flat Prices\n\nresale <- read_csv(\"data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv\")\n\nRows: 149071 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): month, town, flat_type, block, street_name, storey_range, flat_mode...\ndbl (3): floor_area_sqm, lease_commence_date, resale_price\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(resale)\n\nRows: 149,071\nColumns: 11\n$ month               <chr> \"2017-01\", \"2017-01\", \"2017-01\", \"2017-01\", \"2017-…\n$ town                <chr> \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO …\n$ flat_type           <chr> \"2 ROOM\", \"3 ROOM\", \"3 ROOM\", \"3 ROOM\", \"3 ROOM\", …\n$ block               <chr> \"406\", \"108\", \"602\", \"465\", \"601\", \"150\", \"447\", \"…\n$ street_name         <chr> \"ANG MO KIO AVE 10\", \"ANG MO KIO AVE 4\", \"ANG MO K…\n$ storey_range        <chr> \"10 TO 12\", \"01 TO 03\", \"01 TO 03\", \"04 TO 06\", \"0…\n$ floor_area_sqm      <dbl> 44, 67, 67, 68, 67, 68, 68, 67, 68, 67, 68, 67, 67…\n$ flat_model          <chr> \"Improved\", \"New Generation\", \"New Generation\", \"N…\n$ lease_commence_date <dbl> 1979, 1978, 1980, 1980, 1980, 1981, 1979, 1976, 19…\n$ remaining_lease     <chr> \"61 years 04 months\", \"60 years 07 months\", \"62 ye…\n$ resale_price        <dbl> 232000, 250000, 262000, 265000, 265000, 275000, 28…\n\n\nHaving a glimpse of our data, we can observe that this dataset has a total of 11 columns and 149071 rows. The columns consist of month, town, flat_type, block, street_name, storey_range, floor_area_sqm, flat_model, lease_commence_date, remaining_lease, resale_price.\nFor this assignment, the dataset will focus on:\n\nTransaction period: Oct 2022 to February 2023\nTraining dataset period: Oct 2022 to December 2023\nTest dataset period: January 2023 to February 2023\nType of rook flat: 5-room flats\n\nThe code chunk filters our dataset accordingly.\n\nresale<- resale %>% \n  filter(flat_type == \"5 ROOM\") %>%\n  filter(month >= \"2022-10\" & month <= \"2023-02\")\n\nBased on my senior’s experience, “ST.” is usually written as “SAINT” instead - for example, St. Luke’s Primary School is written as Saint Luke’s Primary School. To address, this, we’ll replace such occurrences:\n\nresale$street_name <- gsub(\"ST\\\\.\", \"SAINT\", resale$street_name)\n\nSubsequently, as our dataset is missing the coordinates for the location, I created a function, geocode(), which calls onemapAPI to retrieve the geometry of each location.\n\n#library(httr)\ngeocode <- function(block, streetname) {\n  base_url <- \"https://developers.onemap.sg/commonapi/search\"\n  address <- paste(block, streetname, sep = \" \")\n  query <- list(\"searchVal\" = address, \n                \"returnGeom\" = \"Y\",\n                \"getAddrDetails\" = \"N\",\n                \"pageNum\" = \"1\")\n  \n  res <- GET(base_url, query = query)\n  restext<-content(res, as=\"text\")\n  \n  output <- fromJSON(restext)  %>% \n    as.data.frame %>%\n    select(results.LATITUDE, results.LONGITUDE)\n\n  return(output)\n}\n\nWhile exploring the API, I realised that the best search parameter would be to combine the column block with its street_name. Thus, the function geocode() takes in both the block and street name .\n\nresale$LATITUDE <- 0\nresale$LONGITUDE <- 0\n\nfor (i in 1:nrow(resale)){\n  temp_output <- geocode(resale[i, 4], resale[i, 5])\n  \n  resale$LATITUDE[i] <- temp_output$results.LATITUDE\n  resale$LONGITUDE[i] <- temp_output$results.LONGITUDE\n}\n\nTo reuse the resale dataset, without calling the API, I saved it into a rds file.\n\nwrite_rds(resale, \"data/rds/resale.rds\")\n\n\nresale_rds<-readRDS(\"data/rds/resale.rds\")\n\n\nStructural Data\nIn this section, I will be pre processing the structural data that I need for my tasks.\n\nFloor Level\nlet’s first take a look at the storey_range values.\n\nsort(unique(resale_rds$storey_range))\n\n [1] \"01 TO 03\" \"04 TO 06\" \"07 TO 09\" \"10 TO 12\" \"13 TO 15\" \"16 TO 18\"\n [7] \"19 TO 21\" \"22 TO 24\" \"25 TO 27\" \"28 TO 30\" \"31 TO 33\" \"34 TO 36\"\n[13] \"37 TO 39\" \"40 TO 42\"\n\n\nWe can see that there are 17 storey level ranges categories. The following code chunk recodes the categorical naming to numerical values by assigning 1 to the first range 01 TO 03 and 17 to the last range 49 TO 51.\n\nstorey <- sort(unique(resale_rds$storey_range))\nstorey_order <- 1:length(storey)\nstorey_range_order <- data.frame(storey, storey_order)\nstorey_range_order\n\n     storey storey_order\n1  01 TO 03            1\n2  04 TO 06            2\n3  07 TO 09            3\n4  10 TO 12            4\n5  13 TO 15            5\n6  16 TO 18            6\n7  19 TO 21            7\n8  22 TO 24            8\n9  25 TO 27            9\n10 28 TO 30           10\n11 31 TO 33           11\n12 34 TO 36           12\n13 37 TO 39           13\n14 40 TO 42           14\n\n\nNext, I will combine it to the original resale df.\n\nresale_rds <- left_join(resale_rds,  storey_range_order, by = c(\"storey_range\" = \"storey\"))\n\n\n\nRemaining Lease\nCurrently, the remaining_lease is in string format but we need it to be numeric. Thus, we need to split the string into month and year and then replace the original values with the calculated value of the remaining lease in years.\n\n#e.g of resale$remaining_lease - 53 years 10 months\nstr_list <- str_split(resale_rds$remaining_lease, \" \")\n\n#after spliting, [53, years, 10, months]\nfor (i in 1:length(str_list)){\n  if (length(unlist(str_list[i])) > 2) {\n      year <- as.numeric(unlist(str_list[i])[1])\n      month <- as.numeric(unlist(str_list[i])[3])\n      resale_rds$remaining_lease[i] <- year + round(month/12, 2)\n  }\n  else {\n    year <- as.numeric(unlist(str_list[i])[1])\n    resale_rds$remaining_lease[i] <- year\n  }\n}\n\n\n\nAge of Unit\nTo get the age of the unit, I decided to take the current year i.e 2023 and minus the lease commence date year of the the unit.\n\nstr_list <- resale_rds$lease_commence_date\n\nfor (i in 1:length(str_list)){\n    resale_rds$lease_commence_date[i] <- 2023 -\n      resale_rds$lease_commence_date[i]\n}\n\nFinally, we can convert it into a sf.\n\nresale_sf <- st_as_sf(resale_rds, \n                      coords = c(\"LONGITUDE\", \n                                 \"LATITUDE\"), \n                      crs=4326) %>%\n  st_transform(crs = 3414)\n\n\n\n\n\nData Pre-processing\nUnder this section, we will check for:\n\ncheck for missing values\ncheck correct coordinates system\ncheck for invalid geometries\n\n\nCheck for Missing Values\n\n#resale_sf\nsum(is.na(resale_sf))\n\n[1] 0\n\n\nWe can observe that we have no missing values.\n\n\nCheck for Correct Coordinates System\n\n#resale_sf\nst_crs(resale_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nOur dataset have the correct Singapore coordinates system of 3414.\n\n\nCheck for Invalid Geometries\n\n#resale_sf\nlength(which(st_is_valid(resale_sf) == FALSE))\n\n[1] 0\n\n\nBased on the output, our geometries are valid.\nOnce again, I saved the processed resale_sf in an rds file.\n\nwrite_rds(resale_sf, \"data/rds/resale_sf.rds\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#data-pre-processing",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#data-pre-processing",
    "title": "IS415-GAA",
    "section": "Data Pre-processing",
    "text": "Data Pre-processing\nUnder this section, we will check for:\n\nmissing values\ncheck correct coordinates system\ncheck for invalid geometries\nremove unnecessary columns\n\n\nMissing Values\n\n#resale_rds\nsum(is.na(resale_rds))\n\n[1] 0\n\n#shopping_mall_sf\nsum(is.na(shopping_mall_sf))\n\n[1] 0\n\n#primary_school_sf\nsum(is.na(primary_school_sf))\n\n[1] 0\n\n#popular_primary_schools_sf\nsum(is.na(popular_primary_schools_sf))\n\n[1] 0\n\n\nWe can observe that we have no missing values."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#data-pre-processing-1",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#data-pre-processing-1",
    "title": "IS415-GAA",
    "section": "Data-Pre Processing",
    "text": "Data-Pre Processing\nSame as before, we will conduct the following steps for data preprocessing, with an additional step of removing irrelevant columns for certain datasets:\n\nremoving irrelevant columns\ncheck for missing values\ncheck correct coordinates system\ncheck for invalid geometries\n\n\nRemove Irrelevant Columns\nSo for our datasets, we only need to know the name and the geometry so in the following code chunks i will be removing/dropping irrelevant columns.\n\n#mpsz\nmpsz <- mpsz %>% select(\"Name\")\n\n#childcare_sf\n#combine name and address postal to make it unique, some childcare have same name but diff locations\nchildcare_sf$full_address <- paste(childcare_sf$NAME, childcare_sf$ADDRESSP)\nchildcare_sf <- childcare_sf %>% select(\"full_address\")\n\n#eldercare_sf\neldercare_sf$full_address <- paste(eldercare_sf$NAME, eldercare_sf$ADDRESSPOSTALCODE)\neldercare_sf <- eldercare_sf %>% select(\"full_address\")\n\n#hawkercentre_new_sf\nhawkercentre_new_sf <- hawkercentre_new_sf %>% select(\"NAME\")\n\n#kindergartens_sf\nkindergartens_sf <- kindergartens_sf %>% select(\"NAME\")\n\n#nationalparks_sf\nnationalparks_sf <- nationalparks_sf %>% select(\"NAME\")\n\n#supermarket\nsupermarket_sf <- supermarket_sf %>% select(\"Description\")\n\n#bus stop\nbus_stop_sf$stop_name <- paste(bus_stop_sf$BUS_STOP_N, bus_stop_sf$BUS_ROOF_N, bus_stop_sf$LOC_DESC)\nbus_stop_sf <- bus_stop_sf %>% select(\"stop_name\")\n\n#mrt\n#combine stn name and exit to make each row unique\nmrt_sf$stn <- paste(mrt_sf$stn_name, mrt_sf$exit_code)\nmrt_sf <- mrt_sf %>% select(\"stn\")\n\n\n\nCheck for Missing Values\n\n#mpsz\nsum(is.na(mpsz))\n\n[1] 0\n\n#childcare_sf\nsum(is.na(childcare_sf))\n\n[1] 0\n\n+\n#eldercare_sf\nsum(is.na(eldercare_sf))\n\n[1] 0\n\n#hawkercentre_new_sf\nsum(is.na(hawkercentre_new_sf))\n\n[1] 0\n\n#kindergartens_sf\nsum(is.na(kindergartens_sf))\n\n[1] 0\n\n#nationalparks_sf\nsum(is.na(nationalparks_sf))\n\n[1] 0\n\n#supermarket_sf\nsum(is.na(supermarket_sf))\n\n[1] 0\n\n#bus stop\nsum(is.na(bus_stop_sf))\n\n[1] 0\n\n#MRT\nsum(is.na(mrt_sf))\n\n[1] 0\n\n#shopping_mall_sf\nsum(is.na(shopping_mall_sf))\n\n[1] 0\n\n#primary_school_sf\nsum(is.na(primary_school_sf))\n\n[1] 0\n\n#popular_primary_schools_sf\nsum(is.na(popular_primary_schools_sf))\n\n[1] 0\n\n\nBased on the output, there is no missing values.\n\n\nCheck for Correct Coordinates System\n\n#mpsz\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n#childcare_sf\nst_crs(childcare_sf)\n\nCoordinate Reference System:\n  User input: SVY21 / Singapore TM \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n#eldercare_sf\nst_crs(eldercare_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n#hawkercentre_new_sf\nst_crs(hawkercentre_new_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n#kindergartens_sf\nst_crs(kindergartens_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n#nationalparks_sf\nst_crs(nationalparks_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n#supermarket_sf\nst_crs(supermarket_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n#bus_stop_sf\nst_crs(bus_stop_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n#mrt_sf\nst_crs(mrt_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n#shopping_mall_sf\nst_crs(shopping_mall_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n#primary_school_sf\nst_crs(primary_school_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n#popular_primary_schools_sf\nst_crs(popular_primary_schools_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nBased on the output, our datasets are in the correct coordinate systems.\n\n\nCheck for Invalid Geometries\n\n#mpsz\nlength(which(st_is_valid(mpsz) == FALSE))\n\n[1] 9\n\n#childcare_sf\nlength(which(st_is_valid(childcare_sf) == FALSE))\n\n[1] 0\n\n#eldercare_sf\nlength(which(st_is_valid(eldercare_sf) == FALSE))\n\n[1] 0\n\n#kindergartens_sf\nlength(which(st_is_valid(kindergartens_sf) == FALSE))\n\n[1] 0\n\n#hawkercentre_new_sf\nlength(which(st_is_valid(hawkercentre_new_sf) == FALSE))\n\n[1] 0\n\n#nationalparks_sf\nlength(which(st_is_valid(nationalparks_sf) == FALSE))\n\n[1] 0\n\n#supermarket_sf\nlength(which(st_is_valid(supermarket_sf) == FALSE))\n\n[1] 0\n\n#bus_stop_sf\nlength(which(st_is_valid(bus_stop_sf) == FALSE))\n\n[1] 0\n\n#mrt_sf\nlength(which(st_is_valid(mrt_sf) == FALSE))\n\n[1] 0\n\n#shopping_mall_sf\nlength(which(st_is_valid(shopping_mall_sf) == FALSE))\n\n[1] 0\n\n#primary_school_sf\nlength(which(st_is_valid(primary_school_sf) == FALSE))\n\n[1] 0\n\n#popular_primary_schools_sf\nlength(which(st_is_valid(popular_primary_schools_sf) == FALSE))\n\n[1] 0\n\n\nBased on the output, we can see that only the mpsz dataset has invalid geometries. The code chunk beow addresses this issue.\n\nmpsz <- st_make_valid(mpsz)\nlength(which(st_is_valid(mpsz) == FALSE))\n\n[1] 0\n\n\n\n\nVisualisations\nThe following code chunks shows some visualisations for the data we have just preprocessed.\n\nSubzone Boundary of Singapore 2014\n\nplot(st_geometry(mpsz))\n\n\n\n\n\n\nMRT/LRT Stations Map\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(mrt_sf) +\n  tm_dots(col=\"red\", size=0.05) +\n  tm_view(set.zoom.limits = c(11, 14))\n\n\n\n\n\n\n\n\nBus Map\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(mpsz) +\n  tm_borders(alpha = 0.5) +\n  tmap_options(check.and.fix = TRUE) +\ntm_shape(bus_stop_sf) +\n  tm_dots(col=\"red\", size=0.05) +\n  tm_layout(main.title = \"Bus Stops\",\n          main.title.position = \"center\",\n          main.title.size = 1.2,\n          frame = TRUE)\n\n\n\n\n\n\nEducation/Healthcare related map\nThe code chunk below shows the location of:\n\nchildcare centres - blue dots\neldercare centres - red dots\nkindergartens - orange dots\nprimary schools - black dots\ntop 10 popular primary schools - grey dots\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(childcare_sf) +\n  tm_dots(alpha=0.5, #affects transparency of points\n          col=\"#2ec4b6\",\n          size=0.05) +\ntm_shape(eldercare_sf) +\n  tm_dots(alpha=0.5,\n          col=\"#e71d36\",\n          size=0.05) +\ntm_shape(kindergartens_sf) +\n  tm_dots(alpha=0.5,\n          col=\"#ff9f1c\",\n          size=0.05) +\ntm_shape(primary_school_sf) +\n  tm_dots(alpha=0.5,\n          col=\"#011627\",\n          size=0.05) +\ntm_shape(popular_primary_schools_sf) +\n  tm_dots(alpha=0.5,\n        col=\"grey\",\n        size=0.05) +\n  tm_view(set.zoom.limits = c(11, 14))\n\n\n\n\n\n\nOne thing we can observe is that there are more childcare centres as compared to the rest.\n\n\nAmenities Related Map\nThe code chunk below shows the location of:\n\nsupermarkets - red dots\nshopping malls - orange dots\nnational parks - dark green dots\nhawker centres - blue dots\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(supermarket_sf) +\n  tm_dots(alpha=0.5, #affects transparency of points\n          col=\"#d62828\",\n          size=0.05) +\ntm_shape(shopping_mall_sf) +\n  tm_dots(alpha=0.5,\n          col=\"#f77f00\",\n          size=0.05) +\ntm_shape(supermarket_sf) +\n  tm_dots(alpha=0.5,\n          col=\"#fcbf49\",\n          size=0.05) +\ntm_shape(nationalparks_sf) +\n  tm_dots(alpha=0.5,\n          col=\"#023020\",\n          size=0.05) +\ntm_shape(hawkercentre_new_sf) +\n  tm_dots(alpha=0.5,\n          col=\"#ADD8E6\",\n          size=0.05) +\n  tm_view(set.zoom.limits = c(10, 14))\n\n\n\n\n\n\n\n\nResale Flat Prices\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntmap_options(check.and.fix = TRUE)\ntm_shape(resale_sf) +  \n  tm_dots(col = \"resale_price\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  # sets minimum zoom level to 11, sets maximum zoom level to 14\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\nWe can observe that from July 2022 to December 2022, the are majority of the 5 room HDB flats can be found in the east side of Singapore and the higher priced flats tend to be in the southern-eastern side of Singapore.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#formulated-functions",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#formulated-functions",
    "title": "IS415-GAA",
    "section": "Formulated Functions",
    "text": "Formulated Functions\n\nProximity Distance Calculations\nAs per our task , we need to find the proximity of certain facilities i.e proximity to CBD, eldercare, hawker centres, MRT, park, good primary school, shopping mall and supermarket. It can be computed by creating a function called proximity which utilises st_distance() to find the closest facility (shortest distance) with the rowMins() function of our matrixStats package. The values will then be appended to the resale_sf as a new column.\n\nproximity <- function(df1, df2, varname) {\n  dist_matrix <- st_distance(df1, df2) %>%\n    drop_units()\n  df1[,varname] <- rowMins(dist_matrix)\n  return(df1)\n}\n\n\nresale_sf<-readRDS(\"data/rds/resale_sf.rds\")\n\n\n#CBD, eldercare, hawker centres, MRT, park, good primary school, shopping mall and supermarket.\nresale_sf <- \n  proximity(resale_sf, cbd_sf, \"PROX_CBD\") %>%\n  proximity(., eldercare_sf, \"PROX_ELDERCARE\") %>%\n  proximity(., hawkercentre_new_sf, \"PROX_HAWKER\") %>%\n  proximity(., mrt_sf, \"PROX_MRT\") %>%\n  proximity(., nationalparks_sf, \"PROX_PARK\") %>%\n  proximity(., popular_primary_schools_sf, \"PROX_GDPRISCH\") %>%\n  proximity(., shopping_mall_sf, \"PROX_SHOPPINGMALL\") %>%\n  proximity(., supermarket_sf, \"PROX_SPRMKT\")\n\n\n\nFacility Count within Radius Calculations\nAs per our task, we also want to find the number of facilities within a particular radius. Thus, we have to create another function called num_radius() which utilise st_distance() to compute the distance between the flats and the desiredfacilities, and then sum up the observations with rowSums(). The values will be appended to the data frame as a new column.\n\nnum_radius <- function(df1, df2, varname, radius) {\n  dist_matrix <- st_distance(df1, df2) %>%\n    drop_units() %>%\n    as.data.frame()\n  df1[,varname] <- rowSums(dist_matrix <= radius)\n  return(df1)\n}\n\n\n#Numbers of kindergartens within 350m, Numbers of childcare centres within 350m, Numbers of bus stop within 350m, Numbers of primary school within 1km\nresale_sf <- \n  num_radius(resale_sf, kindergartens_sf, \"NUM_KNDRGTN\", 350) %>%\n  num_radius(., childcare_sf, \"NUM_CHILDCARE\", 350) %>%\n  num_radius(., bus_stop_sf, \"NUM_BUS_STOP\", 350) %>%\n  num_radius(., primary_school_sf, \"NUM_PRISCH\", 1000)\n\nOnce again, I would like to save the latest resale_sf file.\n\nwrite_rds(resale_sf, \"data/rds/resale_sf_complete.rds\")\n\n\nglimpse(readRDS(\"data/rds/resale_sf_complete.rds\"))\n\nRows: 2,491\nColumns: 25\n$ month               <chr> \"2022-10\", \"2022-10\", \"2022-10\", \"2022-10\", \"2022-…\n$ town                <chr> \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO …\n$ flat_type           <chr> \"5 ROOM\", \"5 ROOM\", \"5 ROOM\", \"5 ROOM\", \"5 ROOM\", …\n$ block               <chr> \"306\", \"310A\", \"551\", \"551\", \"431\", \"501\", \"253\", …\n$ street_name         <chr> \"ANG MO KIO AVE 1\", \"ANG MO KIO AVE 1\", \"ANG MO KI…\n$ storey_range        <chr> \"10 TO 12\", \"13 TO 15\", \"01 TO 03\", \"16 TO 18\", \"2…\n$ floor_area_sqm      <dbl> 123, 119, 118, 118, 119, 121, 137, 110, 110, 110, …\n$ flat_model          <chr> \"Standard\", \"Improved\", \"Improved\", \"Improved\", \"I…\n$ lease_commence_date <dbl> 46, 11, 42, 42, 44, 42, 27, 22, 22, 22, 5, 5, 5, 2…\n$ remaining_lease     <chr> \"53.83\", \"89\", \"57.33\", \"57.33\", \"55.42\", \"57.33\",…\n$ resale_price        <dbl> 700000, 980000, 568000, 603000, 720000, 670000, 80…\n$ storey_order        <int> 4, 5, 1, 6, 9, 3, 1, 7, 1, 5, 2, 9, 7, 8, 7, 4, 3,…\n$ geometry            <POINT [m]> POINT (29383.53 38640.51), POINT (29171.38 3…\n$ PROX_CBD            <dbl> 8625.861, 8544.547, 9537.543, 9537.543, 8876.533, …\n$ PROX_ELDERCARE      <dbl> 211.9637, 143.8115, 1064.6617, 1064.6617, 356.4709…\n$ PROX_HAWKER         <dbl> 331.2628, 496.9721, 482.8156, 482.8156, 375.3304, …\n$ PROX_MRT            <dbl> 573.2417, 797.7411, 1080.8607, 1080.8607, 365.2200…\n$ PROX_PARK           <dbl> 554.3174, 593.1602, 735.9373, 735.9373, 390.1731, …\n$ PROX_GDPRISCH       <dbl> 1526.5641, 1292.1608, 3212.9380, 3212.9380, 2376.2…\n$ PROX_SHOPPINGMALL   <dbl> 490.9497, 708.9643, 1213.2871, 1213.2871, 514.1228…\n$ PROX_SPRMKT         <dbl> 246.5007, 313.5439, 419.9139, 419.9139, 313.5926, …\n$ NUM_KNDRGTN         <dbl> 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,…\n$ NUM_CHILDCARE       <dbl> 6, 2, 3, 3, 4, 1, 3, 4, 4, 3, 4, 4, 4, 4, 6, 5, 8,…\n$ NUM_BUS_STOP        <dbl> 6, 6, 2, 2, 6, 9, 11, 6, 6, 6, 6, 8, 8, 7, 7, 12, …\n$ NUM_PRISCH          <dbl> 2, 2, 1, 1, 3, 1, 3, 3, 3, 3, 2, 3, 3, 2, 2, 4, 4,…"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#test-of-multicollinearity-vif",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#test-of-multicollinearity-vif",
    "title": "IS415-GAA",
    "section": "Test of multicollinearity (VIF)",
    "text": "Test of multicollinearity (VIF)\nWe can use ols_vif_tol() function of our olsrr package to calculate VIF. In general, if the VIF value is less than 5, then there is usually no sign/possibility of correlations.\n\nols_vif_tol(mlr1)\n\n           Variables Tolerance      VIF\n1     floor_area_sqm 0.5490904 1.821194\n2    remaining_lease 0.4939016 2.024695\n3       storey_order 0.8716868 1.147201\n4           PROX_CBD 0.4984548 2.006200\n5     PROX_ELDERCARE 0.7790581 1.283601\n6        PROX_HAWKER 0.7879037 1.269191\n7           PROX_MRT 0.8119679 1.231576\n8          PROX_PARK 0.8281714 1.207480\n9      PROX_GDPRISCH 0.7162116 1.396235\n10 PROX_SHOPPINGMALL 0.7027136 1.423055\n11       PROX_SPRMKT 0.8063695 1.240126\n12       NUM_KNDRGTN 0.7460480 1.340396\n13     NUM_CHILDCARE 0.6323711 1.581350\n14      NUM_BUS_STOP 0.8319573 1.201985\n15        NUM_PRISCH 0.6071676 1.646992\n\n\nTh higher the VIF, the less reliable our regression results are going to be. Based on the output, there are no signs of multicollinearity as VIF <5.\nThe code chunk below saves the results of our model to an rds file.\n\nwrite_rds(mlr1, \"data/model/mlr1.rds\" )"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#calculating-predictive-error",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#calculating-predictive-error",
    "title": "Take-home Assignment 3: Predicting HDB Public Housing Resale Pricies using Geographically Weighted Methods",
    "section": "Calculating Predictive Error",
    "text": "Calculating Predictive Error\n\n\nCode\nlm_predicted_value <- predict.lm(mlr1, newdata = test_data[,-2])\n\n# Calculate MSE\nMSE <- mean((test_data$resale_price - lm_predicted_value)^2)\n\nrmse_lm <- sqrt(MSE)\nrmse_lm\n\n\nThe Root Mean Square Error (RMSE) of our regression model is 72461. Based on research, the lower the value of RMSE, it indicates a better fit. RMSE is a also good measure of how accurately the model predicts the response."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#converting-the-sf-data.frame-to-spatialpointdataframe",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#converting-the-sf-data.frame-to-spatialpointdataframe",
    "title": "IS415-GAA",
    "section": "Converting the sf data.frame to SpatialPointDataFrame",
    "text": "Converting the sf data.frame to SpatialPointDataFrame\n\ntrain_data_sp <- as_Spatial(train_data[,-2])\ntrain_data_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1496 \nextent      : 6958.193, 42645.18, 28157.26, 48741.06  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 16\nnames       : floor_area_sqm, remaining_lease, resale_price, storey_order,         PROX_CBD,      PROX_ELDERCARE,      PROX_HAWKER,         PROX_MRT,        PROX_PARK,    PROX_GDPRISCH,   PROX_SHOPPINGMALL,         PROX_SPRMKT, NUM_KNDRGTN, NUM_CHILDCARE, NUM_BUS_STOP, ... \nmin values  :            104,           49.75,       415000,            1,  1610.9563636452, 0.00121769219557756, 49.4878683203785, 27.3898687165439, 60.0396043524142, 115.665591934146, 0.00118710679792838, 0.00109267327868705,           0,             0,            0, ... \nmax values  :            149,           95.33,      1345000,           14, 23277.3731998479,     8265.9709141931, 6633.82585101601, 2070.17690667209, 6003.01683926018, 9048.73399591188,    4009.20093399522,    1451.97426427893,           7,            19,           19, ..."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#computing-adaptive-bandwidth",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#computing-adaptive-bandwidth",
    "title": "IS415-GAA",
    "section": "Computing adaptive bandwidth",
    "text": "Computing adaptive bandwidth\n\nbw_adaptive <- bw.gwr(resale_price ~.,\n                  data = train_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE\n                  )\n\n\nDoing cross-validation while utilising a gaussian kernel, the smallest CV score is 3.054284e+12 and its adaptive bandwidth is 22.\nNext, we will save the adaptive bandwidth as an rds file.\n\nwrite_rds(bw_adaptive, file = \"data/model/bw_adaptive.rds\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#constructing-the-adaptive-bandwidth-gwr-model",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#constructing-the-adaptive-bandwidth-gwr-model",
    "title": "IS415-GAA",
    "section": "Constructing the adaptive bandwidth GWR model",
    "text": "Constructing the adaptive bandwidth GWR model\nFirst, let’s call the saved bandwith by using the code chunk below.\n\nbw_adaptive <- read_rds(\"data/model/bw_adaptive.rds\")\n\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and Gaussian kernel as shown in the code chunk below.\n\ngwr_adaptive <- gwr.basic(formula = resale_price ~.,\n                          data=train_data_sp,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n\nThe code chunk below will be used to save the model in rds format for future use.\n\nwrite_rds(gwr_adaptive, \"data/model/gwr_adaptive.rds\")\n\nThe code below can be used to display the model output.\n\ngwr_adaptive <- read_rds(\"data/model/gwr_adaptive.rds\")\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-03-26 19:54:55 \n   Call:\n   gwr.basic(formula = resale_price ~ ., data = train_data_sp, bw = bw_adaptive, \n    kernel = \"gaussian\", adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  .\n   Number of data points: 1496\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-232352  -45529   -4238   37272  385542 \n\n   Coefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)       -1.973e+05  5.911e+04  -3.338 0.000863 ***\n   floor_area_sqm     6.251e+03  3.568e+02  17.518  < 2e-16 ***\n   remaining_lease    6.167e+03  2.397e+02  25.730  < 2e-16 ***\n   storey_order       2.088e+04  1.038e+03  20.112  < 2e-16 ***\n   PROX_CBD          -1.891e+01  1.077e+00 -17.560  < 2e-16 ***\n   PROX_ELDERCARE     1.140e+01  3.175e+00   3.589 0.000342 ***\n   PROX_HAWKER       -2.752e+01  3.720e+00  -7.398 2.30e-13 ***\n   PROX_MRT          -3.121e+01  5.644e+00  -5.530 3.79e-08 ***\n   PROX_PARK          2.249e+01  4.435e+00   5.070 4.48e-07 ***\n   PROX_GDPRISCH      1.427e+00  1.433e+00   0.996 0.319346    \n   PROX_SHOPPINGMALL -6.769e+00  6.302e+00  -1.074 0.282938    \n   PROX_SPRMKT        1.095e+01  1.375e+01   0.797 0.425714    \n   NUM_KNDRGTN        1.036e+04  2.050e+03   5.054 4.86e-07 ***\n   NUM_CHILDCARE     -4.090e+03  1.036e+03  -3.950 8.20e-05 ***\n   NUM_BUS_STOP       1.515e+02  6.918e+02   0.219 0.826681    \n   NUM_PRISCH        -8.990e+03  1.447e+03  -6.213 6.75e-10 ***\n   coords.x1          7.400e-01  3.382e-01   2.188 0.028814 *  \n   coords.x2         -3.416e+00  7.531e-01  -4.536 6.20e-06 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 74740 on 1478 degrees of freedom\n   Multiple R-squared: 0.7123\n   Adjusted R-squared: 0.709 \n   F-statistic: 215.3 on 17 and 1478 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.257033e+12\n   Sigma(hat): 74342.42\n   AIC:  37841.04\n   AICc:  37841.56\n   BIC:  36584.84\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 22 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                            Min.     1st Qu.      Median     3rd Qu.       Max.\n   Intercept         -8.5125e+08 -9.6843e+06  1.5185e+06  1.3389e+07 5.3217e+09\n   floor_area_sqm    -4.0120e+03  2.5913e+03  3.9227e+03  5.9585e+03 1.5536e+06\n   remaining_lease   -9.7555e+04  5.4572e+03  7.2246e+03  9.0025e+03 4.9399e+04\n   storey_order       6.3018e+03  1.2182e+04  1.5164e+04  1.8695e+04 2.5594e+04\n   PROX_CBD          -2.9220e+04 -5.6422e+02 -2.5133e+00  4.8308e+02 2.3844e+05\n   PROX_ELDERCARE    -4.8216e+04 -1.2686e+01  1.9113e+01  6.4787e+01 1.2264e+03\n   PROX_HAWKER       -1.3614e+03 -4.4118e+01 -6.8652e+00  3.3078e+01 1.1669e+04\n   PROX_MRT          -1.4344e+04 -9.4417e+01 -5.0592e+01 -4.7851e+00 8.5307e+02\n   PROX_PARK         -6.4812e+02 -4.2881e+01 -9.2976e+00  2.3041e+01 1.5562e+03\n   PROX_GDPRISCH     -3.9664e+03 -3.6373e+01  5.5693e+00  5.4074e+01 8.6887e+03\n   PROX_SHOPPINGMALL -7.0975e+02 -6.6587e+01 -2.0800e+01  2.1314e+01 1.1766e+04\n   PROX_SPRMKT       -1.0656e+03 -3.0144e+01  5.7481e+00  4.8561e+01 2.5178e+03\n   NUM_KNDRGTN       -5.2077e+04 -3.3102e+03  3.0430e+03  1.2316e+04 1.7806e+05\n   NUM_CHILDCARE     -5.1618e+04 -3.3261e+03  1.7252e+02  2.4240e+03 2.2769e+04\n   NUM_BUS_STOP      -4.4570e+04 -8.6265e+02  1.3844e+03  3.6228e+03 2.4699e+04\n   NUM_PRISCH        -6.5807e+04 -5.8145e+03  2.0226e+03  8.5195e+03 2.8605e+05\n   coords.x1         -2.3903e+04 -3.5479e+02 -5.3459e+01  1.5550e+02 1.3272e+04\n   coords.x2         -1.8906e+05 -3.2348e+02 -4.1838e+00  4.0785e+02 2.9058e+04\n   ************************Diagnostic information*************************\n   Number of data points: 1496 \n   Effective number of parameters (2trace(S) - trace(S'S)): 552.7639 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 943.2361 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 36424.99 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 35569.1 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 36939.87 \n   Residual sum of squares: 1.369059e+12 \n   R-square value:  0.9523041 \n   Adjusted R-square value:  0.9243233 \n\n   ***********************************************************************\n   Program stops at: 2023-03-26 19:54:57 \n\n\nFrom the output above, we can observe that the GWR Adjusted R-square is 0.9243233, which is higher than the non-spatial mulitiple linear regression model’s Adjusted R-square of 0.709 . Based on research, R-squared measures the goodness of fit of a regression model. Hence, a higher R-squared indicates the model is a good fit, while a lower R-squared indicates the model is not a good fit which suggests that the GWR model is a better fit.\nGWR model also has a lower AIC i.e 35569.1 than the regression model’s AIC i.e 37841.04. Based on research, the lower AIC scores, the better it is, as AIC penalizes models that use more parameters."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#extracting-coordinates-data",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#extracting-coordinates-data",
    "title": "IS415-GAA",
    "section": "Extracting coordinates data",
    "text": "Extracting coordinates data\nThe code chunk below extract the x,y coordinates of the full, training and test data sets.\n\ncoords_train <- st_coordinates(train_data)\ncoords_test <- st_coordinates(test_data)\n\nBefore continue, we write all the output into rds for future uses\n\ncoords_train <- write_rds(coords_train, \"data/model/coords_train.rds\" )\ncoords_test <- write_rds(coords_test, \"data/model/coords_test.rds\" )\n\nNow, we need to retrieve our training data, which does not have the geometry.\n\ntrain_nogeo_rds<-readRDS(\"data/model/train_nogeo.rds\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#calculating-root-mean-square-error",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#calculating-root-mean-square-error",
    "title": "IS415-GAA",
    "section": "Calculating Root Mean Square Error",
    "text": "Calculating Root Mean Square Error\nThe root mean square error (RMSE) allows us to measure how far predicted values are from observed values in a regression analysis.\n\nMultiple Linear Regression (OLS)\n\n#lm\n#MSE <- mean((test_data$resale_price - lm_predicted_value)^2)\n#rmse_lm <- sqrt(MSE)\n\nsqrt(mean((prices_pred_df$actual_price - prices_pred_df$ols_pred)^2))\n\n[1] 69388.4\n\n\n\n\nRandom Forest\n\nsqrt(mean((prices_pred_df$actual_price - prices_pred_df$rf_pred)^2))\n\n[1] 55187.82\n\n\n\n\nGWRF model\nIn the code chunk below, rmse() of Metrics package is used to compute the RMSE for GRF model.\n\n#grf\nrmse(test_data_p$resale_price, \n     test_data_p$GRF_pred)\n\n[1] 53185.72\n\n#sqrt(mean((prices_pred_df$actual_price - prices_pred_df$gwrf_pred)^2))\n\nComparing the 3 models RMSE, the GWRF model has the lowest RMSE of 53185.72 as compared to OLS model’s RMSE of 69388.4 and random forest model’s RMSE of 55187.82. RMSE measures the average difference between values predicted by a model and the actual values and provides an estimation of how well the model is able to predict the target value (accuracy). Hence, among the 3 models, the GWRF model will be better model at predicting the resale price."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#visualising-the-predicted-values",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#visualising-the-predicted-values",
    "title": "IS415-GAA",
    "section": "Visualising the Predicted Values",
    "text": "Visualising the Predicted Values\n\ntest_data_models <- cbind(test_data, prices_pred_df)\n\ngwr_plot <- ggplot(data = test_data_p,\n       aes(x = GRF_pred,\n           y = resale_price)) +\n  geom_point()+\n  geom_abline(col = \"Red\")\n\n\nlm_plot <- ggplot(data = test_data_models,\n       aes(x = ols_pred,\n           y = actual_price)) +\n  geom_point()+\n  geom_abline(col = \"Red\")\n\nrf_plot <- ggplot(data = test_data_models,\n       aes(x = rf_pred,\n           y = actual_price)) +\n  geom_point()+\n  geom_abline(col = \"Red\")\n\nplot_grid(gwr_plot, lm_plot, rf_plot, labels = \"AUTO\")\n\n\n\n\nBy comparing the 3 graphs, GWRF model is more linear while OLS and random forest model points on the right half side, majority of it is above the red line. Thus, we can see that the GWRF model is better than the OLS and random forest model as the scatter points are more closer to the diagonal line."
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#multiple-linear-regression-ols",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#multiple-linear-regression-ols",
    "title": "IS415-GAA",
    "section": "Multiple Linear Regression (OLS)",
    "text": "Multiple Linear Regression (OLS)\nThe code chunk below uses predict.lm() function of the stats package to run inference on our test data and save it into RDS format.\n\nols_pred <- predict.lm(mlr1, test_data[,-2]) %>%\n  write_rds(\"data/model/ols_pred.rds\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#random-forest",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#random-forest",
    "title": "IS415-GAA",
    "section": "Random Forest",
    "text": "Random Forest\nThe code chunk below uses predict() function of the ranger package to run inference on our test data and save it into RDS format.\n\ntest_nogeo <-test_data  %>%\n  st_drop_geometry()\n\nrf_pred <- predict(rf, test_nogeo) %>%\n  write_rds(\"data/model/rf_pred.rds\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#gwrf-model",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#gwrf-model",
    "title": "IS415-GAA",
    "section": "GWRF model",
    "text": "GWRF model\nThe code chunk below will be used to combine the test data with its corresponding coordinates data.\n\ntest_data <- cbind(test_data, coords_test) %>%\n  st_drop_geometry()\n\nNext, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.\n\nset.seed(1234)\ngwRF_pred <- predict.grf(gwRF_adaptive, \n                           test_data, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\nBefore moving on, let us save the output into rds file for future use.\n\nGRF_pred <- write_rds(gwRF_pred, \"data/model/GRF_pred.rds\")\n\nThe output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis.\n\nGRF_pred <- read_rds(\"data/model/GRF_pred.rds\")\nGRF_pred_df <- as.data.frame(GRF_pred)\n\nIn the code chunk below, cbind() is used to append the predicted values for both the GRF and multiple linear regression mode onto test_data.\n\n#for gwrf\ntest_data_p <- cbind(test_data, GRF_pred_df)\n\n\nwrite_rds(test_data_p, \"data/model/test_data_p.rds\")"
  },
  {
    "objectID": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#models-evaluation",
    "href": "Take-home_Assgn/Take-home_Assgn3/Take-home_Assgn3.html#models-evaluation",
    "title": "IS415-GAA",
    "section": "Models Evaluation",
    "text": "Models Evaluation\nTo evaluate the 3 models, we can combine the resale price and predicted prices of each model into a single dataframe.\n\nols_pred_df <- read_rds(\"data/model/ols_pred.rds\") %>%\n  as.data.frame()\ncolnames(ols_pred_df) <- c(\"ols_pred\")\n\nrf_pred_df <- read_rds(\"data/model/rf_pred.rds\")$predictions %>%\n  as.data.frame()\ncolnames(rf_pred_df) <- c(\"rf_pred\")\n\ngwRF_pred_df <-  GRF_pred %>%\n  as.data.frame()\ncolnames(gwRF_pred_df) <- c(\"gwrf_pred\")\n\nprices_pred_df <- cbind(test_data$resale_price, ols_pred_df, rf_pred_df,\n                        gwRF_pred_df) %>% \n  rename(\"actual_price\" = \"test_data$resale_price\")\n\nhead(prices_pred_df)\n\n  actual_price ols_pred  rf_pred gwrf_pred\n1       682888 728585.5 769835.8  728135.3\n2       695000 643365.4 660018.7  698225.9\n3       658888 686182.7 754647.1  689452.9\n4       790000 789026.6 763765.2  856222.9\n5       800000 704982.1 762599.3  782600.2\n6       858000 831632.1 755024.8  827002.7\n\n\nLooking at the results, it seems like gwrf model has the closest predicted value to the actual price as compared to the other models."
  }
]